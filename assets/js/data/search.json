[ { "title": "Mentoring_1", "url": "/posts/mentoring_1/", "categories": "", "tags": "", "date": "2025-05-26 00:00:00 +0900", "snippet": "[CMOE 북코칭] “왜 리더인가” / 박중근 멘토님CMOE 에서 진행하는 북코칭을 참여했다. 이번에는 4번째 시간이었고 이나모리 가즈오 선생님의 “왜 리더인가” 라는 책으로 세션을 진행했다. ( cmoe 글 )북코칭은 참 매력적이다. 단순히 Lecture 와 Listener 가 아니라 상호 소통하는 시간을 가지면서 Coach 는 Follower 들의 인사이트를 깨우쳐주고 한번 더 생각할 질문을 던진다.이런 시간을 가지다보면 혼자서는 생각하기 어려운 부분들을 건드릴 수 있다.마치 올바른 길을 가기위해 영양제를 조금씩 먹는 느낌 혹은 아주 소프트한 망치로 못을 살살 건드려서 올바른 길로 각도를 틀어주는 그런 기분이다.박중근 멘토님과의 대화는 항상 그런 기분이다. Direct 하게 뭔가 꽂히는 느낌보다는 “아 그렇구나, 내가 이렇게 생각해보면 좋겠다.” 와 같은 간접적인 가르침을 받을 수 있다.Anyway….왜 리더인가 라는 책은 정말 명료하면서도 울림이 있는 책이다.일본 최고의 경영자라고 불리는 이나모리 가즈오 선생님의 철학이 잘 녹아있으며 리더로서 아니 한 사람으로서 어떤 마음가짐과 생각을 가지고 살아야하는지를 잘 이야기하고 있다.대부분의 내가 읽어 본 일본 책들은 번역도 나름 잘 되어있어서 쉽게 이해할 수 있었다.책에서는 불교 철학이 꽤나 많이 나온다. 그래서 더 가깝게 느낄 수 있었던 것 같다.AI 활용 설정원 제목 : 마음. 인생을 마음대로 하는 힘왜 리더인가. 책 제목이 꽤나 자극적이다.원 타이틀을 보면 조금은 (아니 매우..?) 다른 것을 알 수 있다.마음. 인생을 마음대로 하는 힘.리더로서의 자기개발서라기 보다는 철학 서적에 가깝다고도 볼 수 있겠다.그만큼 마음이 가지는 힘이 크다는 것을 의미한다.오직 성공만 생각하고, 성공할 것처럼 행동하라.마음이 무너지지 않으면 그 무엇도 무너지지 않는다.출처 입력이 말이 참 좋다.우리가 힘들 때를 생각해보면 항상 마음이 먼저 무너진다. 사실 마음이 무너졌을 때 육체적인 부분은 힘이 남아있을 경우가 많다.그래서 움직이면 마음이 다시 추스러지기도 한다. 그래서 이 말이 더 중요한 것 같다. 마음이 무너지지 않아야한다.나는 힘들 때 이런 말을 되뇌이곤 한다.“힘들면 나만 손해. 아무도 나를 책임져주지 않는다.”되뇌이는 이 말에 대해서 공감을 얻은 것 같아서 기분이 나쁘지 않았다.마음이 무너지지 않으면 그 무엇도 무너지지 않는다. 마음을 굳게 먹어야하고 원하는 방향과 성취 목표가 있다면 항상 시각화를 하면서 끊임없는 노력이 필요하다.우리 현대인들에게 매우 필요한 부분이라고 생각한다.Leader Keyword리더의 키워드에 대해서 많은 이야기를 나누었는데 그 중 기억에 남는 두 가지를 적어본다.Trust신뢰는 매우 중요하다. 근데 한 가지 흥미로웠던 점은 신뢰에도 Competence 가 필요하다는 것이다.사람간의 soft 한 신뢰도 중요하지만, 리더로서는 역량적인 신뢰 또한 매우 중요하다.내가 따르는 리더가 일을 못한다면? 탁월하지 못하다면? 과연 신뢰가 생길까.아무리 공감의 리더십이 중요하다고 하더라도, 그게 수행하는 업무들의 결과에서 탁월한 성과가 나오지 않는다면 과연 Follower 들은 그를 신뢰할까.나는 어떻게 이 전문성있는 역량을 키울 수 있을까. 끊임없이 고민해야한다.Empathy공감능력도 지능이다. 이런 말이 있다. 더 이상 위에서 강압하는 리더십이 통하는 시대가 아니다.공감하지 못하는 리더는 능력이 없는 것이다. 팀원들과 공감하는 대화를 해야하며 이를 위한 첫 걸음이 Humble Inquiry 이다. 즉, coaching.순수한 호기심으로 상대에게 질문하고, 단언하지 않으며 대화하는 기법. 이를 통해서 상대방을 공감할 수 있다.가식적인 공감은 없으니 못하다. 순수한 호기심과 끊임없는 노력으로 상대방을 공감하며 신뢰를 형성해야 한다.ROC InvestmentYour chracter is yout fateHeraclitusReturn on Character신박한 ETF 상품이다.회사의 Intergrity, Responsibility, Forgiveness, Compassion 지표를 통해서 회사를 평가한다.즉, 이런 부분들이 뛰어나면 좋은 성과를 가져올 것이라는 것이다.왜 리더인가 라는 책에 나오는 대목들과 매우 유사하다.이를 잘 따르는 기업은 Apple, Microsoft 가 있다.북코칭을 통해 좋은 책을 읽어볼 수 있었고, 생각할 시간을 가질 수 있어서 매우 유익했다.선한 인격, 이타적 마음, 의지, 정직 배려 겸손, 감사한 마음이를 통해서 멋진 사람이 되자." }, { "title": "2023년 3분기 회고", "url": "/posts/blog-3q-review/", "categories": "Blog", "tags": "blog", "date": "2023-10-21 00:00:00 +0900", "snippet": "Intro어느덧 시간이 흘러서 2023년의 3분기가 지나갔습니다. 항상 있었던 일을 기록하고 회고를 해야겠다고 생각만 했었지, 블로그에 글로 남기는 일은 정말 오랜만인 것 같습니다.링크드인으로 연결 된 안수빈님 블로그의 3분기 회고글을 받고 영감을 얻어 저 또한 굵직한 일들이 많았던 23년 3분기를 회고하고 정리해야겠다는 생각에 이렇게 글을 써봅니다.23년의 3분기는 특히나 여러 굵직한 일들이 많았던 것 같습니다. 그럼 하나씩 정리해보도록 하겠습니다 😊😊🏢 첫번째 퇴사, 첫번째 이직23년 3분기 회고에서 처음으로 이야기 할 이벤트는 사실 올 해가 아니라 제 인생 전반을 두고 보았을 때도 작지 않은 경험이었습니다. 첫 회사에서 경험한 희망 퇴직과 첫 이직 준비, 그리고 새로운 도전까지.혼자서 주관적으로 자신의 수준을 판단하는 것이 아닌 객관적인 시선으로 본인을 바라보는 것이 굉장히 중요하다는 것을 깨달았고, 그 누구도 나를 위해 대신 희생해주지 않는다는 것 또한 한번 더 깨닫게 되었습니다. 또한 많은 일들을 겪으며 “지치면 나만 손해” 라는 저의 좌우명을 몇 번이고 다시 되새기는 시점이기도 하였습니다.겪었던 많은 상황들을 이야기하고 제가 느낀 점을 적다보니 글이 길어져서 제 블로그의 다른 포스팅으로 해당 부분은 대체하도록 하겠습니다 😊✈️ 3주간의 솔로 해외 여행23.09.06-23.09.24 🇹🇷 🇸🇮 🇭🇺20살 때 “대만”, “스페인 + 모로코”로 약 3주 간의 솔로 여행을 각각 다녀온 이후, 처음으로 나름의 장기(*비록 3주 정도 였습니다만* 😅) 솔로 해외 여행을 다녀왔습니다. 돌이켜 생각해보면 저는 고등학교 때부터 여행을 동경해왔던 것 같습니다. 제게 해외 여행은 새로운 문화를 배워올 수 있다는 선망의 행위 그 자체였습니다. 그러한 마음 가짐이 제가 성인이 된 이후 해외 여행을 포함한 여러 새로운 경험들을 추구하도록 만들었던 것 같습니다.제가 여행 중 만난 장기 여행객들은 크게 세 부류였습니다. 1) 고등학교 혹은 대학교 졸업 및 대학생. 2) 퇴사 및 퇴직 후 여행. 3) 휴가가 긴 외국인.저는 그 중 두 번째에 해당했습니다. 감사하게도 멋진 회사에서 새로운 도전을 할 수 있는 기회를 얻었고, 새로운 회사로 합류 전 약 한 달이 조금 넘는 시간이 주어졌습니다. 이 값진 시간을 허송세월 보낼 수 없었던 저는 맘 속에 품고 있었던 해외 여행을 가기로 결정하였습니다.동서양의 콜라보!, 유럽과 아시아를 잇는 이스탄불!, 난공 불락의 성벽 콘스탄티노플!, 세계 3대 미식의 나라!, 한국의 형제 국가! … 터키에 대해서 궁금한게 너무 많았고 굉장히 흥미로운 나라였기에 터키를 중심으로 여행을 가보자 라고 생각하였습니다.이번 여행의 가장 큰 목적은 20살의 김현우가 했던 솔로 여행과 비교했을 때 어떤 큰 차이가 있을까가 가장 궁금했고, 서구권에서 살아보고 싶은 입장으로써 나의 영어 실력은 얼마나 쓸모있을까를 확인해보고 싶었습니다.1. 20살때의 여행과 비교20살때 솔로여행에서 가장 기억에 남는 것은 사람들과의 관계 형성이었습니다. 어린 나이이기도 하였고 새로운 사람과의 친밀감 형성에서 많이 부족하고 어색했던 기억이 납니다. 저는 항상 그런 부분에서 당당해지고 싶었고 약 8년간 나름의 경험을 쌓아왔다고 생각했기 때문에 어떠한 변화가 생겼을까 가 궁금했습니다.결론부터 말씀드리면 “그렇게 다이나믹한 변화가 있지는 않았다” 입니다 😇다만, 다양한 사건들에 있어서 조금 더 유하게 받아들인다거나 그렇게 하려고 노력하는 부분이 성숙해졌다고 느꼈습니다. 예를 들어, 유럽권과 터키에서 제가 느낀 가장 큰 부분은 많은 사람들이 미소를 아끼지 않는다는 것이었는데요, 저 또한 여행 중 화가 나거나 짜증이 나는 상황에서도 미소를 지으려고 노력하며 기분을 긍정적으로 전환하려고 하였습니다. 또한 택시 기사님과 다툼이 있을때 내가 어떻게 했으면 서로 서로 좋은 상황을 맞이할 수 있었을까 라는 생각도 하며 조금 더 성숙한 생각을 하는 제 자신을 볼 수 있었습니다.전혀 일면식이 없는 새로운 사람을 맞이하는 일은 언제나 쉽지 않습니다. 다만 외국이라는 상황에서는 한국보다는 남의 시선을 덜 신경쓰는 것은 맞았던 것 같습니다.2. 영어실력의 변화저는 중 고등학교 때 부터 영어라는 과목에 가장 큰 흥미를 느꼈었습니다. 흥미라는 것이 단순 독해 지문 해석이나 듣기 평가는 아니었고 영어라는 언어 자체에 대해서 흥미를 가졌던 것 같습니다.20살 해외여행 때 부터 영어를 열심히 쓰려고 노력했으나, 번번히 제 자신의 부족함을 느껴왔습니다. 2023년 1월. 저의 꿈을 조금 더 구체화 했고, 캠블리라는 영어 회화 플랫폼 사용을 꾸준히 사용해왔습니다.또한 토스트마스터즈라는 영어 스피치 커뮤니티에 합류하여 주기적인 영어 스피치와 매주 커뮤니티 참석을 통한 영어 사용을 해왔습니다.이렇게 노력해왔던 나의 영어 실력이 이전과 비교하여 어떤 변화가 있는지 궁금했고 이번 여행에서 증명해볼 수 있었습니다.내가 바라는 목표에 달성하기 까지 아직 나의 영어 실력은 많이 부족하다.영어에 대한 사용 빈번도가 올라가며 사용하는 어구에 대한 유창함은 눈에 띄게 높아졌다고 느껴졌습니다. 하지만 제 실력은 아직 그 정도인듯 했습니다 🤣 네이티브들과 일상대화을 할 때, 깊은 주제에 대해서 영어로 대화할 때 아직 제 생각과 하고 싶은 말을 자유롭게 사용하지 못하더군요.또한 영어를 자유자제로 사용하기 위해서는 지금 공부 방법으로는 네이티브와의 간극을 좁히기 힘들겠다는 생각이 들었습니다.영어 실력 향상을 위해 시도해볼 것들 세계 경제 및 정치 시사 상식이 필요 영어 문법적인 요소들의 지식이 좀 더 필요. 필요한 어구들을 꺼내오는 능력들. 주변 모든 환경을 영어로. 혼자 생각하는 것 조차도 영어로! (나중엔 블로그도 영어로 한번… 😅)3. 여행을 통해 알게된 것 1) 튀르키예라는 나라는 내가 생각한 것 보다 훨씬 크고 발전된 나라다. 2) 아직 영어 실력이 많이 부족하다. 3) 혼자 여행하는 것이 마냥 길다고 좋은 것은 아니다. 사랑하는 사람과 함께 추억을 쌓고 싶다.☕️ Googler, Amazonian 과의 커피챗운이 좋게도 지인을 통해 Google 에서 엔지니어로 근무 중이신 정경태님과 Amazon에서 엔지니어로 근무 중이신 박주한님과 함께 커피챗이라는 명분하에 대화를 나눌 기회를 가지게 되었습니다.외국계 회사에 대한 정보가 많이 없는 저에게는 정말 소중한 시간이었고, 이번 분기 회고를 통해서 나누었던 대화들을 조금 정리해볼까 합니다. 이 자리를 빌어 바쁘신 와중에도 흔쾌히 시간을 할애해주신 두 분께 다시 한번 감사인사를 드리고 싶습니다 😊외국 빅테크 기업에서 근무하시는 분들과 긴 시간 동안 대화를 나눈 것은 처음이었습니다. 외국 빅테크 기업에 대한 업무 방식이나, 실제로 회사에서 생활하는 방식들이 조금은 미스테리였습니다. 제가 알고 있는 내용들은 전부 아티클이나 책 혹은 강연을 통해서 알게 된 것들이었기 때문에 실제로 그렇게 하고 있는지, 직원들은 어떻게 생각하는지, 어느정도 만족하는지 등 회사의 전반적인 생활에 대해서 너무 궁금했습니다.이런 저의 궁금증을 정말 명쾌하게 해결해주셨던 것 같습니다. 외국 빅테크 기업에서의 삶은 전반적으로 매우 만족한다고 말씀해주셨고, 스케일이 큰 업무들을 맡는다는 부분에 대해서 자부심을 가질 수 있다고 해주셨습니다. 또한 흔히 알려져있듯이 해당 기업들에는 역시 똑똑한 인재들이 정말 많다는 것 또한 확인할 수 있었습니다.마지막으로 개인적인 목표들을 여쭤보았습니다. 이 부분에서 저 또한 많은 영감을 얻을 수 있었는데요, 엔지니어로써 세상을 바꿀 수 있는 영향력을 고민하고 계셨습니다. 본인이 가지고 있는 능력을 활용하여 더 넓은 범위로 영향력을 확대하고 싶다는 말씀이 굉장히 와닿았던 부분이었습니다.초면임에도 불구하고 이렇게 기꺼이 시간 내주시고 이런 저런 질문들에 명쾌하게 답변해주시는 부분이 정말 감사했습니다.📢 Toastmaster Speech올 해부터 다시 토스트마스터즈 클럽을 참여하고 있습니다. 토스트마스터즈는 간단히 설명하자면 커뮤니케이션 능력과 리더십을 기르는 것을 목적으로 한 클럽인데요, 세계적인 조직이다 보니 모두 영어로 진행되며, 영어 스피치 세션을 가지고 있습니다. 영어 실력 향상 뿐만 아니라 대중 앞에서 발표하는 능력을 기르기 위해서 꾸준히 토스트마스터즈 클럽을 참여하려고 노력 중입니다.이번 3분기에는 한 번의 영어 스피치와 다른 역할로서 여러 번 참여를 하였습니다. (분기 회고를 하면서 몇 번의 스피치를 했는지 확인하게 되었는데요, 생각보다 적어서 놀랐습니다. 한 달에 한번은 했을 줄 알았는데요.. 😅)토스트마스터즈에 참여할 때마다 느끼는 것은 아직 나의 영어 실력은 많이 부족하고, 대중 앞에서 이야기하는 것은 여전히 쉽지 않구나 라는 것이었습니다. 하지만 이런 느낀점이 부정적으로만 다가오는 것은 아닙니다. 이런 생각은 저에게 더 많은 자극을 주고 영어 공부와 스피치 참여를 할 수 있는 기폭제가 되어줍니다.앞으로는 한 달에 한번은 스피치를 준비하여 발표할 수 있도록 더 노력해야겠습니다. 또한 몇 일전에 준비하는 것이 아니라, 조금 더 여유있게 안정적인 스피치를 만들어 보고 싶다는 생각이 있습니다.📚 읽은 책들1. 삼국지 시리즈새로운 회사로의 합류 전 시간이 조금 있어서 목표했던 삼국지 시리즈 (정비석 역) 를 읽었습니다. 역시나 삼국지는 재미있더군요. 생각보다 많은 것들을 배울 수 있었던 시간이었습니다. 다들 잘 아시다시피 삼국지에는 아주 많은 인물들이 나옵니다. 그들의 성격을 삼국지라는 책을 통해서 엿볼 수 있었고, 나는 어떤 영웅의 성격을 가지고 있는지, 내가 유비였다면 어떻게 행동했을지, 이런 상황에서 내가 조조처럼 행동했다면 어땠을까 라는 질문을 던지기도 했습니다. “삼국지를 세 번 읽은 사람과는 논쟁하지 말라” 이런 말이 괜히 나온게 아니겠다 라는 생각이 들었던 시간이었습니다 🤣🤣2. 유난한 도전 &amp;amp; 무엇이 성과를 이끄는가토스 뱅크 입사를 준비하면서 추천받은 도서 두 권이 있었습니다. 유난한 도전 , 무엇이 성과를 이끄는가. 두 권 모두 아주 재미있게 읽었고 많은 것들을 느낄 수 있었습니다.유난한 도전이라는 책은 토스팀의 성공과 실패 그리고 도전들을 흥미롭게 풀어놓았으며 어떻게 토스가 밑 바닥 부터 여기까지 올 수 있었는지, 그들의 마음가짐과 자세는 어떠했는지에 대해서 간접적으로 경험해볼 수 있었습니다. 이 책을 읽으면서 심장이 두근 두근 하기도 했습니다. 정말 일에 대해 이 정도의 열정을 가지고 한번 쯤은 일해봐야 하지 않겠어? 라는 생각도 하며 토스팀에 합류한 저의 모습을 상상하기도 했습니다.유난한 도전을 읽다 보면 토스팀의 핵심 가치 8가지가 나오는데요, 이 부분 또한 매우 흥미로웠습니다. 어떻게 이런 문화를 만들 수 있었을까, 이게 궁극적으로 팀에 어떤 영향을 불러일으킬까 라는 생각을 많이 할 수 있었습니다. 이 부분에 대해서는 두 번째 책인 “무엇이 성과를 이끄는가” 에서 아주 잘 설명되어 있었습니다.이 책을 읽고 많은 것들을 배울 수 있었기에 책의 내용을 바탕으로 개인적으로 참여 중인 박중근 멘토님의 멘토링 그룹에서 발표를 하기도 했습니다. (발표 자료가 궁금하신 분들은 여기를 참고해주시면 감사하겠습니다.)결론적으로 돈과 정서적 압박 그리고 타성과 같은 간접 동기는 성과를 떨어트리고, 즐거움, 의미, 성장과 같은 직접 동기는 성과를 올릴 수 있다는 것입니다. 토스팀이 정한 핵심 가치는 이 3가지 직접 동기를 끌어올리기 위해서 만들어졌다는 것을 알 수 있었습니다.3. 자기경영노트자기 관리 지침서에서 바이블로 불리는 피터드러커의 자기경영노트를 읽어보았습니다. 한국경제신문을 구독하고 있는데 어느 날 구독자 사은품이라고 이 책을 선물로 주시더군요 ☺️ 제목만 들어본 책이라서 읽어보고 싶었던 찰나에 여행을 다니면서 해당 책을 틈틈히 읽어보았습니다.자기경영노트는 경영자로써의 방향성에 조금 더 초점이 맞추어져 있었습니다. 하지만 저와 같은 주니어들에게도 많은 영감을 주는 책이었습니다. 예를 들어 경영자는 목표 달성을 위해서 직원들을 어떻게 동기부여하고 어떻게 고성과를 이끌기 위해서 노력하는지를 알게되고 제가 어떤 방향으로 움직여야 하는지에 대해서 생각해볼 수 있었습니다.가장 기억에 남는 내용은 Generalist vs Specialist 에 대한 내용이었습니다. 직접적으로 둘 간의 비교는 하지 않지만, 경영자로서 목표달성을 위해선 직원의 강점을 잘 활용할 수 있는 곳에 업무를 분배해야 한다는 이야기가 있었습니다. Generalist 를 추구하던 저에게 어떤 방향성이 더 도움될지 생각해볼 수 있는 계기였습니다.연장선에 있는 내용으로서 갤럽의 Strength Finder 는 들어보신 분들이 많으실 것 같습니다. 갤럽 프레스가 쓴 위대한 나의 발명 강점혁명 이라는 책에 더 관심을 가지게 되었습니다. 추후 시간이 될 때 해당 툴을 이용해서 저의 5가지 강점을 찾아보고 더 뾰족하게 하기위해서 노력해볼 생각입니다 😊Outro2023년 3분기는 태어나서 처음해보는 경험들도 많이 있었고 정신없이 흘러가는 시간들이었던 것 같습니다. 그럴 때일 수록 이렇게 회고글을 작성해보는 것은 저에게 큰 도움이 되는 듯 합니다.여러 이벤트들이 연달아 있으면 많은 생각들이 머릿속을 스쳐지나가곤 합니다. 이런 컨텍스트들을 꽉 잡아서 깊은 생각을 해보고 싶은데 그런게 쉽지는 않더군요 😅벌써 10월이 절반이나 지났지만, 나름대로 3분기 회고글을 마무리할 수 있어서 아주 기쁩니다 ㅎㅎ요즘은 또 저의 뿌리 라는 포인트에 빠져서 생각해보곤 합니다. 저는 윤소정 선생님의 글을 굉장히 좋아하는데요, 소정쌤의 글과 생각에 많은 영감을 받고 여러 긍정적인 영향을 받아왔던 것 같습니다. 몇일 전 소정쌤의 유튜브에 올라온 “당신의 롤 모델은 누구인가요?” 라는 영상이 있었습니다. 열심히 자기개발서적들을 읽다보면 여러 의견에 제 가치관이 흔들리는 경우를 많이 경험했는데요, 그럴 수록 제 자신의 굳건한 생각과 롤 모델과 같은 코어가 있어야겠다라는 생각을 많이 했습니다. 제가 따라가고자 하는 롤 모델은 누구인가를 고민해보는 요즘입니다 ☺️2023년 4분기도 이제 두 달하고 2주 정도 남았습니다. 저는 아마 새로운 회사에서 적응해가며 이리 저리 뛰어다닐 것 같습니다. 항상 인생에 큰 도움을 주시는 박중근 멘토님께 감사드리며 모든 분들의 멋진 4분기를 응원합니다 🌟" }, { "title": "🏢 첫번째 퇴사, 첫번째 이직", "url": "/posts/blog-newjourney/", "categories": "Blog", "tags": "blog", "date": "2023-10-03 00:00:00 +0900", "snippet": "Intro언제 어디서나 매번 느끼는 것이지만 시간은 정말 빠른 것 같습니다. 벌써 제가 첫번째 회사를 퇴사한지 1달이 지났습니다.저는 카카오엔터프라이즈라는 회사에 2020년 6월 인턴으로 입사하였으며, 10월부터 정규직으로 근무하였습니다. 음성 시스템 파트, ML 플랫폼 파트, 검색 클라우드 시스템 파트 등의 여러 팀에서 근무를 하였으며 2023년 8월 18일(공교롭게도 제 생일과 동일한 날짜네요 😄)을 마지막 근무로 퇴사를 하게 되었습니다.비록 외부적인 요인으로 퇴사를 고려하게 되었지만, 주니어로써 짧지 않은 시간을 카카오엔터프라이즈라는 회사에서 보냈고 좋은 기억들도 많이 쌓았습니다.또한 앞으로 제가 살아감에 있어서 이번 경험이 큰 도움이 될 것이라고 판단되었습니다. 그래서 이번 경험들을 바탕으로 제가 어떤 생각을 했었는지 기록해보고 싶어서 이번 회고 글에서는 회사의 경험에 대한 이야기보다는 그 때의 상황들과 제가 느꼈던 감정들에 초점을 맞춰보았습니다.🏢 첫번째 퇴사, 첫번째 이직1) 카카오엔터프라이즈의 희망 퇴직때는 바야흐로 2023년 6월. 재직 중이던 카카오엔터프라이즈에서 경영 악화로 인해 희망 퇴직을 실시하겠다고 공지가 나왔습니다. (관련 뉴스 기사)CEO는 사퇴를 했고 회사에서는 사내 임원 전부를 교체하겠다고 했습니다. 카카오엔터프라이즈는 저에게 첫번째 회사였고, 첫 사회 생활 경험이었습니다. 저는 희망 퇴직이나 권고 사직에 대한 이야기를 들었을 때 처음에는 믿지 못했습니다. “직원이 1200명이 가까이 되는 이렇게 큰 회사가? 에이, 설마 그게 말이 돼?” 라고 생각을 했었지만, 그 이야기들이 현실로 다가오기 시작했습니다.회사는 전체적으로 분해되기 시작했고, 클라우드 CIC와 검색 CIC 그리고 카카오엔터프라이즈 나머지로 나뉘었습니다.팀장님이 저희 팀을 모으고 이렇게 이야기했습니다.“여러분, 얼른 이직 준비하세요”충격적이었습니다. 조직장으로부터 이직 권유를 받는다? 상상도 하지 못했던 부분이었습니다. 지금 다시 생각해보면 팀장님이 정말 미래를 잘 내다본 것 같다는 생각이 듭니다.그렇게 저 뿐만 아니라 팀원 대부분이 이직 준비를 시작했습니다.2) 수 많은 이직 시도능력이 출중하셨던 혹은 미리 이직을 준비하고 계시던 몇 몇 분들은 6월 달에 바로 퇴사 후 이직을 진행했습니다. 그렇게 저 또한 여러 회사의 포지션을 알아보고 이직을 준비했습니다. 이때 제 포트폴리오도 한번 제대로 만들어보았구요.이직을 준비하면서 저는 이때 처음으로 나 자신에 대한 객관적 시선이 정말 많이 부족하구나 라고 깨달았습니다. 항상 저는 어느 정도의 자신감을 가지고 있었지만, 생각보다 현실은 냉혹했습니다. 정말 고민이 많았던 시기였습니다.하지만 돌이켜 생각해보면 이런 시점이 정말 필요했다고 느껴집니다. 반복되는 회사 생활에 익숙해지다보면 본인의 객관화가 부족해지는 상황이 발생하는 것 같습니다. 외부에서의 새로운 기회를 찾지 않고 내부의 상황들에 몰입하다보면 본인이 어떤 부분이 부족한지, 정말로 본인이 이루고자 하는 목표를 위해서는 어떤 부분에 초점을 맞춰야하는지 이런 생각들을 하기가 어려워지는 것 같습니다.회사 생활을 하면서 항상 그런 생각을 했습니다. “했던 것들 얼른 정리해야지. 항상 이직 준비해야지. 포트폴리오 만들어야지” 매번 생각과 다짐은 했지만 행동으로 옮기는 것이 어려웠습니다. 이번에는 강한 외부적인 요인으로 인해 어쩔 수 없이 매번 생각만 해왔던 것을 정리하게 되었습니다 😅어쨌든 이런 상황이 발생하지 않았다면 제가 언제쯤 포트폴리오를 정리했었을지 라는 생각도 들면서 긍정적인 영향도 받았다는 생각이 들었습니다.3) 검색 조직으로의 이동과 검색 엔진 공부열심히 이곳 저곳 이직을 준비하던 중, 카카오엔터프라이즈 검색 CIC 내에 자리가 생겼고, 검색 팀장님께서 저를 긍정적으로 봐주신 덕분에 검색 조직으로 자리를 옮길 수 있는 기회가 있었습니다.이때 사실 많은 고민이 있었지만, 이전 팀장님도 검색을 추천해주셨고 이직을 준비하는 회사들의 합격 또한 보장된 자리가 아니었기 때문에 저는 검색으로의 이동을 선택하였습니다. 그렇게 저는 7월 1일부터 검색 조직으로 합류하였습니다.카카오가 가지고 있는 검색 기술은 정말 대단했습니다. 다음 시절 때 부터 쌓아온 노하우는 물론이고, 대규모 서비스를 잘 지탱하고 있는 포털엔진의 검색 시스템은 배울 점이 정말 많은 부분이었습니다.뿐만 아니라 제가 속했던 검색 조직의 조직장님은 정말 배울 점이 많은 분이셨습니다. 기술적인 것은 물론이고 조직을 리딩하는 부분에 있어서 많은 것들을 배울 수 있었습니다. 본인이 주변 사람들의 도움을 얻으면서 성장했듯이 주니어들이 검색에서 좋은 시스템과 좋은 기술들을 잘 보고 배워서 좋은 방향으로 성장하는 것을 진심으로 원하고 계신다는 것을 느낄 수 있었습니다.여기서 Microsoft의 평가 방식이 떠올랐습니다. Microsoft에서는 인사 평가를 수행할 때 “당신은 타인의 성공에 얼마나 기여를 하셨나요?” 라는 질문으로 평가를 한다고 들었습니다. 다른 사람의 성장에 기여한다는 것은 타인의 성장 뿐만 아니라 본인에게도 긍정적인 영향을 끼친다고 생각합니다. 이러한 가치관을 가진 팀장님이 계신다는 것을 보고 많은 것을 느낄 수 있었습니다.다른 조직들에 비해서 검색 조직은 필수 불가결한 존재로 인식되었고, 나름 평화로운 시기를 보내고 있었기에 저 또한 검색 조직에서 제대로 공부하고 최선을 다해보자 라는 생각을 가졌습니다. 제가 잡은 하나의 큰 목표는 “2023년 하반기까지 정말 열심히 달려서 대규모의 검색 시스템을 잘 이해하고 사수의 지식을 다 따라잡겠다“ 였습니다.그렇게 검색 엔진 공부, 검색 시스템에 대한 이해 등을 하기 위해 노력하였고 파트 내 세미나까지 진행하였습니다. (발표 자료를 공유하고 싶지만 내부 정보가 있어서 공유하지 못하는 부분이 아쉽습니다.)그러던 어느 날 7월 14일 회사에서 희망 퇴직 공지가 나왔습니다. 검색 CIC 조직 또한 희망 퇴직 대상에 포함이 되어있었습니다. 무려 30프로 감원을 하라는 지시가 떨어졌습니다. (이 이야기는 팩트 체크가 되지 않은 찌라시였지만 회사 내 소문을 퍼트리기엔 충분히 자극적이었습니다.)이게 무슨 일..? 정말 데자뷰 같은 순간이었습니다. 5월 달에 이전 팀에서 겪었던 그 상황 그대로 검색 조직에서도 일어나고 있었습니다.이 회사에 더 이상 남아있을 수 없겠다는 생각이 들었고, 만약 남아있는다면 나에게 어떤 상황이 발생할까? 라는 질문을 자문해보았을 때 이득이 될 부분이 전혀 보이지 않았습니다.그렇게 저는 다시 한번 이직 준비를 시작하게 되었습니다.비록 짧은 시간을 검색 조직에서 보냈지만 어떤식으로 포털 검색 엔진과 여러 검색 서비스들이 운영되고 개발되는지 조금은 들여다볼 수 있는 기회가 되었습니다. 대규모 색인 데이터를 가진 검색 엔진의 경우 분산 시스템에 대한 이해도가 굉장히 중요했고, 러닝커브가 상당한 토픽이라는 것 또한 알게 되었습니다.4) 희망 퇴직과 이직 시도현재 상황에서 어차피 이직은 불가피하다는 판단을 하였고, 희망 퇴직을 신청했을 때 받을 수 있는 패키지가 저에게는 적지만은 않다고 느껴졌습니다.그렇게 저는 제 첫 회사 카카오엔터프라이즈의 퇴사를 결정하였습니다. 난생 처음 겪는 퇴사였기 때문에 희망 퇴직을 신청하는 마지막 버튼을 누를 때 느꼈던 그 망설임과 복합적인 감정, 잊을 수 없었던 순간이었던 것 같습니다.이 상황에서 저는 “누구도 본인을 지켜주지 않는다” 와 “지치면 나만 손해” 라는 말이 떠올랐습니다. 어느 누구도 저를 위해서 자신의 이익을 잃으며 대신 싸워주지 않고, 제가 지쳐 나가 떨어지면 그 시간 동안 발전하지 못하는 저만 손해라는 사실을 다시 한번 상기시켰습니다.제가 강해져야 했습니다. 제가 강하다면 희망 퇴직이든 뭐든 저를 필요로 하는 곳은 분명히 있을테니깐요. 이러한 상황을 겪으며 다양한 감정 변화를 느꼈고 성공과 자기 발전에 대하여 많은 자극을 받을 수 있었습니다.5) 새로운 시작을 준비결과적으로 저는 토스뱅크라는 회사에서 새로운 시작을 하게 되었습니다. 아직 입사는 하지 않았지만 재미있는 일들이 저를 기다리고 있는 것 같아 설레는 마음으로 시간을 보내고 있습니다.수 많은 회사에 지원을 하면서 합격 통보를 받은 곳도 있었지만 불합격도 물론 있었습니다. 이런 상황에서 알게 모르게 내면의 스트레스가 계속 쌓여왔었던 것 같습니다. 그럼에도 불구하고 제 옆에서 응원해주신 분들께 정말 감사한 마음을 가지고 있습니다.Outro이번 퇴사 및 이직 회고를 하면서 초점이 맞춰진 부분은 카카오엔터프라이즈라는 회사가 아닌 이직의 과정이었던 것 같습니다.혼자서 이 정도면 되겠지, 나 정도면 괜찮지 이런 착각을 불러일으키는 생각을 많이 했던 것 같습니다. 전문적인 위치에 올라가기 위해서는 나의 주관이 개입된 판단들을 배제할 필요가 있다고 느꼈습니다. 면접을 볼 때나, 다른 사람들과의 관계를 맺을 때나 목표를 수행하기 위해 가장 중요한 요소는 제 자신 스스로의 객관화였습니다. 제가 가진 역량들을 객관적인 시선으로 바라볼 필요가 있으며, 제가 원하는 위치로 가기 위해서 정확하게 어떤 것들이 필요하고 제가 어떤 것이 부족한지 알 수 있어야 했습니다.이직을 수행하며 수 차례 면접을 보았습니다. 제가 잘 알고있다고 생각했던 내용 조차도 쉽게 이야기하지 못하는 경우가 많았습니다. 또한 그들이 원하는 요구사항을 충족하지 못하는 경우도 수도 없이 많았습니다.제가 원하는 목표를 다시 한번 구체화하고 글로 써보아야 하며 그를 기반으로 제 자신을 객관적으로 판단할 수 있어야 한다라는 생각이 들었습니다.처음 경험하는 굉장히 특수한 상황에서 저에게 많은 도움을 주셨던 박중근 멘토님, 좋은 조언과 실질적인 피드백으로 큰 도움을 주신 테드와 레오, 이직 준비 과정에서 구체적인 피드백으로 도움을 주신 Staybility 의 상찬이 형에게 감사 인사드립니다. 마지막으로 제가 스트레스 받고 힘들어 할 때 늘 옆에서 힘이 되어준 짝꿍에게 고맙다고 이야기해주고 싶습니다." }, { "title": "[SRE] Ch18. Software Engineering in SRE", "url": "/posts/sre-ch18/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-30 00:00:00 +0900", "snippet": "결론 : SRE 는 단순 Operation 을 하는 포지션이 아니라 Software Engineering 에 많은 부분 기여를 하고 있으며, 이러한 역량과 작업은 매우 중요하다. 구글에서는 어떤식으로 SRE 의 Software Engineering 이 이루어지고 있는지 엿볼 수 있는 장SRE 조직의 소프트웨어 엔지니어링 역량이 중요한 이유 다른 회사들과 비교하여도 구글처럼 크고 복잡한 프로덕션 환경을 가지고 있는 회사는 없을 것이다. 이 때문에 구글에서는 다양한 형태의 소프트웨어 개발이 내부적으로 이루어질 수 밖에 없음. 구글이 필요로 하는 규모를 감당하는 서드파티 도구가 거의 없기 때문 그렇다면 구글의 SRE 조직이 내부 소프트웨어를 개발할 수 있는 조직이 된 이유는 무엇일까? 구글만의 프로덕션 환경에 대한 SRE 조직의 폭넓고 깊은 지식을 활용하여 소프트웨어를 디자인하고 개발 가능 SRE 들은 중요한 사안에는 모두 참여하므로 전체적인 큰 그림과 요구사항들을 쉽게 이해 개발하는 도구를 직접 사용할 사용자들과의 관계가 직접적이기 때문에 빠른 피드백 적용 가능 Auxon 사례 연구 : 프로젝트 배경 및 문제가 발생한 부분 Auxon 은 프로덕션 환경에서 실행되는 서비스의 Capacity 계획을 자동화하기 위해 만든 툴이다. 전통적인 수용량 계획이 존재하긴 하지만, 예측이 변경되고 배포가 지연되고 예산이 줄어드는 등 아주 다양한 변수가 발생하고 정확한 정답이 없는 문제임. 이는 노동집약적이고 모호하며, 본질적으로 불안정함.구글의 해결책 : 의도 기반 수용량 계획 구현이 아닌 요구사항을 명확히 하자. 의도(intent)란 서비스 담당자가 자신들의 서비스를 운영하고자 하는 의도를 의미함. 실제 수용량 계획 의도를 이끌어내기 위해서는 현실적인 자원 수요를 바탕으로 한 타당한 이유를 만들어내기 위해서는 여러 단계의 추상화가 필요함. Foo 서비스를 위해 X, Y 그리고 Z 클러스터에 50개의 코어가 필요합니다. → 왜? Foo 서비스를 위해 YYY 지역의 클러스터 중 세 개에서 50개의 코어가 필요합니다 → 덜 구체적이긴 하지만 왜? Foo 서비스에 대한 각 지역별 수요를 충당하기를 원하며 N+2 의 다중화를 원합니다. → 자유도가 훨씬 높아짐. 인간이 이해하기 쉬워짐. 하지만 왜 ? Foo 서비스에 99.999% 가용성을 지원하고 싶습니다 → 매우 추상화된 요구사항. 훨씬 유연함. → 3단계의 추상화를 제공할 떄 가장 좋은 성과를 내고, 아주 정교한 서비스는 4단계가 더 어울릴 수 있다.의도를 파악하기 위한 선행 작업 의존성 성능지표 우선순위 결정Auxon 소개 Auxon 은 사용자의 설정 언어 혹은 프로그래밍 API 를 통해 수집한 후, 사람의 의도를 기계가 이해할 수 있는 제약으로 변환함 성능 데이터 : 서비스의 규모를 의미 서비스별 수요 예측 데이터 : 예측된 수요 신호의 사용 궤적을 의미 자원 공급 : 기본적인 자원에 대한 기초 수준의 가용성에 대한 데이터 자원 가격 : 기본적인 자원을 기초적인 수준으로 확보하기 위한 비용에 대한 데이터 의도 설정 : 의도 기반 정보를 Auxon 에 전달하기 위한 핵심 Auxon 설정 언어 엔진 : 의도 설정으로부터 전달받은 정보를 바탕으로 동작 Auxon 해법 엔진 : 이 도구의 뇌에 해당하며 설정 언어 엔진에게서 수신한 최적화된 요청을 바탕으로 거대한 혼합 정수 혹은 선형 프로그램을 공식화함 할당 계획 : Auxon 해법 엔진의 출력 결과" }, { "title": "[SRE] Ch17. Testing for Reliability", "url": "/posts/sre-ch17/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-30 00:00:00 +0900", "snippet": "결론 : 테스트는 엔지니어들이 자신의 제품의 신뢰성을 향상시킬 수 있는 가장 적절한 투자 수단이다. 테스트의 종류는 굉장히 여러가지가 있는데, 신뢰성을 올리기 위해서 대략적으로 테스트의 종류와 중요도에 대해서 설명하고 있다. 테스트는 어떤 변경 사항이 일어났을 때 특정 부분에서 동일한 결과를 기대할 수 있다는 것을 보여주기 위한 메커니즘이다.소프트웨어 테스트의 종류전통적인 테스트 소프트웨어를 개발하는 동안 소프트웨어가 올바른 동작을 수행하는지 여부를 평가하는 테스트Unit Test 클래스나 함수 등을 테스트하여 각 동작을 독립적으로 테스트하는 방법 주로 TDD 방법론과 함께 사용됨 dependency 가 있을 때는 어떻게? → Dependency Injection 사용. Dagger 와 같은 툴을 사용해서 Mock 객체를 생성하여 수행 Integration Test 단위 테스트를 통과한 소프트웨어 컴포넌트는 그보다 더 큰 규모의 컴포넌트에 편입되는데, 이떄 수행하는 테스트가 통합 테스트임System Test 아직 배포가 완료되지 않은 시스템에 대해 엔지니어가 수행할 수 있는 가장 큰 규모의 테스트. Smoke Test 전자 회로 기판에 전원을 넣었을 때 기판에서 연기가 나는지 확인하는 테스트에서 유래됨. 즉, 안정성을 유지하기 위해서 주요 기능들이 제대로 작동하는지 확인 안정성 검사 (Sanity Test) 로 알려져 있으며, 약간의 장애 상황을 가정한 테스트를 포함해서 더 많은 부분을 테스트 함 Performance Test 성능 테스트 Regression Test 이미 알려진 버그들을 통해 시스템의 실패나 잘못된 결과가 나타나는 것을 유추하는 테스트. 프로덕션 테스트 실제 동작하는 웹 서비스에 대해 배포된 소프트웨어 시스템이 올바르게 동작 중인지를 판단하는 테스트 Black Box 테스트라고도 부름설정 테스트 실제 프로덕션 환경에서 동작하는 특정 바이너리가 실제로 어떻게 설정되어 있는지를 확인하고 해당 설정 파일과 일치하지 않는 부분을 찾아 보고 함스트레스 테스트 시스템을 구성하는 컴포넌트들의 한계를 이해하기 위함. 예시 DB 용량이 어느 정도에 다다르면 쓰기 작업에 실패하는가? 서버는 초당 몇 개의 쿼리까지 응답할 수 있는가? 카나리 테스트 서버의 일부만을 새로운 버전의 바이너리나 설정 파일로 업그레이드한 후, 일정 기간 동안 살펴보는 형태로 진행테스트 및 빌드 환경 구성하기 일반적으로 초기 설계 부터 테스트 커버리지 100프로를 달성하면서 프로젝트를 수행하는 경우 (TDD) 는 잘 없음. 뿐만 아니라 SRE 엔지니어들은 대부분 프로젝트가 한창 진행 중일 때 합류하기 때문에, 어느 부분부터 테스트를 시작해야하는지 판단하는 부분이 중요함 고려해야할 점들 어떤 형태로든 중요도를 측정해서 컴포넌트들의 우선순위를 결정해야함 사활이 걸려있거나, 비지니스 관점에서 중요한 기능이나 클래스를 특정할 수 있는지? 다른 팀들이 통합해서 사용하는 API들이 있는지? → 가장 강력한 테스트 문화 수립 방법은 지금까지 보고된 모든 버그를 테스트 케이스의 형태로 문서화하고, regression Test 에 포함시켜야함" }, { "title": "[SRE] Ch17. Testing for Reliability", "url": "/posts/sre-ch17/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-27 00:00:00 +0900", "snippet": "결론 : 테스트는 엔지니어들이 자신의 제품의 신뢰성을 향상시킬 수 있는 가장 적절한 투자 수단이다. 테스트의 종류는 굉장히 여러가지가 있는데, 신뢰성을 올리기 위해서 대략적으로 테스트의 종류와 중요도에 대해서 설명하고 있다. 테스트는 어떤 변경 사항이 일어났을 때 특정 부분에서 동일한 결과를 기대할 수 있다는 것을 보여주기 위한 메커니즘이다.소프트웨어 테스트의 종류전통적인 테스트 소프트웨어를 개발하는 동안 소프트웨어가 올바른 동작을 수행하는지 여부를 평가하는 테스트Unit Test 클래스나 함수 등을 테스트하여 각 동작을 독립적으로 테스트하는 방법 주로 TDD 방법론과 함께 사용됨 dependency 가 있을 때는 어떻게? → Dependency Injection 사용. Dagger 와 같은 툴을 사용해서 Mock 객체를 생성하여 수행 Integration Test 단위 테스트를 통과한 소프트웨어 컴포넌트는 그보다 더 큰 규모의 컴포넌트에 편입되는데, 이떄 수행하는 테스트가 통합 테스트임System Test 아직 배포가 완료되지 않은 시스템에 대해 엔지니어가 수행할 수 있는 가장 큰 규모의 테스트. Smoke Test 전자 회로 기판에 전원을 넣었을 때 기판에서 연기가 나는지 확인하는 테스트에서 유래됨. 즉, 안정성을 유지하기 위해서 주요 기능들이 제대로 작동하는지 확인 안정성 검사 (Sanity Test) 로 알려져 있으며, 약간의 장애 상황을 가정한 테스트를 포함해서 더 많은 부분을 테스트 함 Performance Test 성능 테스트 Regression Test 이미 알려진 버그들을 통해 시스템의 실패나 잘못된 결과가 나타나는 것을 유추하는 테스트. 프로덕션 테스트 실제 동작하는 웹 서비스에 대해 배포된 소프트웨어 시스템이 올바르게 동작 중인지를 판단하는 테스트 Black Box 테스트라고도 부름설정 테스트 실제 프로덕션 환경에서 동작하는 특정 바이너리가 실제로 어떻게 설정되어 있는지를 확인하고 해당 설정 파일과 일치하지 않는 부분을 찾아 보고 함스트레스 테스트 시스템을 구성하는 컴포넌트들의 한계를 이해하기 위함. 예시 DB 용량이 어느 정도에 다다르면 쓰기 작업에 실패하는가? 서버는 초당 몇 개의 쿼리까지 응답할 수 있는가? 카나리 테스트 서버의 일부만을 새로운 버전의 바이너리나 설정 파일로 업그레이드한 후, 일정 기간 동안 살펴보는 형태로 진행테스트 및 빌드 환경 구성하기 일반적으로 초기 설계 부터 테스트 커버리지 100프로를 달성하면서 프로젝트를 수행하는 경우 (TDD) 는 잘 없음. 뿐만 아니라 SRE 엔지니어들은 대부분 프로젝트가 한창 진행 중일 때 합류하기 때문에, 어느 부분부터 테스트를 시작해야하는지 판단하는 부분이 중요함 고려해야할 점들 어떤 형태로든 중요도를 측정해서 컴포넌트들의 우선순위를 결정해야함 사활이 걸려있거나, 비지니스 관점에서 중요한 기능이나 클래스를 특정할 수 있는지? 다른 팀들이 통합해서 사용하는 API들이 있는지? → 가장 강력한 테스트 문화 수립 방법은 지금까지 보고된 모든 버그를 테스트 케이스의 형태로 문서화하고, regression Test 에 포함시켜야함" }, { "title": "[SRE] Ch12. Effective TroubleShooting", "url": "/posts/sre-ch12/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-16 00:00:00 +0900", "snippet": "[SRE] 12. Effective Troubleshooting굉장히 흥미로웠던 장 중 하나였음.구글에서는 실제로 Production 환경에서 어떤 식으로 트러블 슈팅을 하는지에 대해서 알 수 있었고, 추후 운영 업무에서 도움이 될 수 있는 부분이 많았음.처음에 나오듯이 Trouble shooting 이라는 것은 필수적인 기술이지만, 설명하는 것은 굉장히 어렵다. 하지만 구글에서는 해당 장애 조치 능력 또한 학습과 가르침이 가능한 부분이라고 믿음.일반적으로 1) 장애 조치를 범용적으로 수행, 2) 시스템에 대한 탄탄한 이해 를 바탕으로 문제해결을 하는데, 첫번째 방법만으로는 효율적이지 않고, 효과가 없음이론장애 해결에서의 통상적인 문제 관련이 없는 증상을 들여다보거나 시스템의 지표의 의미를 잘못 이해. 결과만 쫒는 행동임 시스템의 변경이나 입력 값 혹은 환경에 대한 잘못된 이해. → 첫 번째, 두 번째 문제를 해결하려면, 시스템에 대해 더 많이 이해하고 분산 시스템에서 사용되는 공통 패턴에 대해 경험을 더 쌓아야함 가능성이 희박한 가설을 세우거나, 과거에 발생한 문제의 원인과 결부시킴 → 논리적 오류를 피해야 함 사실은 우연히 발생했거나 혹은 동일한 원인에 의해 발생한 관련 현상들을 계속해서 쫒아다니는 행위즉, 이런 문제들을 피하기 위해서는 장애에 대해 이해하는 것이 가장 먼저 수행해야하는 작업임실전문제 보고 모든 문제 해결은 문제에 대한 보고에서부터 시작함 효과적인 문제 보고는 아래와 같음 실제로 기대한 동작은 무엇인지 현재는 어떻게 동작하는지 문제가 되는 동작을 어떻게 재현할 수 있는지 일정한 양식으로 구성되어야 함 구글에서는 어떻게 전달 받든, 모든 이슈에 대해 버그를 오픈하는 것이 일반적우선순위 판단 문제 보고 다음 단계는 대처 방법을 찾는 것 우선 시스템이 가능한 정상적으로 동작하게 만들어야 함 ex. 버그로 인해 데이터 복구가 불가능할 정도로 손상을 입었다면, 방치하는 것보다 큰 손실을 막기 위해 시스템 중단을 해야 함. 문제 관찰 시스템 모니터링 (시계열 데이터) 로그진단단순화하기와 범위 좁히기 각 단계마다 테스트 데이터를 입력하고 올바른 결과가 출력되는지 확인하는 것은 효과적임 하지만 너무 오래 걸리는 큰 시스템이라면 Binary Search!무엇이, 어디서, 왜를 고민하기 증상, 이유, 어떤 지점에서의 이슈, 해결책.. 이런 방향으로 디버깅하기가장 마지막으로 수정된 부분에 주목하기테스트와 조치 어떤 것이 실제로 문제를 일으킨 근본 원인인지를 파악해야 함 실험적이지만, 가설을 하나씩 넣어보거나 혹은 하나씩 제거해 나가는 방법을 사용할 수 있음 또한 작업들과 결과에 대해서 꼼꼼히 기록해야함부정적인 결과의 마법 부정적인 결과는 무시해서도 안 되고 평가절하 해서도 안된다. 부정적인 결과로 끝난 실험 역시 결론이다. → 결과가 잘 나오지 않았다고 하더라도, 잘 정리하고 모아두면 (마이크로 벤치마크, 문서화된 안티 패턴들, 프로젝트 포스트모텀 등) 나중에 새로운 실험들에 도움이 됨. 자신의 결과를 공표하자 → 실패하더라도, 모든 사람들에게 이야기 하면서 배워나가야함 처방 해당 원인이 실제 문제의 원인인지를 증명해야 함 문제를 유발하는 요소들을 발견했다면 시스템에서 어떤 문제가 발생했는지, 문제를 어떻게 추적해냈는지, 어떻게 해결했는지 그리고 재발을 어떻게 방지할 수 있는지에 대한 노트를 작성 해야함 → 포스트모텀 문서를 작성해야함사례 연구 앱 엔진 사례 Dapper a Large-Scale Distributed Systems Tracing Infrastructure Frontend 리버스 프록시가 요청을 받은 시점부터 앱의 코드가 응답을 리턴하기까지 전 과정을 추적하면서 요청을 처리하는 각 서버들이 발행하는 RPC 요청을 살펴봄 ref. https://storage.googleapis.com/pub-tools-public-publication-data/pdf/36356.pdf " }, { "title": "[SRE] Ch11. Being On-Call", "url": "/posts/sre-ch11/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-15 00:00:00 +0900", "snippet": "플랫폼을 운영하면서 완전 Hardware 이슈를 제외하고 전반적인 운영업무를 진행했었다. 온콜을 정해서 운영하진 않았지만, 암묵적인 온콜이었다. 조금의 경험을 하고 구글의 SRE 가 어떤식으로 On-Call 에 대응하는지를 읽으니 아주 재미있었다.이 책을 읽으면서 배워야 하는 부분은, 구글은 이렇게 하는구나 정도로 시야를 넓힐 수 있는 부분에 집중해야할 것 같다.소개 구글에서의 여러 주요 서비스들 (검색, 광고, 메일 등)은 해당 서비스들의 성능과 신뢰성을 책임지는 전담 SRE 팀이 있음 SRE 는 Ops 와는 다르게 문제를 해결하기 위한 엔지니어링적 접근법을 강조함! 운영과 관련된 것들이지만, 소프트웨어 엔지니어링 솔루션 없이는 다루기 어려운 것들임 → 어떤 예시가 있을까? SRE 는 앞서 나온 것 처럼 50%의 엔지니어링 업무 + 50%의 운영업무를 수행함 운영 50% 중에서 25%는 온콜, 25%는 일반 운영업무 비상 대기 엔지니어의 삶 비상 대기 시에 엔지니어는 수 분 이내에 프로덕션 환경에서 필요한 운영 작업을 수행할 수 있어야 함. 매우 중요한 서비스는 5분 이내, 아닐 경우는 30분 이내 사용자에게 노출되는 경우에는 분기별로 99.99%의 가용성을 반드시 확보해야함 허용되는 다운타임은 분기별로 13분 정도 서비스의 SLO 에 따라서 다르긴 함. 품질의 균형 Post Moterm 뒤의 장에서 좀 더 자세하게 다룸. 장애 상황을 분석할 때 사람을 중심으로 하는 것이 아니라 사건을 중심으로 분석하고 회고함. → 큰 가치를 제공함 누군가를 비난하는 것이 아니라, 프로덕션 환경의 장애를 시스템적으로 분석하고 가치를 만듬. 사람의 실수를 줄이는 최고의 방법 중 하나는 자동화를 할 수 있는 부분을 선별하는 것 안전에 대한 고려 사람은 본인이 직면한 도전에 대해 크게 두 가지 방향으로 생각함 1) 직관적, 자동화적, 그리고 신속한 대응 2) 합리적, 집중적, 그리고 계획적이며 경험에 기반한 행위 SRE 운영에서도 장애 상황에서 1,2번에 따라 대응방식이 다름 둘 다 장점이 있지만 일반적으로 후자의 경우가 더 나은 결과를 도출해내며 계획에 따른 장애 조치가 가능함 감을 믿으면 데이터에 근거한 지원이 제대로 이루어지지 않을 수도 있음.부적절한 운영 부하에서 벗어나기 목표치 → 목표치가 정량화 되어야함. 일일 티켓의 수는 5개 미만 같은 예시 모니터링 호출 알림은 서비스의 SLO를 위협하는 증상이 발생하는 경우에만 보내져야함 그리고 대응 조치가 가능한 것들이어야 함 동시에 여러번 발생하고 관련된 것들은 묶어서 발신해야함 운영 업무의 부족 대응 운영 업무가 너무 없으면, 자만하거나 SRE 들간의 지식 간의 차이가 벌어질 수 있음 → 구글에서는 Disaster Revocery Training (DiRT) 를 열어서 며칠에 걸쳐 인프라스트럭쳐 시스템과 개별 시스템에 대한 테스트를 수행함 1년에 특정 주를 잡아서 수행함. ref. https://queue.acm.org/detail.cfm?id=2371516 넷플릭스의 Chaos Engineering ref. https://medium.com/@Netflix_Techblog/the-netflix-simian-army-16e57fbab116 " }, { "title": "[SRE] Ch10. Practical Alerting from Time-Series Data", "url": "/posts/sre-ch10/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-02 00:00:00 +0900", "snippet": "[SRE] Ch10. Practical Alerting from Time-Series Data구글에서 발전시켜온 보그몬 이라는 모니터링 시스템을 소개하는 챕터.시계열 데이터를 기반으로 시스템을 모니터링하는 방법과 어떤식으로 발전되어 왔는지에 대한 이야기가 나와있다.시계열 데이터를 통한 모니터링이 무엇인지 잘 파악하지 못했었는데, 프로메테우스를 서비스에서 적극적으로 사용했던 나로서는 책을 읽으면서 조금 더 깊은 이해를 할 수 있었다 . 모니터링은 게층구조의 가장 밑바닥에 있음. 안정적인 서비스를 운영하기 위해서는 반드시 필요한 기본 구성 요소이다.보그몬의 탄생 구글에서 2003년 보그가 탄생했고, 이를 바탕으로 새로운 모니터링 시스템인 Borgmon 이 완성됨 보그몬은 스크립트를 실행해서 장애를 탐지하는 것이 아니라, 공용 데이터 해설 형식을 이용함. 즉, 다량의 데이터를 수집하여 모니터링을 함. 화이트박스 모니터링임 프로메테우스와 매우 유사 보그몬의 쿼리도 Prometheus Query와 매우 유사 시계열 데이터를 위한 저장소 보그몬은 in-memory DB에 저장하고, 이를 다시 TSDB(Time-Series Database) 로 알려진 외부 시스템에 보관함 TSDB는 RAM보다는 상대적으로 느리지만, 저렴한 비용으로 더 많은 데이터를 보관할 수 있음. RAM에 보관하고, 시간이 지나면서 가비지 컬렉터가 오래된 항목부터 제거함알림 알림의 상태는 갈대처럼 나부낄 수 있음( 금세 다른 상태로 바뀔 수 있다는 의미) 이를 해결하기 위해서 최소 2번 이상 실행하도록 해서 알림을 보냄" }, { "title": "[SRE] Ch09. Simplicity", "url": "/posts/sre-ch9/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-08-01 00:00:00 +0900", "snippet": "Keep It Simple Stupid역시 심플한게 근본이고 최고이다. 뭐든지 심플하게 생각하고 심플하게 만들어야 한다.소프트웨어에서는 이런 철학이 더 더 강조되어야 하고 더욱 중요하게 생각해야한다고 느낄 수 있었던 장이었다. SRE의 임무를 한 문장으로 표현하면, 시스템의 신속함과 안정성 사이의 균형을 유지하는 것 이다.시스템의 안정성 vs 신속함 둘 모두 장단점이있다. 어떨 때는 실험적 코딩을 이용해 신속함을 위해 안정성을 희생할 수도 있다. 어쨌든 둘 간의 균형이 매우 중요함지루함의 미덕 소스 코드에서는 재미, 스릴, 퍼즐 같은 것이 전혀 없는 것이 바람직한 모습 There is No Silver bullet. 근본적인 복잡성과 돌발적인 복잡성을 구분하고 제거 및 해결해야함내 코드는 절대 포기하지 않을거야 말도 안되는 소리임 소프트웨어에서는 코드 한 줄 한 줄이 다 부채일 수 있음. 클린 코드에서도 나오듯이, 주석은 해롭다!최소한의 API “완벽함이란 더 이상 추가할 것이 없을 때가 아니라, 더 이상 걷어낼 것이 없을 때 비로소 완성된다” Simple is the BEST API를 최소화 하는 것은 소프트웨어 시스템의 간결함을 추구하기 위한 가장 기본적인 관점임모듈화 utils, misc 이러한 바이너리를 쓰거나 디렉터리를 쓰고 있다면, 좋지 않은 사례임. 잘 디자인된 시스템은 명확하고 분명한 범위의 목적을 가진 바이너리들로 구성됨" }, { "title": "[SRE] Ch08. Release Engineering", "url": "/posts/sre-ch8/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-31 00:00:00 +0900", "snippet": "릴리즈 엔지니어링이라는 용어와 포지션이 있다는 것이 신기했다.내가 아주 큰 규모의 프로젝트를 해보지 못해서 그런지는 모르겠지만, 일반적으로 SWE(Software Engineer) 가 SRE 업무는 물론 릴리즈 엔지니어링 까지 함께 해왔던 것 같다.사실 SRE 업무는 DevOps에 가깝기 때문에 조금 거리가 있겠지만, 릴리즈 정도는 SWE가 했던 것이 아닌가 싶긴하다.내가 MLOps 플랫폼 구축하고 운영할 때 했던 업무들을 생각해보면 아주 SRE + 릴리즈 엔지니어링에 가까운 역할이었다.누구든지 쉽게 배포를 할 수 있게 하기위해서 배포 자동화를 진행했고, 다른 사람들도 쉽게 자동화를 구축할 수 있게 세미나를 진행하는 등 그런 업무들을 수행했었는데, 구글에서는 릴리즈 엔지니어링이라는 포지션이 따로 있다는 것이 신기해서 재미있게 읽은 장이었다.다만, 구글의 툴들에 대한 설명이 있는 부분에서 모르는 용어들이 많이 나와서 빠르게 이해하지 못하는 부분들도 꽤 있는 장이었다.릴리즈 엔지니어의 역할과 철학 보유한 도구들을 사용해서 일관되고 반복 가능한 방법을 통해 프로젝트를 릴리즈하기 위한 최선의 방법들을 정의함.자기 주도 서비스 모델 자동화를 통해 개발자의 개입을 최소화빠른 릴리즈 주기 Push on Green 릴리즈 모델을 통해 매 빌드가 모든 테스트를 통과하면 배포를 하는 등 빠르게 진행해야함밀폐된 빌드 디펜던시가 없어야 함원리와 절차의 강제 확고환 원리 및 가이드를 통하여 해당 순서를 강제할 수 있도록 해야 함지속적 빌드와 배포 구글은 Rapid 라는 릴리즈 시스템을 개발하여 사용하고 있음 즉, 프로젝트의 CI CD 를 위한 시스템을 따로 개발해서 사용함 빌드, 브랜칭, 테스트, 패키징을 지원하고 있음설정 관리 기법SCM(Software Configuration Management) 이란? 형상관리. 소프트웨어의 변경사항을 체계적으로 추적하고 통제하는 것. 문서 변경 및 코드 변경에 대한 형상 관리 툴 Github 도 SCM 툴임 여기서는 git 과 같은 변경 내용 추적보다는 ansible 과 같은 툴로 설정 값을 여러 host에서 관리할 수 있는 부분을 이야기하는 것으로 보임설정 관리에는 여러가지 방법들이 있는데, 프로젝트 소유자가 여러 옵션들을 고려해서 어떤것이 적합한지 판단하고 사용해야함결론릴리즈 엔지니어링은 제품 개발 주기의 처음부터 반드시 릴리즈 엔지니어링 자원에 대한 여유를 확보해야함 이후에 도입할 때 드는 비용이 너무 큼" }, { "title": "[Linux] mmap() 에 대해서", "url": "/posts/linux-mmap/", "categories": "Linux", "tags": "linux, mmap, process, memory", "date": "2023-07-28 00:00:00 +0900", "snippet": "mmap() 에 대해서 Memory Mapped I/O는 마이크로프로세서(CPU)가 입출력 장치를 접근할 때, 입출력과 메모리의 주소 공간을 분리하지 않고 하나의 메모리 공간에 취급하여 배치하는 방식이다. -위키피디아-회사에서 검색 엔진 공부를 하고 있는데, 엔진 내부에서 데이터 사용을 위하여 Memory Store를 사용함.여기서 mmap 을 사용하여 데이터 파일과 메모리를 동기화 하여 사용하고 있는데, mmap 에 대해서 명확하게 이해하기 위해서 정리를 해 봄Memory Mapped I/O란? 표준 파일 입출력의 대안으로 Application 의 파일을 메모리에 맵핑할 수 있는 인터페이스. 메모리 주소 - 파일 단어 사이가 1대1 대응이 된다는 것 → 이를 통해서 Application(CPU) 에서는 데이터를 접근할 때 메모리에 상주하는 데이터처럼 메모리를 통해 파일에 직접 접근할 수 있음. 또한 메모리 주소에 직접 쓰는 것만으로 디스크에 있는 파일에 기록할 수 있음. mmap() mmap()을 호출하면 fd가 가리키는 파일의 offset 위치에서 len 바이트만큼 메모리에 맵핑하도록 커널에 요청한다.#include &amp;lt;sys/mman.h&amp;gt;void * mmap (void *addr, size_t len, int prot, int flags, int fd, off_t offset); addr addr가 포함되면 메모리에서 해당 주소를 선호한다고 커널에 알려줌 그저 힌트일 뿐이며 대부분 0을 넘겨줌 len fd 가 가리키는 파일의 offset 위치에서 len 바이트만큼 메모리에 맵핑하도록 커널에 요청함. prot 접근권한을 지정 맵핑에 원하는 메모리 보호 정책을 명시 PROT_NONE: 접근 불가 PROT_READ: 읽기 가능 PROT_WRITE: 쓰기 가능 PROT_EXEC: 실행 가능 flag 맵핑의 유형과 그 동작에 관한 몇 가지 요소를 명시 MAP_FIXED : mmap()의 addr 인자를 힌트가 아니라 요구사항으로 취급하도록 함 MAP_PRIVATE : 맵핑이 공유되지 않음을 명시. 파일은 copy-on-write 로 맵핑됨. MAP_SHARED : 같은 파일을 맵핑한 모든 프로세스와 맵핑을 공유 MAP_SHARED와 MAP_PRIVATE를 함께 지정하면 안됨. 반환 메모리 맵핑의 실제 시작 주소를 반환한다. fd를 맵핑하면 해당 파일의 참조 카운터가 증가한다. → 따라서 파일을 맵핑한 후에 fd를 닫더라도 프로세스는 여전히 맵핑된 주소에 접근할 수 있다. 예시 fd가 가리키는 파일의 첫 바이트부터 len 바이트까지를 읽기 전용으로 맵핑한다. void *p; p = mmap (0, len, PROT_READ, MAP_SHARED, fd, 0); if (p == MAP_FAILED) perror (&quot;mmap&quot;); mmap() 에 전달하는 인자가 맵핑하는 과정 How does mmap work on? mmap 실행 시, Virtual Memory Address에 file 주소 매핑 해당 메모리 접근 시 1) Page Fault Interrupt 발생 / 2) OS에서 File data 복사해서 Physical Memory 에 넣어줌 메모리 Read 시, 해당 물리 페이지 데이터를 읽음 메모리 Write 시, 해당 물리 페이지 데이터 수정 후, 페이지 상태의 dirty bit 가 1로 수정 됨 File Close 시, 물리 페이지 데이터가 File에 업데이트 됨 궁금한 점 Process의 메모리 공간에서 정확히 어떤 위치에 어떻게 매핑이 되는지? mmap 을 사용할 때 flag 의 MAP_SHARED 와 MAP_PRIVATE 에서 구분됨. MAP_SHARED 를 사용했을 때는 Process의 Shared Memory 영역에 매핑이 되고, MAP_PRIVATE를 사용하면 Private Memory 영역에 매핑이 되는데 Private Memory 영역은 프로세스의 Data 영역과 Heap 영역을 의미함. mmap으로 매핑된 파일 write 을 수행하고, 동시에 해당 파일을 read() 했을 때 동기화가 바로 되는 것인지? 예시) A 프로세스가 mmap() 으로 foo.txt 파일을 메모리로 읽어서 수정 작업을 진행함. 동시에 B 프로세스가 fopen() 으로 동일한 foo.txt 파일을 읽고 있음. 그럼 일관성이 유지되면서 동기화가 될까? → fopen() 과 mmap() 은 메커니즘이 다르기 때문에 race condition이 일어날 수 있음. mmap() 으로 file 을 읽고 수정했을 때, 실제로 해당 변경이 Disk에 있는 파일로 동기화 되는 타이밍은 아래와 같음 MAP_PRIVATE를 사용한 경우 수정 작업은 메모리에만 적용되며, 디스크 파일은 수정되지 않음 MAP_SHARED를 사용한 경우 메모리에 반영되면서, 프로세스들이 동시에 접근하는 파일 시스템을 통해 디스크 파일에도 자동으로 반영됨. munmap() mmap()으로 생성한 맵핑을 해제하기 위한 munmap() 시스템 콜을 제공함.#include &amp;lt;sys/mman.h&amp;gt;int munmap (void *addr, size_t len); 페이지 크기로 정렬된 addr에서 시작해서 len 바이트만큼 이어지는 프로세스 주소 공간에 존재하는 페이지를 포함하는 맵핑을 해제함. 맵핑 해제하고 다시 접근하면 SIGSEGV 시그널이 발생함. 성공 시 0을 반환, 실패 시 -1을 반환하고 errno 설정 예제 [addr, addr+len] 사이에 포함된 페이지를 담고 있는 메모리 영역에 대한 맵핑을 해제함. if (munmap (addr, len) == −1) perror (&quot;munmap&quot;); exit() 에 의해서 자동으로 munmap 이 수행되긴 하지만, 그래도 직접 해주자맵핑 예제#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;sys/mman.h&amp;gt;// 인자로 파일 이름을 받음 int main (int argc, char *argv[]){ struct stat sb; off_t len; char *p; int fd; if (argc &amp;lt; 2) { fprintf (stderr, &quot;usage: %s &amp;lt;file&amp;gt;\\n&quot;, argv[0]); return 1; } fd = open (argv[1], O_RDONLY); // 인자로 넘겨받은 파일을 연다 if (fd == −1) { perror (&quot;open&quot;); return 1; } if (fstat (fd, &amp;amp;sb) == −1) { // fstat : 주어진 파일에 대한 정보 반환 perror (&quot;fstat&quot;); return 1; } if (!S_ISREG (sb.st_mode)) { // 주어진 파일이 디바이스 파일이나 디렉터리가 아닌 일반 파일인지 점검 fprintf (stderr, &quot;%s is not a file\\n&quot;, argv[1]); return 1; } p = mmap (0, sb.st_size, PROT_READ, MAP_SHARED, fd, 0); // 맵핑 수행 if (p == MAP_FAILED) { perror (&quot;mmap&quot;); return 1; } if (close (fd) == −1) { perror (&quot;close&quot;); return 1; } for (len = 0; len &amp;lt; sb.st_size; len++) putchar (p[len]); if (munmap (p, sb.st_size) == −1) { perror (&quot;munmap&quot;); return 1; } return 0;}mmap()의 장점 read()와 write() 시스템 콜을 사용하는 것보다 mmap()을 이용해서 파일을 조작하는 것이 좀 더 유용하다. read, write 시스템 콜 사용할 때 발생하는 불필요한 복사를 방지할 수 있음. 사용자 영역의 버퍼로 데이터를 읽고 써야 하기 때문에 추가적인 복사가 발생함. (잠재적인 페이지 폴트 가능성을 제외하면) 시스템 콜 호출이나 컨텍스트 스위칭 오버헤드가 발생하지 않음 여러 개의 프로세스가 같은 객체를 메모리에 맵핑한다면 데이터는 모든 프로세스 사이에서 공유된다. lseek() 같은 시스템 콜을 사용하지 않고도 맵핑영역 탐색 가능 mmap()의 단점 메모리 맵핑은 항상 페이지 크기의 정수배만 가능하다. 예) 페이지 크기가 4k 이고 7byte를 맵핑하면 4089 byte가 낭비됨! 메모리 맵핑은 반드시 프로세스의 주소 공간에 딱 맞아야한다. 다양한 사이즈의 맵핑이 있다면 파편화가 일어남 메모리 맵핑과 관련 자료구조를 커널 내부에서 생성, 유지하는데 오버헤드가 발생한다. 이중 복사 제거 방법으로 방지할 수 있음 읽기 요청마다 표준 입출력 버퍼를 가리키는 포인터를 반환하는 대체 구현을 통해 데이터를 표준 입출력 버퍼에서 직접 읽을 수 있음 → 불필요한 복사 피함 → 맵핑하려는 파일이 크거나 (낭비되는 공간이 전체 맵팽에서 낮은 비율일 때), 맵핑된 파일의 전체 크기가 페이지 크기로 딱 맞아 떨어질 때 (낭비되는 공간이 없는 경우) mmap() 의 장점을 극대화할 수 있음Reference https://www.youtube.com/watch?v=8hVLcyBkSXY https://bannavi.tistory.com/80 시스템 프로래밍 책" }, { "title": "[SRE] Ch07. The Evolution of Automation at Google", "url": "/posts/sre-ch7/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-26 00:00:00 +0900", "snippet": " 흑인 예술에 견줄 수 있는 것은 자동화와 기계화 뿐이다.SRE 는 신뢰성을 관리하는 조직으로서 Human Error 와 불필요함을 요구하는 부분을 제거하는 작업은 필수적으로 보임! 또한 자동화를 통해서 주어진 방향으로 힘을 더할 수 있음 하지만, 정확률을 높여주는 것은 아님. 따라서 자동화도 잘 디자인 해야한다자동화의 가치일관성 정확하게 정의된 업무 범위와 정해진 절차를 수행하기 위해서 일관성은 매우 중요함!플랫폼 올바르게 디자인해서 구현된 자동 시스템은 확장성 있고, 이윤을 창출할 수도 있는 플랫폼을 제공함 ex. borg 플랫폼을 사용하면 휴먼에러를 줄이고, 실수를 중앙집중화할 수 있음더 신속한 수리와 더 신속한 조치 사람이 하는 것 보다 자동화를 통해 기계가 해당 이슈를 처리하는게 훨씬 빠름시간 절감 물론 시간 절감도 됨!구글 SRE의 가치 용이성과 확장성을 중요시 함 구글은 내부적으로 자동화 플랫폼 혹은 자동화 작업을 정말 많이 함! 외부에서 살 수 있어도 내부에서 구현하곤 함 그래서 k8s같은게 나올 수 있는 건가… 자동화의 사례 그냥 아무것이나 자동화를 해버리면 시스템만 늘어나고 관리 포인트가 늘어나게되면서 로드만 더 걸림. 따라서 잘 해야함 시스템 자동화의 단계 자동화를 하지 않는 단계 (작업자가 수동으로 실행) 별도로 관리되며 시스템에 특화된 자동화를 수행 (작업자가 시스템에 맞게 작성한 장애 대응 스크립트를 돌림) 별도로 관리되는 범용 자동화 수행 (모두가 함께 사용하는 범용 장애대응 스크립트로 수행) 내재화되었지만 시스템에 특화된 자동화를 수행 (시스템 자체적으로 스크립트 실행) 자동화가 불필요한 시스템을 도입 (사람 개입없이 자동으로 장애 대응) MySQL 예시와 클러스터 턴업 예시 → 적합성, 지연시간, 연관성을 중요시 하며 자동화가 진행됨 Hub and Spock" }, { "title": "[SRE] Ch06. Monitoring Distrubuted Systems", "url": "/posts/sre-ch6/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-25 00:00:00 +0900", "snippet": " 모니터링에 대한 정의를 할 수 있어서 좋았음. 어떤 것이 좋은 모니터링이고, 나쁜 것인지 판단할 수 있는 기준이 좀 더 명확해졌고 여러가지 모니터링 종류를 파악할 수 있어서 도움이 되었다.정의모니터링 쿼리의 수와 종류, 에러의 수와 종류, 처리 시간 및 서버의 활동 시간 등 시스템에 대한 정량적 실시간 데이터를 보여줌화이트박스 모니터링 시스템 내부 지표들을 토대로 하는 모니터링. 내부의 통계 지표 중요도가 낮은 서비스들에 대해서는 화이트박스 모니터링 수행 로그나 HTTP endpoint 와 같은 시스템의 내부 동작들을 규범에 따라 살펴보는 기법블랙박스 모니터링 사용자가 보게 되는 확인 가능한 동작들을 외부에서 테스트하는 과정을 의미 중요한 서비스들에 대해서는 블랙박스 모니터링을 수행 서버 상에 나타나는 증상을 기본으로 하며 현재 문제가 발생하는 상황을 모니터링 (발생 할 X) “시스템이 지금 현재 올바르게 동작하지 않고 있는” 상황 파악대시보드 중요한 지표들을 사용자에게 보여줌알림 노티근본 원인과 노드, 머신푸시 서비스에 대한 변경사항들모니터링의 필요성 장기적인 트렌드 분석 시간순 혹은 실험 그룹에 대한 비교 알림 당장 수정이 필요하거나, 살펴봐야할 때를 위해 대시보드 임시적인 회고 분석의 수행 (디버깅)→ 모니터링이 사람을 호출하는 경우 해당 부분은 비용이 정말 많이 드는 일임!!!(워치타워 너무 피곤하고 비용이 많이 든다.)모니터링에 대한 적절한 기대치 설정하기 모니터링 시스템에 있어 중요한 것은 최대한 간결하면서도 팀 모두가 이해할 수 있도록 유지하는 것 → 잘못된 알람 (noise) 비율을 낮게 유지하고, 시스템은 간결하며 안정적이어야 함 증상과 원인 증상 : 어떤 장애가 발생했는가 ex) HTTP 500 이나 404 오류 원인 : 왜 장애가 발생했는가 ex) 데이터베이스 서버가 연결 요청을 거부함 네 가지 결정적인 지표지연응답 요청이 서비스에 의해 처리되기까지의 시간을 말함트래픽 시스템에 얼마나 많은 요청이 들어오는지를 측정하는 것. 초당 HTTP 요청 개수, 성질 등 에러 실패한 요청의 비율 백엔드 프론트엔드의 케이스가 많이 다를 것 같음 우리 서비스에서 4XX 에러는 사용자 에러로 정의하고 응답을 받지않았음. 서비스 설계할 때 5XX 에러를 우리 서버 에러로 판단하였다. 다른 사람들은 어떻게? 서비스 포화 상태 서비스가 얼마나 포화 상태로 동작하는지를 의미함 리소스를 집중해서 측정해야 함 더욱 단순하게가 아니라 최대한 단순하게 모니터링 시스템은 일반적으로 고려할 것들이 매우 많기 때문에 복잡해짐. 사용자, 개발자, 기획자들의 요구사항들이 많기 때문에 시스템이 커질 수 밖에 없음. 따라서 모니터링 시스템을 디자인할 때는 최대한 간결함을 추구해야함 규칙은 최대한 간결하고 예측 가능해야함 수정 빈도가 높지 않은 데이터의 수집, 집계, 알림에 관련된 설정을 제거하자 수집은 되지만 사용되지 않는 데이터도 제거하자 결론 제대로 구성된 모니터링과 알림 파이프라인은 간결하고 명료함. 알림을 통해서 증상을 탐지하고, 문제를 디버깅하기 위한 방안을 제시하고, 원인 분석을 통해 스스로 학습하고 성장할 수 있는 기회를 제공해야함 이메일 알림은 그다치 큰 가치가 없고 소음처럼 간주되는 경우가 많음. 이메일로는 비교적 덜 심각하지만 현재 발생 중인 문제들을 모두 모니터링하는 대시보드로 제공하는게 좋음 " }, { "title": "[SRE] Ch05. Eliminating Toil", "url": "/posts/sre-ch5/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-19 00:00:00 +0900", "snippet": "삽질은 이제 그만!삽질 정의 부터 단순히 하고 싶지 않은 일만을 의미하는 것은 아님. 그렇다고 허드렛일이나 지저분한 일을 의미하는 것도 아님. 구글에서 정의하는 삽질을 확인해보자 수작업을 필요로 한다 반복적이다 자동화가 가능하다 사후 대처가 필요하다 가치가 지속되지 않는다 작업을 끝냈는데도 계속 같은 상태에 남아있는 경우 서비스가 커짐에 따라 O(n) 으로 증가한다. 삽질이 줄어들면 좋은 이유 구글에서는 삽질 시간을 50% 이하로 유지하는 목표를 가지고 있음. 삽질에 소요되는 시간이 많아지만, 엔지니어링에 쓸 수 있는 시간이 줄어들기 때문에 SRE의 목표와 부합하지 않음.엔지니어링에 해당하는 업무는??본질적으로 사람의 판단을 필요로 하는 업무! 창의적이고 혁신적 소프트웨어 엔지니어링 코드 작성 수정, 문서화 작업 자동화 스크립트, 프레임워크 개발 인프라 코드 개발 시스템 엔지니어링 시스템 설정 조절 모니터링, 수정 LB 변경 및 설치 삽질 서비스 운영과 직접 관련된 반복적인 수작업들 부하 서비스 운영과 직접 관련되지 않은 관리 업무들 삽질은 무조건 나쁠까?또 그렇진 않음. 왜냐면 빠른 시간 내에 처리 가능하고, 성취감이 있음. 예측이 가능하기도 하고 큰 무리없이 수행할 수 있음근데 너무 많아지면 (50% 이상) 문제가 있음 경력 개발 침체 의욕 저하 혼란 가중 성장 저하 인력 유출 신뢰에 문제 발생결론적으로 매주 조금씩 삽질을 걷어낼 수 있는 노력을 기울이면 서비스를 깔끔하게 유지할 수 있고, SRE 업무 (엔지니어링)에 시간을 투입할 수 있음" }, { "title": "[SRE] Ch04. Service Level Objectives", "url": "/posts/sre-ch4/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-18 00:00:00 +0900", "snippet": "서비스 수준 관련 용어척도SLI : Service Level Indicator→ 서비스 수준을 판단할 수 있는 몇 가지를 정량적으로 측정한 값 예) 응답 속도, 전체 요청 수 대비 에러율, 시스템 처리량 … 가용성 SRE가 중요하게 생각하는 SLI 중 하나 서비스가 사용 가능한 상태로 존재하는 시간의 비율 100% 는 실현 불가능하지만, 100%에 가까운 가용성은 얼마든지 달성 가능 목표SLO : Service Level Objectives→ SLI에 의해 측정된 서비스 수준의 목표 값 혹은 일정 범위의 값을 의미SLI ≤ 목표치 or 최솟값 ≤ SLI ≤ 최댓값 SLO를 설정하고 고객에게 이를 공개하는 것은 서비스의 동작에 대한 예측을 가능하게 함.협약SLA : Service Level Agreements→ SLO를 만족했을 경우 혹은 그렇지 못한 경우의 댓가에 대한 사용자와의 명시적 혹은 암묵적인 계약.(경제적인 것 혹은 다른 것으로 나타남) SLA가 필수적인 것은 아니지만 (ex. 구글의 검색기능), 서비스마다 SLI와 SLO를 설정하고 이를 토대로 서비스를 관리해야 함!!지표 설정진짜 중요한게 무엇일까?이전 셰익스피어 서비스를 다시 가져와보자SLI 몇 가지를 선정해보자 사용자가 직접 대면하는 시스템 → 가용성, 응답시간, 처리량 저장소 시스템 → 응답 시간, 가용성, 내구성 데이터 처리 시스템 → 처리량, 응답 시간 Common → 정확성 (Correctness)척도 수집하기 대부분의 지표들의 경우 평균보다는 분포가 중요함! 평균만 집계하면 0 혹은 Max 값의 경우 알 수 없음.척도의 표준화 각각의 척도들의 최우선 원칙이 무엇인지를 고민할 필요 없도록 SLI들에 대한 정의를 표준화 해야함 예. 집계 간격 : 평균 1분 집계 범위 : 하나의 클러스터에서 수행되는 모든 테스크들 측정 빈도 : 매 10초 … 목표 설정에 대한 실습목표 설정 명확성을 극대화하기 위해 SLO는 측정 방식과 유효한 기준이 반드시 명시되어야 함. 예. Get RPC 호출의 99%는 100ms 이내에 수행되어야 한다. 목표치 선택하기 SLI 와 SLO 를 어떻게 설정하는지는 제품과 사업에 영향을 미친다. 목표치 선택에 도움되는 몇 가지 조언 현재의 성능을 기준으로 목표치를 설정하지 말 것 최대한 단순하게 자기 만족 X 가능한 적은 수의 SLO 설정 처음부터 완벽하게 하려고 하지 말 것 SLO는 반드시 사용되어야 함 → 사용자가 어떤 점을 중요하게 생각하고 있는지를 반영하기 때문 다만, 너무 높은 SLO는 팀이 힘들어짐 따라서 현명하게 잘 활용해야함측정하기 시스템의 SLI들을 모니터링하고 측정하기 SLI를 SLO와 비교해서 별도 대응이 필요한지 판단하기 대응이 필요한 경우 어떻게 대응할지 판단 대응" }, { "title": "[SRE] Ch03. Embracing Risk", "url": "/posts/sre-ch3/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-14 00:00:00 +0900", "snippet": "[SRE] Ch03. Embracing Risk일반적으로 구글의 서비스를 많이 사용하는 유저라면, 구글이 100% 신뢰할 수 있으며 장애가 절대로 일어나지 않는 서비스들을 구축했을 것이라 생각할 수 있다!하지만 신뢰성을 극대화하면 기능 개발 속도나 제품 출시 기간에 제동을 걸게 되며 비용이 상승하여 방해가 된다고 한다.사용자 경험이란 모바일 네트워크나 일반적인 네트워크와 같은 신뢰성이 낮은 컴포넌트들에 의해 좌우되기 때문에, 사용자들은 높은 수준의 신뢰성과 극대화된 신뢰성의 차이를 알아치리지 못한다.따라서 SRE는 Uptime 을 극대화하기보다, 서비스가 다운될 수 있는 위험 요소와 빠른 혁신과 효과적인 서비스 운영 사이의 균형을 찾고 전체적인 서비스 만족도를 향상시키는 것에 집중하고 있음위험 요소 관리하기시스템 구축 비용은 신뢰성의 증가에 비례해서 증가하는 것이 아니다. 신뢰성 향상으로 비용이 100배 증가하는 경우도 있다!따라서 구글에서는 가용성 목표치를 99.99%로 설정하면 이를 초과 달성하려고 노력하기는 하지만, 넘치게 초과하려고는 하지 않음.넘치게 초과하려고 하다보면 기술 부채와 운영 비용을 줄이기 위한 기회를 잃어버리는 경우가 있음서비스 위험 측정하기 시간을 기준으로 한 가용성 종합 가용성구글은 Fault Isolation 방식을 사용하기 때문에 특정 서비스의 트래픽 중 일부는 세계의 어느 한 지역에 제공하고 있음. 따라서 전체 시간 중 최소 일부는 정상 동작 중이라는 의미그렇기 때문에 시간을 기준으로 한 가용성은 의미가 없고, 종합 가용성을 고려함서비스의 위험 수용도Risk Tolerance (위험 수용도) 는 소비자를 대상으로 하는 서비스와 Infra 서비스의 경우 기준이 달라짐 소비자 대상 서비스의 위험 수용도 정의와 인프라스트럽처 서비스의 위험 수용도 정의는 다름 목표 가용성 수준 장애의 종류 비용 에러 예산 활용해보기 SRE의 실적은 서비스 신뢰성이 차지하는 비중이 큼. 따라서 많은 양의 배포 사항을 배제하려는 경향이 있음. 하지만 개발자는 빠르게 배포하고 싶어함. 이 둘 간의 간극에서 대립이 발생함. 의견 대립의 예시 소프트웨어 결함 허용 유연성 충분한 테스트 출시 빈도 카나리 테스트 빈도와 규모 서로 균형을 맞춰야 함!장점 에러 예산을 도입하고 SLO를 설정하면, 제품 개발팀과 SRE팀이 혁신과 신뢰성 사이의 올바른 균형을 찾을 수 있음 즉, 시스템이 SLO 기준에 부합하면 릴리즈하고, 그렇지 않으면 중단 후 개발과 테스트에 자원을 투입!" }, { "title": "[SRE] Ch02. The Production Environment at Google, from the Viewpoint of an SRE", "url": "/posts/sre-ch1/", "categories": "Study, Book, SRE", "tags": "sre, google, devops, study, book, reliable", "date": "2023-07-08 00:00:00 +0900", "snippet": "Hardware 구글의 대부분의 컴퓨터 자원은 구글이 전원 공급, 냉각 기능, 네트워크 및 컴퓨터 하드웨어 등을 모두 직접 디자인한 데이터센터에 있음 Machine 하드웨어 (or VM) 을 의미 Server 서비스를 구현하는 소프트웨어를 의미 구글 데이터센터의 Toplogy 수십 대의 머신이 랙에 장착되어있음 랙은 일렬로 늘어서 있음 하나 또는 여러 랙이 클러스터를 구성 데이터 센터 선물 내에는 여러 개의 클러스터가 있음 근거리의 여러 DC 건물이 모여 캠퍼스를 이룸 Hardware를 조율하는 시스템 소프트웨어머신 관리하기 Borg Large-scale cluster management at Google with Borg https://research.google/pubs/pub43438/ Kubernetes의 전신 Apache Mesos와 유사한 분산 클러스터 운영 시스템 해당 시스템을 이용해서 머신을 관리함 BNS (Borg Naming Service), 리소스 할당 등등.. Storage 가장 낮은 D 계층은 클러스터 내의 거의 모든 머신이 실행하는 파일 서버. D 계층 위의 Colossus 는 GFS(Google File System)의 후속 제품으로서 전통적인 파일 시스템처렁 동작하며 복제와 암호화까지 지원 DB와 유사한 서비스들 Bigtable : 페타바이트 크기의 DB를 처리할 수 있는 NoSQL. 데이터가 분산되어 영구적으로 저장됨. Eventually Consistent 한 DB Spanner : 실시간 일관성이 있어야 하는 곳에 SQL 인터페이스 제공 Blobstore : 대량의 비정형 데이터를 저장할 수 있음 Networking Bandwidth Enforcer (BwE) 가용 대역폭을 관리해서 평균 가용 대역폭을 극대화 함 GSLB (Global Software Load Balancer) DNS 요청에 대한 지역 로드밸런싱 수행 사용자 서비스 수준에서의 로드밸런싱 수행 RPC 수준에서의 로드밸런싱 기타 시스템 소프트웨어잠금 서비스 Chubby 분산 Lock 시스템으로 분산된 서버들 사이에서 발생할 수 있는 자원 공유 문제 해결 Paxos 프로토콜 사용 Consensus Algorithm RAFT 모니터링과 알림 Borgmon 모니터링하는 서버들로부터 다양한 지표들을 수집 소프트웨어 인프라스트럭쳐구글의 소프트웨어 아키텍처는 하드웨어 인프라를 최대한 효과적으로 활용할 수 있도록 디자인됨. Stubby 내부 RPC 통신을 위해 만든 시스템 오픈 소스로 gRPC를 만듬 RPC (Remote Procedure Call) RPC 간의 데이터 전송은 Protocl Buffers 를 이용함. Protobufs 는 Apache의 Thrift와 유사 Protobufs 는 XML 을 이용해 구조화된 데이터를 직렬화하는 방식에 비해 더 많은 장점을 제공. 사용하기 더 쉽고, 크기가 3~10배 더 작고, 20~100배 더 빠르고 명료함 Reference https://sre.google/sre-book/table-of-contents/" }, { "title": "[데이터 중심 애플리케이션 설계] Ch1. Reliable, Scalable, and Maintainable Applications", "url": "/posts/data-intensive-ch1/", "categories": "Study, Book, Designing Data Intensive Application", "tags": "data, data-intensive, 맷돼지책, study, book, reliable", "date": "2023-06-27 00:00:00 +0900", "snippet": "오늘날 많은 애플리케이션은 Compute-Intensive 와는 다르게 Data-Intensive 적이다.즉, CPU 성능보다는 데이터의 양, 데이터의 복잡도, 데이터의 변화 속도가 더 큰 문제이다.일반적인 데이터 중심 애플리케이션은 아래와 같은 컴포넌트들이 필요하다. Database Cache Search Index Stream Processing Batch Processing이러한 시스템을 위한 여러 도구들은 다양한 Use Case 에 최적화됐기 때문에 더 이상 전통적인 분류에 딱 맞지 않다. 예를 들어 Kafka 와 Redis 는 데이터를 다루는 도구이지만, 사용되는 목적이 전혀 다르다.따라서 이러한 도구들을 엔지니어링 관점에서 신중하게 신뢰성, 확장성, 유지보수성을 고려하여 사용해야한다!이번 장에서는 신뢰성, 확장성, 유지보수성에 대해서 자세히 알아보자Reliability소프트웨어에 대한 사용자의 일반적인 기대치는 아래와 같다. 사용자가 기대한 기능을 수행 사용자의 실수나 예상치 못한 사용법을 허용 성능은 필수적인 사용 사례를 충분히 만족 허가되지 않은 접근과 오남용을 방지즉, 이 모든 것들은 올바르게 동작함을 의미한다.→ 무언가 잘못되더라도 지속적으로 올바르게 동작함Fault잘못될 수 있는 일을 fault 라고 부른다.Fault Tolerant (or Resilient) 결함을 예측하고 대처할 수 있는 시스템을 의미 하지만 모든 결함을 견딜 수 있는 시스템은 불가능함! (ex. 블랙홀)Failure 와 Fault 의 차이 장애는 사용자에게 필요한 서비스를 제공하지 못하고 시스템 전체가 멈춘 경우 결함은 사양에서 벗어난 시스템의 한 구성 요소Chaos Monkey (chaos engineering) 고의적으로 결함을 유도해서 내결함성 시스템을 지속적으로 훈련하고 테스트하는 방법 넷플릭스에서 이런식으로 수행함Hardware Faults일반적인 디스크 고장, 메모리 고장 등과 같은 이슈들을 말함(하드디스크의 평균 장애시간은 10~50년인데, 이는 10,000개의 디스크로 구성된 클러스터에서 평균적으로 하루 1개의 디스크가 죽는다는 의미)아래의 방법으로 장애율을 줄일 수 있음 RAID 구성 Redundant Array of Independent DIsk 인프라를 구축할 때 여러 개의 Disk 를 묶어서 사용하는 기법 이중 전원 디바이스와 Hot-swap 가능한 CPU 사용 데이터센터의 건전지와 예비 전원용 디젤 발전기 갖추기Software Errors하드웨어의 결함은 무작위적이고 독립적이라고 생각하지만, 소프트웨어는 그렇지 않음.소프트웨어의 오류 문제는 신속한 해결책이 없다. 따라서 빈틈없는 테스트, 프로세스 격리, 재시작 허용, 모니터링, 분석하기 등의 방법이 도움이 될 수 있음Human Errors 잘 설계된 추상화, API … Sandbox 사용 철저한 테스트 (corner case) 빠른 Roll back and Roll out 모니터링 조작 교육과 실습Scalability확장성을 논한다는 것은 “X는 확장 가능하다” 가 아니라 “시스템이 특정 방식으로 커지면 이에 대처하기 위한 선택은 무엇인가?” 와 “추가 부하를 다루기 위해 계산 자원을 어떻게 투입할까?” 같은 질문이다.Describing LoadLoad Parameter부하는 부하 매개변수라고 부르는 몇 개의 숫자로 나타낼 수 있다. 웹 서버의 초당 요청 수 데이터베이스의 읽기 대 쓰기 비율 대화방의 Active User 수 캐시 적중률Twitter 의 예시 트윗을 작성하는 것보다 조회하는 요청이 수백 배 많음! 따라서 트윗을 작성할 때 1번처럼 홈에서 트윗을 읽을 때마다 일치하는 값들을 조회하는 것이 아니라, 2번처럼 트윗 작성 시에 유저들의 데이터 리스트에 삽입을 해주는 것이 더 효율적! 하지만 Worst Case 처럼 Follwer 가 3천만명이 넘는 경우는….? → 1번 방식을 Hybrid 로 사용! Describing Performance시스템 부하를 기술하면 부하가 증가할 때 어떤 일이 일어나는지 조사할 수 있음. 부하 매개변수를 증가시키고 시스템 리소스는 유지하면 어떤 영향을 받나? 부하 매개변수를 증가시켰을 때 성능이 변하지 않고 유지되길 원한다면 자원을 얼마나 늘려야 하는가?이를 위해서는 성능 수치가 필요함!Throughput Hadoop 같은 배치 처리 시스템은 보통 초당 처리할 수 있는 레코드 수나 일정 크기의 데이터 집합으로 작업을 수행할 때 걸리는 시간에 관심을 가짐Response Time 온라인 시스템에서는 클라이언트가 요청을 보내고, 응답을 받는 사이의 시간이 더 중요함 Latency vs Response Time Latency 는 요청이 처리되길 기다리는 시간. Response Time 은 클라이언트 관점에서 본 요청 처리 실제 시간 값이 아닌 분포로 생각하기클라이언트가 몇 번이고 반복해서 동일한 요청을 하더라도 매번 응답 시간은 다름!! 따라서 응답 시간은 단일 숫자가 아니라 분포 값으로 생각해야함!위의 그래프에서 볼 수 있듯이 동일 요청에서도 Outlier가 존재함. 아래의 다양한 변수 때문 백그라운드 프로세스의 Context Switching 네트워크 패킷 손실과 TCP 재전송 Garbage Collection Pause Disk IO Page Fault 서버 랙의 기계적인 진동전형적인 응답 시간을 알고 싶다면 산술 평균 을 사용하는 것은 좋은 지표는 아니고, 일반적으로 평균보단 백분위를 사용하는 편이 좋음Tail Latency아마존의 예시!! 99.9 분위 (즉, 1000건의 요청 중 가장 느린 1건)을 최적화 하는 작업은 좋은 결과를 가져왔음. 보통 응답 시간이 가장 느린 요청을 경험한 고객들은 가장 많은 구매를 하는 고객들이었음. 하지만 99.99분위 (10,000 건의 요청 중 가장 느린 1건) 을 최적화하는 작업은 비용 대비 충분한 이익이 없었음SLO와 SLA SLO (Service Level Objective) SLA (Service Level Agrement) 백분위는 위의 값에 자주 사용 됨. 예시. 응답 시간 중앙값이 200밀리초 미만이고, 99분위가 1초 미만인 경우 정상 서비스 상태로 간주하며 서비스 제공 시간은 99.9% 이상이어야 한다. Approches for coping with Load One-Size-Fits-ALL 확장 아키텍쳐는 없다!아키텍쳐를 결정하는 요소는 읽기의 양, 쓰기의 양, 저장할 데이터의 양, 데이터의 복잡도, 응답 시간 요구사항, 접근 패턴 등이 있다. 이에 맞게 Scale up, out, elastic 하게 아키텍처를 자주 재검토 해줘야 함Maintainability 이상적인 소프트웨어 엔지니어링 workload 차트소프트웨어 비용의 대부분은 초기 개발이 아니라 지속해서 이어지는 유지보수에 들어간다.유지보수는 필수불가결하지만, 유지보수의 고통을 최소화 하고 레거시를 만들지 않게끔 소프트웨어 설계는 가능하다.이를 위한 원칙 세 가지는 아래와 같다.운용성: 운영의 편리함 만들기시스템이 지속해서 원활하게 작동하려면 운영팀이 필수! 좋은 운영팀은 일반적으로 다음과 같은 작업 등을 책임짐 시스템 모니터링 + 서비스 복원 장애, 성능 저하 원인 추적 보안 패치 등 최신 상태로 유지 배포, 설정 관리 등을 위한 도구 마련 보안 유지보수 운영 및 서비스 유지를 위한 절차 정의 개인 인사 이동에도 시스템에 대한 조직의 지식을 보존단순성: 복잡도 관리복잡도는 다양한 증상으로 나타남. 상태 공간의 급증, 모듈 간 강한 커플링, 복잡한 의존성, 일관성 없는 명명과 용어, 성능 문제 해결을 목표로 한 해킹, 임시방편으로 문제를 해결한 특수 사례 등→ 이를 해결하기 위한 최상의 도구는 추상화!좋은 추상화는 깔끔하고 직관적인 괴관 아래로 많은 세부 구현을 숨길 수 있고, 재사용이 가능!!예시. 고수준 프로그래밍 언어는 기계 언어, CPU 레지스터, 시스템 호출을 숨긴 추상화임. SQL 은 디스크에 기록하고 메모리에 저장한 복잡한 데이터 구조를 숨긴 추상화발전성: 변화를 쉽게 만들기Agile 작업과 TDD, Refactoring 은 자주 변화하는 환경에서 도움이 됨.데이터 시스템 수준에서 민첩성을 언급할 때는 민첩성 대신 다른 단어로 발전성을 사용할 예정!Reference https://www.semanticscholar.org/paper/Research-on-Software-Maintenance-Cost-of-Influence-Ren-Xing/c937040f76a3cbab2d0eb3a6eaabda97c55160a3 https://github.com/jeffrey-xiao/papers/blob/master/textbooks/designing-data-intensive-applications.pdf" }, { "title": "[etcd] [Docs Learning] etcd versus other key-value stores", "url": "/posts/etcd-learning-key-value-stores/", "categories": "etcd, docs", "tags": "etcd, docs, learning, kv, api, keyvalue", "date": "2023-06-26 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.Docsetcd 는 /etc + distritubed system 에서 유래했으며, configuration 데이터를 저장하기 위해서 만들어졌는데 여기다가 분산시스템을 더해서 etcd가 되었다!network partition 을 절대 허용하지 않으며, 가용성을 기꺼이 희생함Use cases Container Linux by CoreOS Locksmith 에서 etcd 를 사용해서 분산 세마포를 구현해서 클러스터의 하위 집합만 리붓되도록 함 Kuberentes 에서 데이터를 저장하기 위해 사용됨Comparison chartZooKeeper distributed system coordination and metadata storage 를 해결 etcd가 zookeeper 에서 구현된 좋은 점들을 많이 사용함 자체 RPC 프로토콜을 사용하는데, 언어 등등 불편한 부분이 많음 zookeeper 고려하는 사람들! etcd 써라Consul Service Discovery란? MSA 환경에서 서비스 간 호출이 잦음. 근데 클라우드 환경에서는 컨테이너 기반이기 때문에 서비스의 IP가 동적으로 변경되는 일이 많다. 이때 서비스 클라이언트가 서비스를 호출할 때 서비스의 위치 (즉, IP주소와 포트) 를 알아낼 수 있는 기능이 필요한데, 이것이 Service Discovery 임. e2e service discovery framework consul 1.0 에서 scale out 이 안됨 etcd 와는 다른 문제를 푸는 플랫폼인데, e2e cluseter service discovery 를 찾는다면 consul 도 괜찮은 선택!NewSQL (Cloud Spanner, CockroachDB, TiDB) data center 끼리의 horizontally scale out 을 의도한 DB 들 몇 GB 이상의 데이터를 저장하거나, full SQL 쿼리가 필요하면 NewSQL 써라Using etcd for metadata응용프로그램이 프로세스를 조정하는 것과 같이 메타데이터 또는 메타데이터 순서를 주로 고려하는 경우 등을 선택.애플리케이션에 여러 데이터 센터에 걸쳐 대규모 데이터 저장소가 필요하고 강력한 글로벌 주문 속성에 크게 의존하지 않는 경우 NewSQL 데이터베이스를 선택Using etcd for distributed coordination이론적으로, 강력한 일관성을 제공하는 모든 스토리지 시스템 위에 이러한 기본 요소를 구축할 수 있지만, 알고리즘은 조금 미묘하다. 작동하는 것처럼 보이는 Lock 알고리즘을 개발하는 것은 쉽지만 갑작스럽게 중단될 수 있음.게다가, 트랜잭션 메모리와 같은 etcd에 의해 지원되는 다른 기본 요소들은 etcd의 MVCC 데이터 모델에 의존한다. 즉, 단순한 강한 일관성으로는 충분하지 않음Distributed Coordination 의 경우 etcd를 선택하면 운영상의 문제를 예방하고 엔지니어링 작업을 절약할 수 있음!" }, { "title": "[K8S] Termination of Pods", "url": "/posts/k8s-termination-of-pod/", "categories": "Kubernetes", "tags": "kubernetes, k8s, pod, lifecycle", "date": "2023-06-22 00:00:00 +0900", "snippet": "Pod Creation | vPod Scheduling | vPod Initialization | --&amp;gt; Init Containers Start | | | | | v | | Init Containers Running | | | | | v | | Init Containers Terminated | | | vPod Running | --&amp;gt; Containers Start | | | | | v | | Containers Running | | | | | v | | Containers Terminated | | vPod Termination | vPod Garbage Collection쿠버네티스에서는 파드가 프로세스 대표하기 때문에, 더 이상 파드가 필요하지 않을 때 어떻게 Graceful 하게 프로세스들을 중단하는지 아는 것은 매우 중요하다. (물론, 갑작스럽게 강제로 KILL 하는 경우는 클린업 하는 기회도 없겠지만…)쿠버네티스의 파드 종료 설계는 삭제를 요청하고 언제 프로세스가 종료되는지를 알 수 있게 해줄 뿐만 아니라, 결국 삭제가 완료되는 것을 보장한다.파드 삭제를 요청했을 때, 클러스터는 파드가 강제로 종료되는 것이 가능하기 전까지 삭제 요청을 기록하고 의도된 Grace Period 를 추적한다. 강제 종료 요청이 확인되면, Kubelet은 graceful shutdown 을 시도한다.전형적으로 컨테이너 런타임은 TERM 시그널을 각 컨테이너의 메인 프로세스에 보낸다. 많은 컨테이너 런타임들은 컨테이너 이미지에 정의된 TERM 값을 대신하여 STOPSIGNAL 값을 보내기도 한다. 일단 Grace Period 가 만료되면, KILL 시그널은 남아있는 프로세스에게 보내지고, 파드는 API Server 로부터 삭제된다. 만약 프로세스가 종료되기를 기다리면서 Kubelet이나 컨테이너 런타임의 운영 서비스가 재시작된다면, 클러스터는 grace period 첨부터 삭제를 다시 시도할 것 이다.한번 흐름을 살펴보자. 내가 만약 kubectl 툴을 사용해서 grace period 는 30초로 설정하고 수동으로 특정 파드를 삭제했다고 하자. (kubectl delete pod &amp;lt;pod_name&amp;gt; --grace-period=&amp;lt;seconds&amp;gt;) API Server 에 있는 파드는 grace period 에 따라서 “dead” 로 간주되고 해당 상태값으로 업데이트 된다. 만약 이때 내가 kubectl describe 를 사용하여 삭제하고 있는 파드를 조회한다면, 파드는 “Terminating” 상태로 보일 것이다. 파드가 실행되고 있는 장비에서는 아래와 같은 순서로 진행될 것 이다.(kubelet이 terminating 상태로 마크된 파드를 확인하는대로 kubelet은 local pod shutdown 프로세스를 실행할 것이다.) 만약 파드의 컨테이너 중 하나에 preStop 훅이 걸려있다면, kubelet 은 컨테이너 내부에서 해당 훅을 실행할 것이다. 만약 preStop 훅이 grace period 가 만료되고도 실행되고 있다면, kubelet은 2초의 일회성 grace period 를 한번 더 요청한다. PreStop Hook : API 요청이나 관리 이벤트(liveness / startup probe failure, preemption, resource contention 등.. ) 에 의해서 컨테이너가 종료될 때 그 즉시 호출 됨. kubelet은 컨테이너 런타임을 트리거하여서 각 컨테이너 내부의 프로세스 1에 TERM 시그널을 보내도록 한다. (Process 1은 PID 1을 의미하는 것 같다. 모든 프로세스는 init 프로세스를 부모 프로세스를 가지고 있다. 즉, PID 1은 Init Process 를 의미하기 때문에 해당 프로세스가 죽으면 모든 프로세스가 죽음) Kubelet이 파드를 graceful하게 셧다운하려고 하는 동시에, 컨트롤 플레인은 해당 파드를 EndpointSlice 객체에서 삭제할지 말지 평가한다. 여기서 해당 객체라는 것은 Selector 가 구성된 Service를 의미한다. (즉, 해당 Pod를 select 하여 사용하고 있는 service 가 있는지를 확인한다는 의미). ReplicaSets 이나 다른 워크로드 리소스들은 더이상 죽고있는 파드를 유효하다고 보지 않는다. Grace Period 가 만료되면, Kubelet은 강제 종료를 트리거 한다. 컨테이너 런타임은 SIGKILL 시그널을 파드 내에 여전히 돌고 있는 프로세스들에게 뿌린다. 또한 Kubelet 은 숨겨져 있는 pause 컨테이너를 클린업 한다. Kubelet 은 해당 파드를 Terminal 상태로 바꾸어준다. (컨테이너의 마지막 상태에 따라서 Failed 혹은 Succeded 로 바꾸어준다. ) 이 스텝은 1.27 버전 이후부터 보증됨. Kubelet은 Grace Period를 0으로 설정하면서 API Server 로부터 파드 객체의 강제 삭제를 트리거한다. API Server 는 파드의 API 객체를 전부 삭제하고 더 이상 어떤 Client에서도 보이지 않게 한다.Forced Pod Termination기본적으로 모든 삭제는 30초로 Graceful Period 가 설정되어있다. kubectl delete 커맨드는 —grace-period=&amp;lt;seconds&amp;gt; 옵션을 제공해서 원하는 대로 설정할 수 있게 한다. 0으로 설정하면 강제 종료를 할 수 있다.강제 종료가 실행되면, API 서버는 kubelet 이 해당 파드가 노드에서 종료되었다는 것을 확인하는 것을 기다리지 않고 바로 삭제해버린다.Garbage collection of Pods실패한 파드들에 대한 API 객체들은 사람이나 controller 가 직접 삭제해주지 않으면 클러스터의 API에 남아있다.컨트롤플레인에 위치한 PodGC(Pod garbage collector) 는 종료된 파드들을 삭제해준다(Succeded or Failed). 이는 파드의 생성과 삭제로 인한 메모리 릭을 방지해줌.ref. Docs" }, { "title": "[Data Structure] MySQL Index 자료구조 B-tree", "url": "/posts/data-mysql-index-b-tree/", "categories": "Data Structure", "tags": "data structure, b-tree, index, hashtable", "date": "2023-06-21 00:00:00 +0900", "snippet": "Index 는 우리가 흔히 알고 있듯이, 어떠한 내용을 빠르게 찾기 위한 색인이다.책을 읽을 때도 Index 를 통해 찾고 싶은 내용을 빠르게 접근할 수 있다.MySQL 과 같은 Database 에서도 이러한 Index 기능을 제공하는데, 이 때 주로 사용되는 자료구조는 B-tree 이다.단순히 생각했을 때, 빠르게 찾는다면 Hash map 을 생각할 수도 있고, Binary Tree 를 떠올려 볼 수도 있다.근데 왜 B-tree를 사용할까?이번 글에서는 B-tree 가 무엇이고 어떤 장단점을 가지고 있으며, 왜 MySQL과 같은 Database 는 B-tree를 자료구조로 선택했는지 간략하게 알아보자.B-tree 란B-tree의 B 는 주로 Balanced 라는 뜻으로 알려져있다. 즉, Left 와 Right 에 균형이 잡혀있다는 것이다.B-tree 는 Binary Tree가 확장된 구조인데, 하나의 노드가 가질 수 있는 자식 노드의 최대 숫자가 2보다 큰 트리 구조이다. 한 노드에 한가지 값이 아닌 여러 값을 가질 수 있다. 데이터가 정렬된 상태로 유지되어 있다.왜 B-tree가 빠른가 B-tree 의 가장 큰 장점 중 하나는 양쪽 자식의 균형을 유지한다는 것이다. 이를 통해서 탐색 시에 무조건 O(logN)의 Time Complexity 를 보장한다. 하지만 노드의 삽입 및 삭제 시에 재정렬하는 과정에서 많은 성능 저하를 초래한다.B+tree 란B+tree 는 B-tree 의 확장 개념인데, 데이터를 오직 Leaf Node에만 저장한다.즉, Branch 노드에는 Key 만 담아두고, 해당 Key의 Data 를 가지고 있는 Leaf Node 를 가리키는 포인터를 가지고 있다.B+tree 의 장점 Leaf node를 제외하고 데이터를 담아두지 않기 때문에, 메모리 확보에 유용함. 하나의 노드에 더 많은 Key들을 담을 수 있어서, 트리의 높이가 더 낮아짐Index 자료구조로 B-tree를 선택한 이유Hash table 은 왜 안되는가Hash Table 은 Key 값만 안다면 탐색의 시간복잡도가 O(1) 이기 때문에 매우 빠르다.하지만, 해당 값들은 정렬되어 있다는 것이 보장되지 않기 때문에 만약 Index로 Hash Table을 쓴다면 해당 Key의 작거나 큰 값이 어디있는지 찾는 부분에 있어서 O(1) 의 시간복잡도를 보장할수도 없을 뿐더러 매우 비효율적이다. 즉, 범위에 대한 연산에 비효율적인 자료구조이다.왜 Binary Tree가 아니라 B-Tree인가이 둘의 가장 큰 차이는 하나의 노드가 가지는 데이터 개수 이다.ref. https://helloinyong.tistory.com/296Referencehttps://ko.wikipedia.org/wiki/B_트리https://ko.wikipedia.org/wiki/B%2B_트리https://en.wikipedia.org/wiki/Hash_tablehttps://zorba91.tistory.com/293https://helloinyong.tistory.com/296" }, { "title": "[K8S] Worker Node의 Kubelet IP 변경하기", "url": "/posts/k8s-kubelet-change-ip/", "categories": "Kubernetes", "tags": "kubernetes, k8s, etcd, kernel, linux, leader_election", "date": "2023-06-20 00:00:00 +0900", "snippet": "GPU 장비 상면 이전을 진행하였는데, 이때 장비의 IP가 변경되었다. 해당 장비는 Kubernetes 클러스터의 Worker 노드로 사용되고 있었고, 노드 삭제 및 추가는 따로 진행하지 않았다.즉, 상면 이전을 진행하는데 단순 Schedule Disabled 만 해둔 상태.이때 해당 장비의 IP가 바뀌었다는 것을 Kubelet은 직접 알지 못한다. 따라서 노드를 삭제하고 추가해주는 작업을 해주거나, Kubelet이 바라보는 IP 를 업데이트 된 IP로 수정해주는 작업이 필요함그렇지 않으면 아래와 같은 에러가 발생하면서 Kubelet 과의 소통에서 Timeout 이 발생함dial tcp 10.240.0.4:10250: i/o timeoutKubelet 설정 추가하기Kubelet은 현재 systemd 서비스 단위로 구성되어있다. 따라서 이에 대한 옵션을 추가하기 위해서는 /etc/systemd/system/kubelet.service 파일의 수정이 필요하다.[Unit]Description=Kubernetes Kubelet ServerDocumentation=https://github.com/GoogleCloudPlatform/kubernetesAfter=docker.serviceWants=docker.socket[Service]User=rootEnvironmentFile=-/etc/kubernetes/kubelet.envExecStart=/usr/local/bin/kubelet \\ $KUBE_LOGTOSTDERR \\ $KUBE_LOG_LEVEL \\ $KUBELET_API_SERVER \\ $KUBELET_ADDRESS \\ $KUBELET_PORT \\ $KUBELET_HOSTNAME \\ $KUBELET_ARGS \\ $DOCKER_SOCKET \\ $KUBELET_NETWORK_PLUGIN \\ $KUBELET_VOLUME_PLUGIN \\ $KUBELET_CLOUDPROVIDER \\ $KUBELET_EXTRA_ARGS --node-ip 10.00.00.101Restart=alwaysRestartSec=10s[Install]WantedBy=multi-user.target-node-ip string Node 의 IP 주소값을 넣어줄 수 있음. 설정되지 않는다면 kubelet은 노드의 기본 IPv4 주소값을 사용하거나 IPv6 주소를 사용한다. ref. https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/$KUBELET_EXTRA_ARGS --node-ip 10.164.136.20 값을 ExecStart 에 추가해주면서 새롭게 업데이트 된 Node IP 를 직접 명시해준다.Kubelet config 적용 및 재시작systemctl daemon-reload &amp;amp;&amp;amp; systemctl restart kubeletNode IP 확인 kubectl get nodes -owide dummy-hostNAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIMEdummy-host Ready &amp;lt;none&amp;gt; 3y154d v1.15.1 10.00.00.101 &amp;lt;none&amp;gt; Ubuntu 18.04.3 LTS 4.15.0-74-generic docker://18.9.7INTERNAL-IP 값이 업데이트 된 것을 확인할 수 있다." }, { "title": "[GPU] NVML Function Not Found Error (ft. pynvml)", "url": "/posts/gpu-nvml-trouble-shooting/", "categories": "Trouble Shooting, GPU", "tags": "NVIDIA, GPU, nvml, pynvml", "date": "2023-06-14 00:00:00 +0900", "snippet": "Pytorch Serve 를 진행하는 중 아래와 같은 에러 발생AttributeError: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetComputeRunningProcesses_v3pynvml.nvml.NVMLError_FunctionNotFound: Function Not Found Full Log File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 850, in _nvmlGetFunctionPointer _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name) File &quot;/usr/lib/python3.8/ctypes/__init__.py&quot;, line 382, in __getattr__ func = self.__getitem__(name) File &quot;/usr/lib/python3.8/ctypes/__init__.py&quot;, line 387, in __getitem__ func = self._FuncPtr((name_or_ordinal, self))AttributeError: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetComputeRunningProcesses_v3During handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;ts/metrics/metric_collector.py&quot;, line 27, in &amp;lt;module&amp;gt; system_metrics.collect_all(sys.modules[&#39;ts.metrics.system_metrics&#39;], arguments.gpu) File &quot;/usr/local/lib/python3.8/dist-packages/ts/metrics/system_metrics.py&quot;, line 119, in collect_all value(num_of_gpu) File &quot;/usr/local/lib/python3.8/dist-packages/ts/metrics/system_metrics.py&quot;, line 90, in gpu_utilization statuses = list_gpus.device_statuses() File &quot;/usr/local/lib/python3.8/dist-packages/nvgpu/list_gpus.py&quot;, line 75, in device_statuses return [device_status(device_index) for device_index in range(device_count)] File &quot;/usr/local/lib/python3.8/dist-packages/nvgpu/list_gpus.py&quot;, line 75, in &amp;lt;listcomp&amp;gt; return [device_status(device_index) for device_index in range(device_count)] File &quot;/usr/local/lib/python3.8/dist-packages/nvgpu/list_gpus.py&quot;, line 19, in device_status nv_procs = nv.nvmlDeviceGetComputeRunningProcesses(handle) File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 2608, in nvmlDeviceGetComputeRunningProcesses return nvmlDeviceGetComputeRunningProcesses_v3(handle); File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 2576, in nvmlDeviceGetComputeRunningProcesses_v3 fn = _nvmlGetFunctionPointer(&quot;nvmlDeviceGetComputeRunningProcesses_v3&quot;) File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 853, in _nvmlGetFunctionPointer raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)pynvml.nvml.NVMLError_FunctionNotFound: Function Not FoundNVML 이란NVIDIA Management Library NVIDIA GPU 디바이스들의 다양한 상태를 관리하고 모니터링하는 C-based API NVML 은 nvidia-smi 를 통해서 직접 쿼리와 커맨드를 날릴 수 있도록 제공함 NVML 런타임 버전은 NVIDIA 디스플레이 드라이버와 함께 제공되며, SDK 는 적절한 헤더와 스텁 라이브러리 및 샘플 애플리케이션을 제공함Pynvml 이란 GPU 관리 및 모니터링 함수에 대한 Python 인터페이스 즉, NVML 라이브러리 래핑 컴포넌트임. 11.0.0 버전부터는 pynvml 에서 사용되는 NVML-wrappers 은 nvidia-ml-py 와 동일하게 가져가고있음!Usage&amp;gt;&amp;gt;&amp;gt; from pynvml import *&amp;gt;&amp;gt;&amp;gt; nvmlInit()&amp;gt;&amp;gt;&amp;gt; print(f&quot;Driver Version: {nvmlSystemGetDriverVersion()}&quot;)Driver Version: 11.515.48&amp;gt;&amp;gt;&amp;gt; deviceCount = nvmlDeviceGetCount()&amp;gt;&amp;gt;&amp;gt; for i in range(deviceCount):... handle = nvmlDeviceGetHandleByIndex(i)... print(f&quot;Device {i} : {nvmlDeviceGetName(handle)}&quot;)...Device 0 : Tesla K40c&amp;gt;&amp;gt;&amp;gt; nvmlShutdown() 위와 같이 사용할 수 있음. 근데 이번 케이스 같은 경우 아래와 같이 에러가 발생&amp;gt;&amp;gt;&amp;gt; nvmlDeviceGetComputeRunningProcesses(handle)Traceback (most recent call last): File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 850, in _nvmlGetFunctionPointer _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name) File &quot;/usr/lib/python3.8/ctypes/__init__.py&quot;, line 382, in __getattr__ func = self.__getitem__(name) File &quot;/usr/lib/python3.8/ctypes/__init__.py&quot;, line 387, in __getitem__ func = self._FuncPtr((name_or_ordinal, self))AttributeError: /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.1: undefined symbol: nvmlDeviceGetComputeRunningProcesses_v3During handling of the above exception, another exception occurred:Traceback (most recent call last): File &quot;&amp;lt;stdin&amp;gt;&quot;, line 1, in &amp;lt;module&amp;gt; File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 2608, in nvmlDeviceGetComputeRunningProcesses return nvmlDeviceGetComputeRunningProcesses_v3(handle); File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 2576, in nvmlDeviceGetComputeRunningProcesses_v3 fn = _nvmlGetFunctionPointer(&quot;nvmlDeviceGetComputeRunningProcesses_v3&quot;) File &quot;/usr/local/lib/python3.8/dist-packages/pynvml/nvml.py&quot;, line 853, in _nvmlGetFunctionPointer raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)pynvml.nvml.NVMLError_FunctionNotFound: Function Not FoundSolution CUDA 버전 업그레이드를 하거나, pynvml 버전을 다운그레이드 해라! pynvml 버전을 11.4로 다운그레이드 해서 테스트 진행 정상동작함!Referencehttps://forums.developer.nvidia.com/t/unable-to-access-pynvml-methods/226914https://github.com/NVIDIA/k8s-device-plugin/issues/331https://stackoverflow.com/questions/73591281/nvml-cannot-load-methods-nvmlerror-functionnotfound?noredirect=1#comment129972360_73591281https://pypi.org/project/pynvml/https://pypi.org/project/nvidia-ml-py/https://developer.nvidia.com/nvidia-management-library-nvml" }, { "title": "[etcd] [Docs Learning] KV API Guarantees", "url": "/posts/etcd-learning-kv-api-guarantees/", "categories": "etcd, docs", "tags": "etcd, docs, learning, kv, api, keyvalue", "date": "2023-06-12 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.DocsAPIs to consider Read APIs range watch Write APIs put delete Combination (read-modify-write) APIs txn Lease APIs grant revoke put (attaching a lease to a key) Watch operation Reference 1 Reference 2 말그대로 client 에서 etcd 의 특정 key를 watching 하는 것 ! etcd specific definitionsOperation completed consensus 를 통해서 commit 이 되어야 etcd operation이 끝났다고 볼 수 있음 → executed client 는 etcd server 로부터 response를 받음으로서 complete 을 알 수 있다. 근데 client 는 etcd server 에 time out 이 생긴지 혹은 멤버간의 network disruption 이 생겼는지 확신할 수 없음. etcd 는 client 에게 “abort “ 응답을 보내지 않는다. Revision Modifying key value store 은 single increasing revision 하게 함 하나의 transaction이 여러개의 key 값을 바꾸더라도 revison은 하나만 올라감 오퍼레이션에 의해 수정된 KV Pair 의 revision 속성은 해당 오퍼레이션의 revision 값과 동일Guarantees providedACID 보장 (Atomicity, Consistency, Isolation, Durability)Atomicity abortability 로 불릴 수 있음. 완벽하게 취소가 가능함!! 1 or nothing 으로 함. 중간은 절대 없음. 완벽하게 되돌릴 수 있음 redis 는 partial failure 가 있음.. atomicity 보장해주지 않음. All API requests are atomic Watch 의 경우에 한 오퍼레이션에 포함된 이벤트들을 부분적으로 관찰하지 않음.Durability 모든 완료된 동작은 durable 함 접근 가능한 데이터는 모두 durable 함 READ는 durable 하지 않은 데이터는 절대 return 하지 않음 → 멤버가 하나뿐이라면 data return (read) 못함. 근데 Serializable 옵션을 주면 Read 가능함! Isolation level and consistency of replicas etcd ensured strict serializability 분산 트랜잭션 데이터베이스 시스템에서 가장 강력한 isolation guarantee 임 read 는 절대 중간 데이터를 관찰하지 않음 https://jepsen.io/consistency/models/strict-serializable serializability 가 강해지면 분산 시스템의 의미가 약화됨. → 왜냐하면 병렬성을 존중해주지 않고 순서를 정해버리기 때문. etcd ensures linearizability as consistency of replicas basically 복제본의 일관성으로 linearizability 를 보장함 serializable 가능 옵션을 명시적으로 지정하는 watch 와 read 는 예외임 https://cs.brown.edu/~mph/HerlihyW90/p463-herlihy.pdf client 측면에서 linearizability 는 추론을 쉽게할 수 있게 해줌 t1 쓰기, t2 읽기의 상황에서 t2 라는 읽기 요청 시점과 읽기에 대한 응답이 오는 t3. 즉, t2 ~ t3 시간 사이에 어떤 사건이 발생할 수도, 아닐 수도 있음. 사건이 없는 경우에는 t1이 최신 사건이 될 것이고 읽기 응답에 포함되어야 함. 근데, t1이후 t2 ~ t3 에 벌어진 사건이 추가로 있다면 응답에 포함해주는 것이 Linearizability 를 보장해주는 것임! → 이렇게 Linearizability 가 보장되면 사용자는 시스템의 응답이 최신이라고 믿고 사용할 수 있음. 반면 보장이 안된다면 응답 결과가 오래 된 값 (Stale) 이라고 의심하고 다루어야 함. etcd 의 Watch 에서는 linearizability 를 보장하지 않음 성능 때문임. k8s 같은 경우 엄청 많은 operator 들이 watch 걸어두고 보고 있는데, 여기서 raft 태워서 리턴을 하게되면 성능이 엄청 떨어짐 그래서 watch 같은 경우는 Client 가 Revision(monotoni clock) 을 가지고 잘 판단해서 데이터 읽어서 써라! 라는 의미임. 다른 operation 에서는 기본적으로 linearizability 를 보장함. linearized requests 는 raft 합의 프로세스를 거쳐야 하기때문에 이게 좀 비쌈. read 에서 lower latencies and higer throughput 를 얻으려면 client 는 serializable 모드로 사용하면 됨 요건 근데 쿼럼과 관련해서 stale data 에 접근할 수도 있지만 성능저하가 사라짐 그럼 client 접근은 랜덤일까? Granting, attaching and revoking leases etcd provides a lease mechanism https://web.stanford.edu/class/cs240/readings/89-leases.pdf " }, { "title": "[Leetcode] 69. Sqrt(x) with Binary Search", "url": "/posts/leetcode-69/", "categories": "algorithm", "tags": "algorithm, leetcode, binarySearch, bruteForce", "date": "2023-06-08 00:00:00 +0900", "snippet": "ProblemLinkGiven a non-negative integer x, return the square root of x rounded down to the nearest integer. The returned integer should be non-negative as well.You must not use any built-in exponent function or operator. For example, do not use pow(x, 0.5) in c++ or x ** 0.5 in python.Example 1:Input: x = 4Output: 2Explanation: The square root of 4 is 2, so we return 2.Example 2:Input: x = 8Output: 2Explanation: The square root of 8 is 2.82842..., and since we round it down to the nearest integer, 2 is returned.SolutionSolution 1 - Using Brute Force단순 브루트 포스 방법으로 풀 수 있다.y = √x⇒y^2 = x⇒y^2 ≤ x따라서 y 를 0에서 부터 1씩 증가시키면서, 만약 y^2 가 x 보다 크다면 y-1 을 리턴할 수 있다.class Solution: def mySqrt(self, x: int) -&amp;gt; int: y = 0 while y * y &amp;lt;= x: y += 1 return y -1Solution 2 - Using Binary SearchSpace Complexity is O(1)공간 복잡도를 최소화 하기 위해서 이진 탐색을 사용할 수 있다.class Solution: def mySqrt(self, x: int) -&amp;gt; int: if x == 0: return 0 l, r = 1, x while l &amp;lt;= r: mid = l + (r - l) // 2 if mid * mid == x: return mid elif mid * mid &amp;gt; x: r = mid - 1 else: l = mid + 1 return r의문점왜 Mid 값을 구할 때 (left + right) // 2 가 아니라 left + (right - left) //2 를 쓰는 것인가?⇒Overflow 때문에 그렇다.left + right ≥ right 는 성립할 수 있지만, left + (right - left) / 2 ≤ right 으로 우측에 있는 값은 right 을 넘을 수 없기 때문에, Overflow 가 발생할 수 없음. 따라서 해당 수식을 사용한다.Reference. https://stackoverflow.com/questions/27167943/why-leftright-left-2-will-not-overflow Chat GPT" }, { "title": "[etcd] [Docs Learning] etcd v3 authentication design", "url": "/posts/etcd-learning-authentication-design/", "categories": "etcd, docs", "tags": "etcd, docs, learning, authentication, v3, auth, client, grpc, http", "date": "2023-06-05 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.Docshttps://etcd.io/docs/v3.5/learning/design-auth-v3/ 한 줄 정리 : RESTful 한 인증 방식인 v2 를 쓰다가, raft 알고리즘을 적용한 v3를 사용했더니 consistency 보장도 좋고, 성능도 좋다!Serializable VS Linearizable Serializable 는 trasaction 개념 Single memeber 가 응답 Linearizable 은 Consistency 개념 Leader 를 통해서 가져옴 선형화 가능 읽기 요청은 가장 최근 데이터를 가져오기 위해 클러스터 멤버의 쿼럼을 거쳐 합의를 도출함. 직렬화 가능 읽기 요청은 오래된 데이터를 제공하는 대가로 멤버 쿼럼이 아닌 단일 etcd 멤버가 제공하므로 선형화 가능 읽기보다 비용이 저렴함.Why not reuse the v2 auth system? v3 프로토콜은 v2처럼 RESTful 한 인터페이스가 아닌 gRPC를 사용함 v2와 비교했을 때 개선점이 많음! 예를 들어,v3는 v2의 느렸던 per-request 인증을 개선한 커넥션 기반 인증을 사용해서 더 빠름 (어쨌든 v3가 더 좋음)Functionality requirements 커넥션 단위 인증 (not per request) 기능면에서 v2보다 심플하고 유용함 v2처럼 디렉터리 구조를 사용하는 것과는 다르게, v3는 flat key space를 제공한다 v2보다 훨씬 강력한 consistency 를 보장한다.Main required changes 클라이언트는 인증된 요청을 보내기 전에 인증만을 위한 전용 커넥션을 무조건 생성해야함. Raft 커맨드에 퍼미션 정보를 추가애햐 함 (유저 ID, 리비전 등) 모든 요청은 API 레이어가 아닌 *state machine* 레이어에서 퍼미션 체크가 됨. raft 알고리즘에 state machine 이 있는데 여기서 permission check를 함 Permission metadata consistency 인증 메타데이터는 etcd에 저장된 다른 데이터들과 마찬가지로 etcd의 Raft 프로토콜에 의해서 저장되고 관리 됨 가용성과 일관성에 영향을 주지 않음 rw 시에 permission info(metadata)를 사용하기 때문에 SPOF 위험있음 전체 노드의 동의가 필요하다는 것은 가용성에 문제가 있는 것을 의미하기 때문에 인증에도 raft 알고리즘을 사용하면 충분하다 v2의 경우 일관성 문제 해결하기 위해서 metadata consistency가 위와 같이 동작해야하는데, 그렇지 않아서 까다로운 부분이다, 실제로는 각 permission 체크는 client 요청을 받는 etcd 멤버가 수행하기 때문에 오래된 데이터로 인증하는 것이 가능! (server/etcdserver/api/v2http/client.go) 이는 운영자가 etcdctl 을 사용했을 때 즉각적으로 auth conf 가 적용되지 못하는 것을 의미 그 stale metadata가 얼마나 오래되었는지 알 수가 없음 사실 실제로는 커맨드 실행 시 바로 적용되긴하는데, 큰 부하의 경우 일관성 없는 상태가 지속될 수 있고, 사용자와 개발자에게 직관적이지 못한 결과를 보여줌 " }, { "title": "[번역] Kubernetes: Flannel Networking", "url": "/posts/eng_k8s-flannel-networking/", "categories": "Kubernetes", "tags": "kubernetes, packet, network, translation, cni, flannel. vxlan, udp", "date": "2023-05-31 00:00:00 +0900", "snippet": " 해당 Posting 을 공부하며 번역하는 글입니다.해당 포스팅은 쿠버네티스 환경에서 flannel 네트워크가 어떻게 동작하는지를 설명하는 글입니다.쿠버네티스는 규모에 맞게 컨테이너화 된 애플리케이션을 관리하는데 굉장히 유용한 도구입니다. 하지만 여러분도 아시다시피, 쿠버네티스를 학습하는 것은 쉽지만은 않습니다. 특히 네트워크 구현의 백앤드쪽은 더 어렵습니다.네트워킹 쪽에서 많은 이슈들을 마주했었고, 트러블 슈팅에 많은 시간을 소요했습니다.이번 포스팅에서, 최대한 간단한 예시로 구현체에 대해서 설명할 것입니다. 이를 통해 쿠버네티스 네트워크가 어떻게 동작하는지 설명하겠습니다. 이 포스팅이 쿠버네티스를 공부하는 사람들에게 많은 도움이 되었으면 좋겠습니다.Kubernetes networking model아래의 도표는 간단한 쿠버네티스 클러스터를 보여줍니다.쿠버네티스는 리눅스 머신(AWS EC2와 같은 VM 이 될 수도 있고, 물리 머신이 될 수도 있습니다) 을 관리하고, 각각의 host 머신에서 쿠버네티스는 수 많은 파드들을 실행합니다. 또한 각각의 파드는 여러 컨테이너를 가지고 있을 수 있습니다. 유저 애플리케이션은 여러 컨테이너들 중 하나의 내부에서 돌아가고 있습니다.쿠버네티스에서 파드는 최소 단위이고, 파드 내부에 위치한 모든 컨테이너는 같은 네트워크 네임스페이스를 공유합니다. 즉, 모든 컨테이너는 같은 네트워크 인터페이스를 가지고 있고 localhost 를 사용하여 각각이 연결될 수 있다는 것을 의미합니다.공식문서에서는 쿠버네티스 네트워크 모델은 아래의 요소들을 필요로 한다고 이야기하고 있습니다. 모든 컨테이너는 모든 다른 컨테이너와 NAT 없이 통신이 가능하다. 모든 노드는 모든 컨테이너와 NAT 없이 통신이 가능하다. (역도 가능) 컨테이너가 자신의 IP 를 확인했을 때 나오는 값과 외부에서 바라보는 IP는 동일하다.우리는 위의 내용에서 나오는 컨테이너를 파드로 바꿔서 볼 수 있습니다. 즉, 모든 컨테이너는 파드 네트워크를 공유합니다.기본적으로 모든 파드는 클러스터 내부의 모든 다른 파드들과 자유롭게 통신할 수 있다는 것을 의미하는데요, 심지어 다른 Host 일 경우도 가능합니다.그리고 이들은 Host 가 존재하지 않는 것처럼 자신들의 IP 주소를 가지고 인식을 합니다. 또한 Host 는 어떠한 주소 변환 (DNAT, SNAT) 없이 자신의 IP 주소로 어떤 파드든 연결이 가능합니다.The Overlay NetworkFlannel 은 쿠버네티스 네트워크를 위하여 CoreOS에 의해 개발되었는데요, 쿠버네티스 뿐만 아니라 다른 목적의 일반적인 네트워크 솔루션으로 사용되기도 합니다.쿠버네티스 네트워크 요구사항을 만족하기 위한 Flannel 의 컨셉은 단순합니다.“Host 네트워크 위에서 작동하는 Overlay 네트워크라고 불리는 Flat 한 다른 네트워크를 만든다.”모든 컨테이너들 (파드) 은 오버레이 네트워크에서 각자 하나의 IP 주소를 할당받을 것이고, 서로의 Ip 주소를 직접 찌르면서 통신할 것입니다.설명에 대한 이해를 돕기 위해서, AWS 위에서 작은 쿠버네티스 클러스터를 올려봤습니다. 클러스터에는 3개의 노드가 존재하고 네트워크 구조는 아래와 같습니다.이 클러스터에는 3가지 네트워크가 존재합니다.AWS VPC Network모든 인스턴스는 하나의 VPC 서브넷 (172.20.32.0/19) 에 위치합니다. 인스턴스들은 이 범위 안에서 IP 주소를 할당받는데요, 모든 호스트는 같은 LAN에 있기 때문에 서로 연결이 가능합니다.Flannel Overlay NetworkFlannel 은 2¹⁶(65536) 개의 주소를 가질 수 있는 더 큰 네트워크(100.96.0.0/16)를 만듭니다. 그리고 이 네트워크는 모든 쿠버네티스 노드들에 걸쳐있고, 각각의 파드들은 이 범위에서 하나의 IP 주소를 할당받을 것입니다.이후에는 어떻게 Flannel 이 이와 같은 네트워크를 얻을 수 있는지 확인해보겠습니다.In-Host Docker NetworkHost 내부에서 Flannel은 해당 호스트에 있는 모든 파드들에게 100.96.x.0/24 네트워크를 할당합니다.같은 호스트 내부에 위치한 컨테이너들은 docker0 라는 Docker bridge를 이용하여 서로 통신이 가능한데요, 이는 간단하기 때문에 이번 아티클에서는 생략하겠습니다.오버레이 네트워크에서 다른 호스트들에 위치한 컨테이너들끼리 통신하기 위해서, Flannel 은 커널 라우트 테이블과 UDP 캡슐화를 사용합니다. 아래의 내용은 이를 설명하기 위한 부분입니다!Cross host container communication노드 1번에 위치한 100.96.1.2 IP 주소를 가진 컨테이너(컨테이너-1 이라고 부르자) 가 노드 2번에 위치한 100.96.2.3 IP 주소를 가진 컨테이너(컨테이너-2 라고 부르자) 와 연결을 하고 싶다고 가정해보겠습니다.그럼 오버레이 네트워크가 어떻게 패킷이 통과 가능하도록 하는지 살펴보겠습니다!Host 들 간의 연결먼저 컨테이너-1 은 src: 100.96.1.2 -&amp;gt; dst: 100.96.2.3 를 가진 IP 패킷을 하나 만듭니다. 패킷은 컨테이너의 게이트웨이인 docker0 브릿지로 이동할 것입니다.각각의 호스트에서, flannel 은 flanneld 라고 불리는 데몬 프로세스를 실행하는데요, 이는 커널의 라우트 테이블에 라우트 규칙을 생성합니다.아래는 노드 1번의 라우트 테이블입니다.admin@ip-172-20-33-102:~$ ip routedefault via 172.20.32.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.1.0100.96.1.0/24 dev docker0 proto kernel scope link src 100.96.1.1172.20.32.0/19 dev eth0 proto kernel scope link src 172.20.33.102위에서 볼 수 있듯이, 패킷의 목적지 주소 100.96.2.3 는 더 큰 네트워크인 오버레이 네트워크 100.96.0.0/16 에 속합니다. 따라서 이는 두번째 규칙에 부합합니다. 이제 커널은 이 패킷을 flannel0 으로 보내야 한다는 것을 알게됩니다. Note: ip route 명령어와 iptables 명령어는 뭐가 다를까? ip route : 네트워크 경로 설정과 관련된 작업을 수행. IP 패킷의 경로를 결정하는 라우팅 테이블을 관리할 수 있음 iptables : Linux에서 방화벽 기능을 제공하는 도구. 네트워크 트래픽을 필터링하고, 허용 또는 거부하는 규칙을 설정할 수 있음flannel0 은 또한 우리 flanneld 데몬 프로세스에 의해서 만들어진 TUN 디바이스이고 (*TUN 은 리눅스 커널에서 구현된 소프트웨어 인터페이스를 뜻합니다*), raw IP 패킷을 유저 프로그램과 커널 사이에서 패스할 수 있습니다.이는 두 가지 방법으로 작동됩니다. IP 패킷을flannel0 디바이스에 쓸 때, 패킷은 커널로 직접 보내지고 커널은 라우트 테이블에 따라서 패킷을 라우팅할 것입니다. IP 패킷이 커널에 도착하면 라우트 테이블은 이 패킷이 flannel0 디바이스로 가야한다고 말할 것입니다. 그리고 커널은 이 장치를 생성한 프로세스 즉, flanneld 데몬 프로세스로 패킷을 보내줄 것입니다.커널이 TUN 장치로 패킷을 보낼 때 이는 곧 바로 flanneld 프로세스로 이동하고, 목적지 주소가 100.96.2.3 인것을 확인합니다. 도표에서는 이 주소가 노드 2번에서 돌아가는 컨테이너에 속해있다는 것을 알 수 있지만, flanned 는 이것을 어떻게 알까요?flannel이 몇 가지 정보를 etcd 라고 불리는 키-값 저장소에 저장을 하고있기 때문에 가능한 일입니다.만약 여러분이 쿠버네티스에 대해서 어느정도 아신다면 그렇게 놀랄일은 아닐 겁니다.flannel 의 경우에는 etcd를 단순 키-값 저장소로 생각할 수 있습니다. flannel 은 서브넷과 호스트 간 매핑 정보를 etcd 서비스에 저장합니다. etcdctl 커맨드로 확인할 수도 있습니다!admin@ip-172-20-33-102:~$ etcdctl ls /coreos.com/network/subnets/coreos.com/network/subnets/100.96.1.0-24/coreos.com/network/subnets/100.96.2.0-24/coreos.com/network/subnets/100.96.3.0-24admin@ip-172-20-33-102:~$ etcdctl get /coreos.com/network/subnets/100.96.2.0-24{&quot;PublicIP&quot;:&quot;172.20.54.98&quot;}그러니깐, 각각의 flanneld 프로세스는 etcd에 쿼리를 날려 각각의 서브넷이 어떤 호스트에 속해있는지 알 수 있고, 목적지 IP를 etcd 에 저장된 모든 서브넷과 비교할 수 있습니다.우리의 경우에는 100.96.2.3 이라는 주소가 100.96.2.0-24 라는 서브넷과 매칭됩니다. 그리고 해당 키에 저장된 값을 확인해보면 Node IP가 172.20.54.98 이라는 것을 알 수 있습니다! 이제 flanneld 는 목적지 주소를 알고 있습니다. 또한 자신의 호스트 IP를 출발지 주소로 하고 타겟 호스트의 IP를 목적지로 하여 원본 IP 패킷을 UDP 패킷으로 캡슐화하였습니다.각각의 호스트에서 flanneld 프로세스는 디폴트 UDP 포트 :8285 를 리슨하고 있을 것입니다.이제 UDP 패킷의 목적지 포트를 8285 로만 설정해주고 유선으로 전송만 하면 됩니다! UDP 패킷이 목적지 호스트에 도착한 뒤에, 커널의 IP 스택은 패킷을 flanneld 프로세스로 보낼 것입니다. 왜냐하면 flanneld 가 UDP Port 인 8285 를 리슨하고 있는 유저 프로세스이기 때문입니다.그럼 flanneld 는 원본 컨테이너로부터 생성된 원본 IP이 담겨있는 패킷인 UDP 패킷의 payload 를 받을 것이고, 간단하게 이 패킷을 TUN 장치인 flanneld0 에 작성할 것입니다. 그러면 이 패킷은 커널로 바로 보내지는데, 이것이 TUN 이 장동하는 방법입니다. 노드 1번과 동일하게, 라우트 테이블은 이 패킷이 어디로 가야할지 결정할 것입니다.노드 2번의 라우트 테이블을 살펴보죠!admin@ip-172-20-54-98:~$ ip routedefault via 172.20.32.1 dev eth0100.96.0.0/16 dev flannel0 proto kernel scope link src 100.96.2.0100.96.2.0/24 dev docker0 proto kernel scope link src 100.96.2.1172.20.32.0/19 dev eth0 proto kernel scope link src 172.20.54.98IP 패킷의 목적지 IP 주소는 100.96.2.3 이었습니다.커널은 가장 근접하게 일치하는 규칙을 적용할 것인데, 3번째가 바로 그것입니다. 패킷은 docker0 장치로 보내질 것입니다.docker0 는 브릿지 장치이고, 해당 장비에 있는 모든 컨테이너들은 이 브릿지를 통해서 연결되어 있기 때문에 패킷은 결국 보내질 것이고 목적지인 컨테이너-2 에서 받을 수 있을 것입니다. 마침내 우리 패킷은 목적지로 가는 여정을 마쳤습니다. 컨테이너-2 가 컨테이너-1로 패킷을 다시 보낸다면 역방향의 라우팅이 정확히 동일하게 진행될 것 입니다.이것이 host 컨테이너가 통신하는 방법입니다!Configuring with docker network위의 설명에서 우리는 한 가지 포인트를 놓쳤습니다.100.96.x.0/24 와 같이 조금 더 작은 서브넷을 사용하려면 도커를 어떻게 설정해야할까요?flanneld 가 서브넷 정보를 호스트에 위치한 파일에 작성하는데, 이를 통해 가능합니다.admin@ip-172-20-33-102:~$ cat /run/flannel/subnet.envFLANNEL_NETWORK=100.96.0.0/16FLANNEL_SUBNET=100.96.1.1/24FLANNEL_MTU=8973FLANNEL_IPMASQ=true이 정보는 도커 데몬의 옵션을 구성할때 사용됩니다. 따라서 도커는 FLANNEL_SUBNET 을 브릿지 네트워크로 사용할 수 있고, 이를 통해 in-host 컨테이너 네트워크는 동작합니다.dockerd --bip=$FLANNEL_SUBNET --mtu=$FLANNEL_MTU Packet copy and Performance새로운 버전의 flannel 은 운영 단계에서 UDP 캡슐화를 추천하지 않으며, 디버깅과 테스트 목적으로만 사용되어야 한다고 이야기합니다. 그 이유는 성능때문입니다.flannel0 TUN 장치는 커널을 통해 패킷을 얻거나 보낼 때 간단한 방법을 사용하지만, 이는 성능적인 패널티가 존재합니다. 패킷이 user space와 kernel space 앞 뒤로 복사가 되어야만 한다는 것입니다.위에서 볼 수 있듯이, 원본 컨테이너 프로세스가 패킷을 보내면 패킷은 User space와 Kernel space 사이에서 3번의 복사가 일어나야 합니다. 이는 네트워크 오버헤드를 상당히 증가시키기 때문에 운영 단계에서는 될 수 있으면 UDP 사용을 피해야합니다.ConclusionFlannel 은 쿠버네티스 네트워크 모델 구현체 중 가장 간단한 것들 중 하나입니다. flannel 은 이미 존재하는 docker 브릿지 네트워크와 UDP 캡슐화를 위해 데몬 프로세스를 이용한 추가적인 TUN 장치를 사용합니다.이 글에서는 주요한 부분의 디테일만 설명드렸습니다: host 컨테이너 간의 통신과 성능 저하에 대한 간단한 언급 정도.쿠버네티스 네트워크의 기본에 대해 이해하는데 이 글이 많은 도움이 되었으면 좋겠습니다. 이러한 지식을 바탕으로 여러분은 더 흥미진진한 쿠버네티스 네트워크를 탐험할 수 있을 것입니다. 또한 Calico, WeaveNet, Romana 와 같은 더 고도화된 구현체를 이해할 수 있을 것입니다!Reference https://ssup2.github.io/theory_analysis/Kubernetes_Flannel_Plugin/전체적인 그림을 위해서 추가적인 내용 ref. https://ssup2.github.io/theory_analysis/Kubernetes_Flannel_Plugin/" }, { "title": "[번역] Life of a Packet in Kubernetes— Part 3", "url": "/posts/eng_life-of-a-packet-in-k8s/", "categories": "Kubernetes", "tags": "kubernetes, packet, network, translation, cni, iptables, kube-proxy", "date": "2023-05-30 00:00:00 +0900", "snippet": " 해당 Posting 을 공부하며 번역하는 글입니다.이번 포스팅은 “쿠버네티스에서 패킷의 삶” 이라는 주제의 파트 3입니다.이제부터 쿠버네티스의 kube-proxy 컴포넌트가 iptables 를 사용하여서 어떻게 트래픽을 컨트롤하는지 알아볼 것입니다.쿠버네티스 환경에서 kube-proxy 의 역할을 이해하고 iptables를 사용하여 어떻게 트래픽을 컨트롤하는지 아는 것은 매우 중요한 부분입니다.Note : 트래픽 흐름을 제어하는데 사용할 수 있는 툴과 플러그인은 매우 다양하지만 여기서는 kube-proxy + iptables 콤보를 이용해서 알아보겠습니다.일단 먼저 쿠버네티스에서 제공하는 다양한 커뮤니케이션 모델들과 그들의 구현에 대해서 알아보겠습니다.만약 여러분이 Service, ClusterIP, Noteport 의 개념에 대해서 잘 알고계신다면, kube-proxy/iptables 챕터로 넘어가도 되겠습니다!Part 1 — Basic container networkingPart 2 — Calico CNIPart 3: Pod-to-Pod Pod-to-External Pod-to-Service External-to-Pod External Traffic Policy Kube-Proxy iptable rules processing flow Network Policy basicsKube-Proxy (iptable mode)쿠버네티스에서 Service 를 구현하는 컴포넌트는 Kube-proxy 입니다.모든 노드에 존재하며 파드와 서비스 간에 다양한 종류의 필터링과 NAT 를 수행할 수 있도록 복잡한 iptables 룰들이 프로그램되어있습니다. NAT (Network Address Translation) 란?→ IP 패킷에 있는 출발지 및 목적지의 IP 주소와 TCP/UDP 포트 숫자 등을 바꿔 재기록하면서 네트워크 트래픽을 주고 받게하는 기술입니다. iptables 이란?→ 커널 상에서 netfilter 패킷 필터링 기능을 제어하는 기능입니다. 패킷의 해더를 보고 그 전체 패킷의 방향을 결정하게 됩니다.여러분이 쿠버네티스 클러스터의 노드에 가서 iptables-save 명령어를 날리게 되면, 쿠버네티스 혹은 다른 프로그램들에 의해 삽입된 룰들을 확인하실 수 있을 겁니다.그 중 가장 중요한 Chain 들은 KUBE-SERVICES, KUBE-SVC-* 와 KUBE-SEP-* 입니다. KUBE-SERVICES 는 서비스 패킷들의 앤트리 포인트입니다. 목적지의 IP:Port 를 매칭해주고 적합한 KUBE-SVC-* 체인으로 패킷을 보내줍니다. KUBE-SVC-* 체인은 로드밸런서 역할을 수행하며 패킷들을 KUBE-SEP-* 체인으로 동일하게 보내줍니다. 각각의 KUBE-SVC-* 는 KUBE-SEP-* 체인과 같은 개수가 존재하는데, 이는 엔드포인트의 수와 동일합니다. KUBE-SEP-* 체인은 Service EndPoint 를 뜻합니다. 이는 단순하게 DNAT 를 수행하는데 즉, 서비스 IP:Port 를 파드의 엔드포인트 IP:Port 로 변환해줍니다.What is DNAT and SNAT DNAT : Destination NAT 도착지 주소를 변경하는 NAT SNAT : Source NAT 출발지 주소를 변경하는 NAT DNAT 에 대해서는, conntrack 가 시작되고 상태 머신을 이용하여 상태 연결을 기록하기 시작합니다. 변경된 대상 주소를 기억하고, 반환 패킷이 돌아왔을 때 다시 변경해야 하므로 상태가 필요합니다.Iptables 는 패킷의 운명을 결정하기 위해서 conntrack 의 상태 (cstate)에 의존합니다.아래의 4가지 conntrack 상태는 특히 중요합니다. NEW : conntrack 는 SYN 패킷을 받았을 때 발생하는 이 패킷에 대해서 전혀 모릅니다. ESTABLISHED : conntrack 는 패킷이 handshake 가 완료된 이후에 발생하는 established connection에 속한다는 것을 알고 있습니다. RELATED : 패킷은 어떠한 연결에도 속해있지 않지만 다른 연결과 제휴를 맺고있습니다. 이는 FTP와 같은 프로토콜에 특히 굉장히 유용합니다. INVALID : 패킷에 관하여 어떠한 것이 문제가 있고 conntrack 은 어떻게 처리해야하는지 모릅니다. 이 상태는 Kubernetes 상에서 중요한 역할을 수행합니다.아래는 pod 와 서비스 간에 어떻게 TCP 연결이 이루어지는지를 나타냅니다.순차적인 이벤트는 아래와 같습니다. 왼쪽에 있는 클라이언트 Pod 는 서비스(2.2.2.10:80) 로 패킷을 보냅니다 패킷은 클라이언트 노드에 있는 iptable 룰을 통과합니다. 그리고 목적지는 pod IP, 1.1.1.20:80 으로 변경됩니다. 서버 Pod 는 패킷을 처리하고 다시 패킷을 1.1.1.10 을 목적지로 보냅니다. 패킷은 클라이언트 노드로 돌아오고, conntrack 은 패킷을 인식하고 출발지 주소를 2.2.2.10:80 으로 다시 수정합니다. 클라이언트 Pod 는 응답 패킷을 받습니다.iptables리눅스 운영체제에서 방화벽 운영은 netfilter 를 사용하여 운영됩니다. 이는 커널 모듈에서 어떤 패킷들이 들어오고 나갈지를 허용하는 부분입니다.iptables 는 단순히 netfilter 의 인터페이스입니다. 이 두가지는 종종 같은 것으로 고려되기도 합니다. 더 나은 관점은 이들을 Backend(netfilter)와 Frontend(iptables)로 바라보는 것입니다.Chains각각의 체인들은 특정 업무에 대한 책임이 있습니다. PREROUTING 이 체인은 패킷이 네트워크 인터페이스에 도착하자마자 어떤 일이 발생할지를 결정합니다. 여러가지 옵션이 있는데, 예를 들어 패킷을 바꾼다거나(아마도 NAT), 패킷을 누락시킨다거나, 아무것도 하지않고 지나가게해서 다른 곳에서 처리하도록 하는 것들이 있습니다. INPUT 이 체인은 가장 인기있는 체인 중 하나이고 우리의 컴퓨터를 해치려고 하는 나쁜 요소들을 피하기 위해서 항상 거의 엄격한 규칙을 포함하고 있습니다. 만약 당신이 포트를 열거나 닫고 싶다면, 이 부분이 당신이 수행해야하는 부분입니다. FORWARD 이 체인은 이름에서 확인할 수 있듯이 패킷의 포워딩에 책임이 있습니다. 우리가 만약 컴퓨터를 라우터로 사용하고 싶으면, 이 체인이 우리가 적용해야하는 부분입니다. OUTPUT 이 체인은 다른 많은 웹 브라우징이 담당하는 부분입니다. 이 체인이 허락하지 않는다면 당신은 하나의 패킷도 보낼 수 없습니다. 포트가 통신할지 말지 관계없이, 수 많은 옵션들이 존재합니다. 만약 응용 프로그램이 어떤 포트를 사용할지 확실하지 않다면, 아웃바운드 트래픽을 제한하는데 가장 좋은 방법입니다. POSTROUTING 이 체인은 패킷이 컴퓨터를 떠나기 전에 마지막으로 추적을 남기는 곳입니다. 여러 작업 중에서 우리가 원하는데로 패킷이 잘 흘러가는지를 확인하는데 사용됩니다. FORWARD 체인은 리눅스 서버에서 ip_forward 가 enabled 일때만 사용이 가능합니다. 따라서 Kubernetes 클러스터를 구축하고 디버깅하는데 아래의 커맨드는 매우 중요합니다.node-1$ sysctl -w net.ipv4.ip_forward=1net.ipv4.ip_forward = 1node-1$ cat /proc/sys/net/ipv4/ip_forward1위의 변화는 지속적이지 않습니다. 리눅스 시스템에서 해당 IP를 영구적으로 이용가능하게 하기위해서는 /etc/sysctl.conf 파일을 아래와 같이 수정해야합니다.net.ipv4.ip_forward = 1tables이제부터 우린 NAT 테이블에 초점을 맞출 예정이지만, 아래는 사용가능한 테이블들에 대한 설명입니다. Filter 기본 테이블입니다. 이 테이블에서는 패킷이 당신의 컴퓨터로 들어올지 말지를 결정합니다. 만약 포트를 막아서 더 이상 아무것도 받고싶지 않다면 이 부분을 멈춰야합니다. Nat 이 테이블은 두번째로 인기있는 테이블이고 새로운 연결을 생성하는데 책임이 있습니다. Network Address Translation 의 줄임말이기도 합니다. 만약 당신이 이 용어에 익숙하지 않아도 괜챃습니다. 아래에서 자세하게 설명하겠습니다. Mangle 특별한 패킷만을 다룹니다. 이 테이블은 들어오고 나가는 패킷 내부의 무언가를 변경합니다. Raw 이 테이블은 이름에서 추측할 수 있듯이 raw 패킷을 다룹니다. 주로 연결 상태를 추적합니다. 아래에서 SSH 연결에서 성공한 패킷을 허가하는 예시를 살펴보겠습니다. Security Filter Table 다음으로 컴퓨터 보안을 담당합니다. SELinux 를 포함하고 있습니다. 만약 이 용어와 익숙하지 않다면, 이것은 현대 리눅스 배포판에서 강력한 보안툴입니다. 만약 iptables 에 대해서 조금 더 자세하기 알고 싶다면 이 칼럼을 읽어주시기 바랍니다.iptable configuration in Kubernetes자, 이제부터 2개의 래플리카를 가진 Nginx 애플리케이션을 minikube에 배포해보고 iptable 규칙을 덤프해봅시다.서비스 타입 : NodePortmaster $ kubectl get svc webappNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEwebapp NodePort 10.103.46.104 &amp;lt;none&amp;gt; 80:31380/TCP 3d13hmaster $ kubectl get ep webapp NAME ENDPOINTS AGEwebapp 10.244.120.102:80,10.244.120.103:80 3d13hmaster $위와 같이 Endpoint 를 조회해보면, ClusterIP 는 어디에도 존재하지 않습니다. 다만, 아래에서 볼 수 있듯이 virtual IP 는 iptable Kubernetes 에 존재하고 CoreDNS에 DNS 항목을 추가합니다.master $ kubectl exec -i -t dnsutils -- nslookup webapp.defaultServer: 10.96.0.10Address: 10.96.0.10#53Name: webapp.default.svc.cluster.localAddress: 10.103.46.104패킷에 필터링과 NAT를 하기위해서 Kubernetes 는 iptable에 커스텀 체인인 KUBE-SERVICES를 생성할 것입니다.아래에서 볼 수 있듯이 해당 체인은 모든 PREROUTING AND OUTPUT 트래픽을 커스텀 체인인 **KUBE-SERVIES** 로 리다이렉트 할 것 입니다.$ sudo iptables -t nat -L PREROUTING | column -tChain PREROUTING (policy ACCEPT)target prot opt source destinationcali-PREROUTING all -- anywhere anywhere /* cali:6gwbT8clXdHdC1b1 */**KUBE-SERVICES** all -- anywhere anywhere /* kubernetes service portals */DOCKER all -- anywhere anywhere ADDRTYPE match dst-type LOCALKUBE-SERVICES 체인 훅을 패킷 필터링과 NAT 로 사용한 후에, Kubernetes 는 자신의 서비스로 들어오는 트래픽을 감지할 수 있고 SNAT/DNAT를 올바르게 적용할 수 있습니다.**KUBE-SERVICES** 체인 마지막에는, 특정 서비스 타입인 NODEPORT 에서 트래픽을 조작하기 위해서 **KUBE-NODEPORTS** 라는 커스텀 체인을 설치해줍니다.만약 트래픽이 ClusterIP 서비스 타입으로 들어오는 것이라면, KUBE-SVC-2IRACUALRELARSND 체인이 트레픽을 처리할 것입니다. 그것이 아니라면 그 다음 체인인 KUBE-NODEPORTS 체인에서 트레픽을 처리할 것입니다.$ sudo iptables -t nat -L KUBE-SERVICES | column -tChain KUBE-SERVICES (2 references) target prot opt source destination **KUBE-MARK-MASQ** tcp -- !10.244.0.0/16 10.103.46.104 /* default/webapp cluster IP */ tcp dpt:www **KUBE-SVC-2IRACUALRELARSND** tcp -- anywhere 10.103.46.104 /* default/webapp cluster IP */ tcp dpt:www **KUBE-NODEPORTS** all -- anywhere anywhere /* kubernetes service nodeports; NOTE: this must be the last rule in this chain */ ADDRTYPE match dst-type LOCAL자, 그럼 이제 KUBE-NODEPORTS 부분에 대해서 알아봅시다.(제 환경 같은 경우는 k8s v1.21.6 버전을 사용하고있는데요, KUBE-NODE-PORT 라는 이름의 체인으로 등록이 되어있습니다. K8S 버전에 따라 상이한 부분이 있는 듯 합니다.)$ sudo iptables -t nat -L KUBE-NODEPORTS | column -tChain KUBE-NODEPORTS (1 references) target prot opt source destination KUBE-MARK-MASQ tcp -- anywhere anywhere /* default/webapp */ tcp dpt:31380KUBE-SVC-2IRACUALRELARSND tcp -- anywhere anywhere /* default/webapp */ tcp dpt:31380이제 이 부분부터는 ClusterIP 와 NodePort 의 처리 과정이 동일합니다.아래에서 나오는 iptable의 다이어그램을 참고해주세요# Statistic mode random -&amp;gt; Random load-balancing between endpoints$ sudo iptables -t nat -L KUBE-SVC-2IRACUALRELARSND | column -tChain KUBE-SVC-2IRACUALRELARSND (2 references) target prot opt source destination KUBE-SEP-AO6KYGU752IZFEZ4 all -- anywhere anywhere /* default/webapp */ statistic mode random probability 0.50000000000KUBE-SEP-PJFBSHHDX4VZAOXM all -- anywhere anywhere /* default/webapp */$ sudo iptables -t nat -L KUBE-SEP-AO6KYGU752IZFEZ4 | column -tChain KUBE-SEP-AO6KYGU752IZFEZ4 (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.120.102 anywhere /* default/webapp */ DNAT tcp -- anywhere anywhere /* default/webapp */ tcp to:10.244.120.102:80$ sudo iptables -t nat -L KUBE-SEP-PJFBSHHDX4VZAOXM | column -tChain KUBE-SEP-PJFBSHHDX4VZAOXM (1 references) target prot opt source destination KUBE-MARK-MASQ all -- 10.244.120.103 anywhere /* default/webapp */ DNAT tcp -- anywhere anywhere /* default/webapp */ tcp to:10.244.120.103:80$ sudo iptables -t nat -L KUBE-MARK-MASQ | column -tChain KUBE-MARK-MASQ (24 references) target prot opt source destination MARK all -- anywhere anywhere MARK or 0x4000Note : 가독성을 위해서 결과를 잘라내어 필요한 부분들만 보여주는 것입니다.ClusterIP: KUBE-SERVICES → KUBE-SVC-XXXX → KUBE-SEP-XXXNodePort: KUBE-SERVICES → KUBE-NODEPORTS → KUBE-SVC-XXX → KUBE-SEP-XXXNote: NodePort 서비스는 내부와 외부 트래픽 관리를 위해 할당된 ClusterIP 가 있습니다.시각적으로 표현된 도표는 아래와 같습니다.ExternalTrafficPolicy: Local이전에 논의된 것 처럼, “externalTrafficPolicy: Local” 을 사용하는 것은 Source IP 를 보존할 수 있고 Local endpoint 가 없는 곳에서 들어오는 패킷은 drop 할 수 있습니다.즉, 외부 트래픽에 대한 응답으로 Service가 노드 안 (Local)에서만 응답을 할 것인지, 아니면 Cluster 전체 (Cluster) 로 나아가서 응답할지 결정하는 옵션입니다.추가적으로 externalTrafficPolicy : Cluster 로 전체 응답하는 것이 Default 값입니다.local endpoint 가 없는 경우에 대해서 iptable을 조금 더 들여다 보죠.master $ kubectl get nodesNAME STATUS ROLES AGE VERSIONminikube Ready master 6d1h v1.19.2minikube-m02 Ready &amp;lt;none&amp;gt; 85m v1.19.2이제 externalTrafficPolicy Local 을 가진 Nginx 을 배포해보겠습니다!master $ kubectl get pods nginx-deployment-7759cc5c66-p45tz -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATESnginx-deployment-7759cc5c66-p45tz 1/1 Running 0 29m 10.244.120.111 minikube &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;externalTrafficPolicy 를 확인해보죠,master $ kubectl get svc webapp -o wide -o jsonpath={.spec.externalTrafficPolicy}Local서비스를 가져와보겠습니다.master $ kubectl get svc webapp -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORwebapp NodePort 10.111.243.62 &amp;lt;none&amp;gt; 80:30080/TCP 29m app=webserver그럼 이제 minikube-m02 노드에 적용되어있는 iptables 를 확인해보게습니다.local endpoint 가 아니라면 패킷을 DROP 하는 룰이 적용되어있을 것입니다!$ sudo iptables -t nat -L KUBE-NODEPORTSChain KUBE-NODEPORTS (1 references)target prot opt source destinationKUBE-MARK-MASQ tcp — 127.0.0.0/8 anywhere /* default/webapp */ tcp dpt:30080**KUBE-XLB-2IRACUALRELARSND** tcp — anywhere anywhere /* default/webapp */ tcp dpt:30080KUBE-XLB-2IRACUALRELARSND 를 확인해보겠습니다.$ sudo iptables -t nat -L KUBE-XLB-2IRACUALRELARSNDChain KUBE-XLB-2IRACUALRELARSND (1 references)target prot opt source destinationKUBE-SVC-2IRACUALRELARSND all — 10.244.0.0/16 anywhere /* Redirect pods trying to reach external loadbalancer VIP to clusterIP */KUBE-MARK-MASQ all — anywhere anywhere /* masquerade LOCAL traffic for default/webapp LB IP */ ADDRTYPE match src-type LOCALKUBE-SVC-2IRACUALRELARSND all — anywhere anywhere /* route LOCAL traffic for default/webapp LB IP to service chain */ ADDRTYPE match src-type LOCAL**KUBE-MARK-DROP** all — anywhere anywhere /* default/webapp has no local endpoints */조금 더 자세히 들여다보면, Cluster Level의 트래픽에서는 전혀 문제가 없습니다. 오직 nodePort 트래픽이 이 노드에서 drop 될 것 입니다.‘minikube’ 노드(master) 의 iptable 룰입니다.$ sudo iptables -t nat -L KUBE-NODEPORTSChain KUBE-NODEPORTS (1 references)target prot opt source destinationKUBE-MARK-MASQ tcp — 127.0.0.0/8 anywhere /* default/webapp */ tcp dpt:30080KUBE-XLB-2IRACUALRELARSND tcp — anywhere anywhere /* default/webapp */ tcp dpt:30080$ sudo iptables -t nat -L KUBE-XLB-2IRACUALRELARSNDChain KUBE-XLB-2IRACUALRELARSND (1 references)target prot opt source destinationKUBE-SVC-2IRACUALRELARSND all — 10.244.0.0/16 anywhere /* Redirect pods trying to reach external loadbalancer VIP to clusterIP */KUBE-MARK-MASQ all — anywhere anywhere /* masquerade LOCAL traffic for default/webapp LB IP */ ADDRTYPE match src-type LOCALKUBE-SVC-2IRACUALRELARSND all — anywhere anywhere /* route LOCAL traffic for default/webapp LB IP to service chain */ ADDRTYPE match src-type LOCALKUBE-SEP-5T4S2ILYSXWY3R2J all — anywhere anywhere /* Balancing rule 0 for default/webapp */$ sudo iptables -t nat -L KUBE-SVC-2IRACUALRELARSNDChain KUBE-SVC-2IRACUALRELARSND (3 references)target prot opt source destinationKUBE-SEP-5T4S2ILYSXWY3R2J all — anywhere anywhere /* default/webapp */Headless Services Kubernetes Docs 에서 가져온 내용가끔씩 로드밸런싱이 필요없고 Service IP 하나만 필요할 때가 있습니다. 이런 경우, 당신은 Cluster IP (.spec.clusterIP) 에 “None”을 특정해서, ‘headless’ 라는 용어를 가진 서비스를 만들 수 있습니다.Kubernetes의 구현체에 묶이지 않고, 다른 Discovery Mechanisms 사용할 때 헤드리스 서비스 인터페이스를 사용할 수 있습니다.헤드리스 Services 에서 Cluster IP는 할당되지 않고, Kube-proxy 는 이 서비스를 처리하지 않습니다. 또한 플랫폼 상에서의 로드벨런싱 혹은 Proxying 은 존재하지 않습니다.DNS가 어떻게 자동으로 구성되는지는 서비스가 Selector 를 가지고 있는지에 따라 달라집니다.With selectorsSelector 가 정의된 헤드리스 서비스에서는 엔드포인트 컨트롤러가 API를 이용해 Endpoints 를 기록하고 Service 뒤쪽에 위치한 Pod 로 직접 찌를 수 있도록 DNS 구성을 변경합니다.master $ kubectl get svc webapp-hsNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEwebapp-hs ClusterIP None &amp;lt;none&amp;gt; 80/TCP 24smaster $ kubectl get ep webapp-hsNAME ENDPOINTS AGEwebapp-hs 10.244.120.109:80,10.244.120.110:80 31sWithout selectorsSelector 가 정의되지 않은 헤드리스 서비스의 경우 엔드포인트 컨트롤러가 Endpoint 리소스를 생성하지 않습니다.하지만 DNS 시스템은 다음 중 하나를 선택하여 구성합니다. ExternalName type 서비스인 경우 CNAME 레코드 반환 다른 모든 타입의 서비스와 이름을 공유하는 아무 Endpoint 의 A 레코드를 반환만약 하나 혹은 그 이상의 클러스터 노드로 라우팅 해주는 External IP들이 존재한다면, Kubernetes 서비스는 이 external IPs 들로 노출될 수 있습니다.Service Port 의 external IP 를 통해 클러스터로 들어오는 트래픽들은 Service 엔드포인트 중 하나로 라우팅 될 것입니다. external IPs 는 Kubernetes 에 의해 관리되지 않고, 클러스터 관리자가 이들을 책임지고 관리해야합니다.Network Policy이정도 살펴보았으면, 아마 Kubernetes 에서 Network Policy 가 어떤식으로 구현되어있는지 감이 오실 것입니다.네 맞습니다, iptables 가 또 나옵니다. 이번에는 Kube-proxyt가 아니라 CNI 가 Network Policy 구현에 신경을 쓰게 됩니다.이번 섹션은 Calico(Part 2) 가 추가 되어야만 하지만, 지금쯤 Network Policy의 디테일이 나오는 것이 맞다고 생각했습니다.자, 3가지 서비스를 만들어보죠. Frontend, Backend 그리고 DB기본적으로, Pods 는 non-isolated 되어있습니다. 즉, 어디서 들어오든지 모든 트래픽을 수용합니다.하지만, Frontend 와 DB간의 트래픽 흐름을 피하기 위해서는 FrondEnd 파드에서 DB 파드로의 트래픽을 제어할 필요가 있습니다.Network Policy 구성에 대해서 이해하기 위해서는 이 칼럼을 읽어보는 것을 권장합니다. 이번 섹션은 Netowork Policy 구성에 대한 딥 다이브 대신 Kubernetes에서 Network Policy가 어떻게 구현되었는지에 초점을 맞출 것입니다.저는 먼저 DB와 Frontend 의 네트워크를 제어하기 위해서 Network Policy를 적용해두었습니다.아래의 결과에서는 frontend 와 db 파드간의 연결은 없을 것입니다.Note: 위의 그림은 서비스에 수많은 파드가 붙을 수 있다는 것을 보여주기 위해서 파드 아이콘 대신에 서비스 아이콘을 사용했습니다. 하지만 실제로 제어 규칙은 파드 단위로 적용됩니다.master $ kubectl exec -it frontend-8b474f47-zdqdv -- /bin/sh$ curl backendbackend-867fd6dff-mjf92$ curl dbcurl: (7) Failed to connect to db port 80: Connection timed out하지만, Backend 는 db 서비스로 문제없이 도달할 수 있습니다.master $ kubectl exec -it backend-867fd6dff-mjf92 -- /bin/sh$ curl dbdb-8d66ff5f7-bp6kf이제 그럼 Network Policy에 대해서 들여다보죠.만약 allow-db-access 값이 true 로 label 이 붙어있다면, 서비스로부터 들어오는 트레픽을 허용하는 것입니다.---apiVersion: networking.k8s.io/v1kind: NetworkPolicymetadata: name: allow-db-accessspec: podSelector: matchLabels: **app: &quot;db&quot;** policyTypes: - Ingress ingress: - from: - podSelector: matchLabels: **networking/allow-db-access: &quot;true&quot;**---apiVersion: apps/v1kind: Deploymentmetadata: name: backend labels: app: backendspec: replicas: 1 selector: matchLabels: app: backend template: metadata: labels: app: backend **networking/allow-db-access: &quot;true&quot;** spec: volumes: - name: workdir emptyDir: {} containers: - name: nginx image: nginx:1.14.2 imagePullPolicy: IfNotPresent ports: - containerPort: 80 volumeMounts: - name: workdir mountPath: /usr/share/nginx/html initContainers: - name: install image: busybox imagePullPolicy: IfNotPresent command: [&#39;sh&#39;, &#39;-c&#39;, &quot;echo $HOSTNAME &amp;gt; /work-dir/index.html&quot;] volumeMounts: - name: workdir mountPath: &quot;/work-dir&quot;...Calico 는 Kubernetes 의 Network Policy 를 Calico 네이티브 한 포맷으로 변경합니다.master $ calicoctl get networkPolicy --output yamlapiVersion: projectcalico.org/v3items:- apiVersion: projectcalico.org/v3 kind: NetworkPolicy metadata: creationTimestamp: &quot;2020-11-05T05:26:27Z&quot; name: knp.default.allow-db-access namespace: default resourceVersion: /53872 uid: 1b3eb093-b1a8-4429-a77d-a9a054a6ae90 spec: ingress: - action: Allow destination: {} source: selector: projectcalico.org/orchestrator == &#39;k8s&#39; &amp;amp;&amp;amp; networking/allow-db-access == &#39;true&#39; order: 1000 selector: projectcalico.org/orchestrator == &#39;k8s&#39; &amp;amp;&amp;amp; app == &#39;db&#39; types: - Ingresskind: NetworkPolicyListmetadata: resourceVersion: 56821/56821iptables 규칙은 filter 테이블을 사용하여 정책을 강화하는데 중요한 역할을 합니다.Calico 가 강화된 개념의 ipset 을 사용하기 때문에 이 상황에서 리버스 엔지니어링을 하는 것은 매우 힘든 부분입니다. iptables 규칙에서 패킷이 backend 에서 출발해야만 db 파드로 들어오는 것을 허용하는 규칙을 볼 수 있습니다. 그리고 정확히 이 부분이 Network Policy가 하고 있는 부분입니다.calicoctl 을 사용해서 Workload 엔드포인트 디테일을 확인해보죠master $ calicoctl get workloadEndpointWORKLOAD NODE NETWORKS INTERFACE backend-867fd6dff-mjf92 minikube 10.88.0.27/32 cali2b1490aa46a db-8d66ff5f7-bp6kf minikube 10.88.0.26/32 cali95aa86cbb2a frontend-8b474f47-zdqdv minikube 10.88.0.24/32 cali505cfbeac50cali95aa86cbb2a - db 파드에서 사용중인 Veth 쌍 중 Host 사이드에 위치하고 있는 것그럼 이제 해당 인터페이스와 연관있는 iptables 규칙을 확인해봅시다!$ sudo iptables-save | grep cali95aa86cbb2a:cali-fw-cali95aa86cbb2a - [0:0]:cali-tw-cali95aa86cbb2a - [0:0]-A cali-from-wl-dispatch -i cali95aa86cbb2a -m comment --comment &quot;cali:R489GtivXlno-SCP&quot; -g cali-fw-cali95aa86cbb2a-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:3XN24uu3MS3PMvfM&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:xyfc0rlfldUi6JAS&quot; -m conntrack --ctstate INVALID -j DROP-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:wG4_76ot8e_QgXek&quot; -j MARK --set-xmark 0x0/0x10000-A cali-fw-cali95aa86cbb2a -p udp -m comment --comment &quot;cali:Ze6pH1ZM5N1pe76G&quot; -m comment --comment &quot;Drop VXLAN encapped packets originating in pods&quot; -m multiport --dports 4789 -j DROP-A cali-fw-cali95aa86cbb2a -p ipencap -m comment --comment &quot;cali:3bjax7tRUEJ2Uzew&quot; -m comment --comment &quot;Drop IPinIP encapped packets originating in pods&quot; -j DROP-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:0pCFB_VsKq1qUOGl&quot; -j cali-pro-kns.default-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:mbgUOxlInVlwb2Ie&quot; -m comment --comment &quot;Return if profile accepted&quot; -m mark --mark 0x10000/0x10000 -j RETURN-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:I7GVOQegh6Wd9EMv&quot; -j cali-pro-ksa.default.default-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:g5ViWVLiyVrKX91C&quot; -m comment --comment &quot;Return if profile accepted&quot; -m mark --mark 0x10000/0x10000 -j RETURN-A cali-fw-cali95aa86cbb2a -m comment --comment &quot;cali:RBmQDo38EoPmxJ0I&quot; -m comment --comment &quot;Drop if no profiles matched&quot; -j DROP-A cali-to-wl-dispatch -o cali95aa86cbb2a -m comment --comment &quot;cali:v3sEoNToLYUOg7M6&quot; -g cali-tw-cali95aa86cbb2a-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:eCrqwxNk3cKw9Eq6&quot; -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:_krp5nzavhAu5avJ&quot; -m conntrack --ctstate INVALID -j DROP-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:Cu-tVtfKKu413YTT&quot; -j MARK --set-xmark 0x0/0x10000-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:leBL64hpAXM9y4nk&quot; -m comment --comment &quot;Start of policies&quot; -j MARK --set-xmark 0x0/0x20000-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:pm-LK-c1ra31tRwz&quot; -m mark --mark 0x0/0x20000 -j cali-pi-_tTE-E7yY40ogArNVgKt-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:q_zG8dAujKUIBe0Q&quot; -m comment --comment &quot;Return if policy accepted&quot; -m mark --mark 0x10000/0x10000 -j RETURN-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:FUDVBYh1Yr6tVRgq&quot; -m comment --comment &quot;Drop if no policies passed packet&quot; -m mark --mark 0x0/0x20000 -j DROP-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:X19Z-Pa0qidaNsMH&quot; -j cali-pri-kns.default-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:Ljj0xNidsduxDGUb&quot; -m comment --comment &quot;Return if profile accepted&quot; -m mark --mark 0x10000/0x10000 -j RETURN-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:0z9RRvvZI9Gud0Wv&quot; -j cali-pri-ksa.default.default-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:pNCpK-SOYelSULC1&quot; -m comment --comment &quot;Return if profile accepted&quot; -m mark --mark 0x10000/0x10000 -j RETURN-A cali-tw-cali95aa86cbb2a -m comment --comment &quot;cali:sMkvrxvxj13WlTMK&quot; -m comment --comment &quot;Drop if no profiles matched&quot; -j DROP$ sudo iptables-save -t filter | grep cali-pi-_tTE-E7yY40ogArNVgKt:cali-pi-_tTE-E7yY40ogArNVgKt - [0:0]-A cali-pi-_tTE-E7yY40ogArNVgKt -m comment --comment &quot;cali:M4Und37HGrw6jUk8&quot; -m set --match-set cali40s:LrVD8vMIGQDyv8Y7sPFB1Ge src -j MARK --set-xmark 0x10000/0x10000-A cali-pi-_tTE-E7yY40ogArNVgKt -m comment --comment &quot;cali:sEnlfZagUFRSPRoe&quot; -m mark --mark 0x10000/0x10000 -j RETURNipset 을 확인해보면, backend pod ip 인 10.88.0.27 로부터 db pod 로 들어오는 것만 허용한다는 것이 명확히 보입니다.minikube $ ipset listName: cali40s:LrVD8vMIGQDyv8Y7sPFB1GeType: hash:netRevision: 6Header: family inet hashsize 1024 maxelem 1048576Size in memory: 408References: 3Number of entries: 1Members:10.88.0.27References [https://kubernetes.io](https://kubernetes.io/) https://www.projectcalico.org/ https://rancher.com/ http://www.netfilter.org/" }, { "title": "[CISCO 네트워킹] 12. IPv6 로 떠나는 여행", "url": "/posts/network-study-ch12/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-05-29 00:00:00 +0900", "snippet": "IPv6 의 필요성 IPv4 주소의 주소공간 부족 IPv6는 IPv4 와 엄청난 양의 주소 차이가 남 NAT Network Address Translation 내부에서는 Private IP 주소, 외부에서는 공인 IP 주소를 사용할 수 있게 하는 것Subnet 서브넷 마스크를 이용해서 네트워크를 잘게 쪼게서 사용하여 주소를 아낌IPv6의 주요 특징 IP 주소 범위 32 bit → 128 bit IP 주소의 자동 구성 Stateless Auto Configuration 를 사용하여 어떤 장비든 네트워크에 접속만 되면 자동으로 주소를 구성할 수 있음 보안 IPSec이 디폴트라서 어디서나 보안을 적용할 수 있음 Mobility 더 효과적이고 간편함 브로드캐스트 브로드캐스트가 없어지고, 멀티캐스트가 그 역할을 대신 함 브로드캐스트는 이전에 필요악이었음. 16진수를 사용함 2001:0db8:010f:0001:0000:0000:0000:0d0c 브로드 캐스트가 사라지고 애니케스트가 생김 아무나 제일 먼저 받는 녀석이 임자가 되는 형태 " }, { "title": "[etcd] [Docs Learning] Learner Design", "url": "/posts/etcd-learning-learner-design/", "categories": "etcd, docs", "tags": "etcd, docs, learning, learner design, client, grpc, http", "date": "2023-05-23 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.Docs “Mitigating common challenges with membership reconfiguration”etcd LearnerBackground (Common Challenges)1. New Cluster member overloads leader 새롭게 etcd 멤버가 데이터가 없는 상태로 join 하게 되면, 리더들의 로그를 catch 하도록 리더한테 많은 양의 업데이트를 요구함 → leader overloaded 2. Network Partitions scenarios 네트워크 파티션이 발생했을 때, 리더가 만약 active quorum 상태에 유지된다면 클러스터는 계속 작동할 것임2.1 Leader isolation 리더와 같은 파티션에 리더의 active follower 가 없는 상태. (Lost quorum) 리더는 follower 로 돌아가고, 이 상황은 클러스터의 가용성에 영향을 미침 Leader Election 이 발생하면서 새로운 리더를 선출하게 됨2.2 Cluster Split 3+1 새로운 노드가 3개의 노드가 있는 클러스터로 조인되었을 때, 쿼럼은 3으로 증가함 (2(3/2+1)) 그리고 네트워크 파티션이 발생했을 경우 만약 새로운 노드가 리더와 같은 파티션에 존재한다면, 리더는 여전히 active quorum 을 가지고 있음. 따라서 클러스터는 계속 작동할 것임. 새로운 멤버가 &quot;리더 파티션&quot;에 속함 ⇒ 리더가 쿼럼을 유지함 ⇒ 클러스터 이용가능2.3 Cluster Split 2+2 2(follower, leader) + 2(follower, new follower) 위와 같은 상황에서는 리더는 2개의 active follower 를 가짐. 즉, 쿼럼의 값이 3인데 이를 채우지 못함. 따라서 리더는 다시 follower로 돌아갈 것이고, 리더 재선출이 발생함2.4 Quorom Lost 네트워크 파티션이 발생하고나서 새로운 멤버가 추가되는 경우 새로운 노드가 시작하기 전에는 클러스터는 오직 2개의 active node를 가지고 있는데, 이는 쿼럼을 만족하지 못함. 즉, 새로운 노드가 추가되면서 전체 클러스터의 쿼럼은 증가하지만, 아직 새로운 노드가 시작을 못했기 때문에 active 하지 않음. → 리더 재선출을 트리거 Since member add operation can change the size of quorum, it is always recommended to “member remove” first to replace an unhealthy node.3. Cluster Misconfigurations (worse case) Membership reconfiguration etcdctl member add Starting an etcd server process with the given peer URL If the URL that given to etcdctl was invalid, it will fail member add 하면 쿼럼에는 바로 적용되어서 클러스터에서는 바뀐 쿼럼 값을 가지고 계속 판단하는데, Misconf 인 노드가 들어오면 active 는 안되고 쿼럼에는 포함되니깐 cluster가 unavailable 상태가 될 수 밖에 없음 → simple misconfiguration can fail the whole cluster into an inoperative state Can we make membership reconfiguration less disruptive by not changing the size of quorum?Can a new node be idle, only requesting the minimum updates from leader, until it catches up?Can membership misconfiguration be always reversible and handled in a more secure way (wrong member add command run should never fail the cluster)?Should an user worry about network topology when adding a new member?Can member add API work regardless of the location of nodes and ongoing network partitions?Raft Learner**Raft Learner will solve those issue** Raft 4.2.1 에서는 Learner 라는 노드의 새로운 상태를 소개함. 클러스터에 새로운 노드가 추가되었을 때 리더들의 로그를 catches up 하기 전까지 non-voting member 상태로 만들어줌 “member add –learner” -&amp;gt; Wait until lerner node catches up to leader’s logs. Until then, learner node neither votes nor counts towards quorum. member promote API Once learner node has caught up to leader’s log, “member promote” API can promote it to a normal voting node that counts towards quorum (승진!!!) etcd v3.4 learner can be promoted only when it satisfies the safety requirement. How does it check the requirement? func canPromote(idx int) bool etcd limits the total number of learners that a cluster can have. To avoid overloading the leader with log replication When a learner has caught up with leader’s progress, the learner can be promoted to a voting member using member promote API, which then counts towards the quorum (see Figure 11). etcd server validates promote request to ensure its operational safety. Only after its log has caught up to leader’s can learner be promoted to a voting member (see Figure 12). Learner only serves as a standby node until promoted: Leadership cannot be transferred to learner. Learner rejects client reads and writes (client balancer should not route requests to learner).Future work Make learner to default state Make voting-member promotion fully automatic Make learner standby failover node Make learner read-only" }, { "title": "[CISCO 네트워킹] 8. 라우팅 프로토콜과의 한판", "url": "/posts/network-study-ch08/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-05-21 00:00:00 +0900", "snippet": "RIP 라는 라우팅 프로토콜에 대한 이야기RIP (Routing Information Protocol) 라우팅 프로토콜 다이내믹 프로토콜 내부용 라우팅 프로토콜(IGP) 디스턴스 백터 알고리즘 라우터가 좋은 길을 경정하는 기준이 되는 요소 → Hop 카운트 디폴트 라우팅 업데이트 주기 → 30초RIP 장 단점 장점 표준 라우팅 프로토콜 Configuration의 간단함 메모리를 적게 사용함 단점 홉 카운트만 가지고 경로를 선택하다 보니 실수를 많이 함 자신의 라우터에서 15개 이상의 라우터를 거치는 목적지의 경우는 Unreachable 로 정의하고 데이터를 보내지 못함. → 커다란 네트워크 상에서 사용하기엔 무리 Distance-Vector 라우팅 알고리즘에서의 문제점과 해결책 한번 배운 라우팅 테이블을 계속 전달하기 때문에 업데이트가 모든 네트워크에 전달되는 시간(Convergence Time)이 많이 걸림. → Looping 이 발생할 수 있음 해당 문제를 해결하기 위한 대책들 Maximum Hop Count 최대 홉 카운트를 정해놓으면 루핑이 발생하더라도 멈출 수 있음 Hold Down Timer 어떤 경로가 죽었다고 판단하면 이 경로에 대한 상태를 바로 바꾸지 않고 일정 시간이 지난 다음에 바꾸겠다는 것 Split Horizon 라우팅 정보가 들어온 곳으로는 같은 정보를 내보낼 수 없다는 것 Route Poisoning 라우팅 테이블에 극약 처방 네트워크 A가 다운되면 라우터가 네트워크 A에 대한 메트릭 값을 16으로 바꾸면서 사용할 수 없게 만들어버림 Poison Reverse 라우팅 정보를 되돌려 보내기는 하되, 이 값을 무한대 값으로 쓰는 방식 디스턴스 벡터 알고리즘은 쉽고, 간편하고, 라우팅 테이블을 적게 사용하는 등 여러가지 장점이 있지만, 루핑이 발생하기 쉬움.IGRP 라우팅 프로토콜 Interior Gateway Routing Protocol 다이내믹 프로토콜 내부용 라우팅 프로토콜 (IGP) 디스턴스 벡터 알고리즘 모든 라우터에서 전부 사용 가능한 프로토콜은 아님 다섯 가지 요인을 가지고 가장 좋은 경로를 선택함 Bandwidth Delay Reliability Load MTU (Maximum Transmission Unit) → Hop 카운트만 가지고 목적지를 찾는 RIP 와는 달리 좀 더 지능적으로 경로를 선택할 수 있음 PPS Packet Per Second PPS = 1 sec / (IFG + Preamble Time + Frame Time) IFG : Inter Frame Gap Preamble Time : 프레임 앞에 붙는 서두 Frame Time : 프레임이 날아가는 시간OSPF 라우팅 프로토콜 Open Shortest Path First" }, { "title": "[etcd] [Docs Learning] Client Design", "url": "/posts/etcd-learning-client-design/", "categories": "etcd, docs", "tags": "etcd, docs, learning, cleint design, client, grpc, http", "date": "2023-05-21 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.DocsIntroduction etcd server has proven its robustness with years of failure injection testing Using Data store and etcd server, most complex application logic is already handled. But To guarantee its correctness and high availability under faulty conditions, client also need different set of intricate protocol Ideally etcd server provides one logical cluster view of many physical machines Client implements automatic failover between replicas Glossary clientv3 etcd official Go client for etcd v3 API clientv3-grpc1.0 etcd v3.1 + grpc-go v1.0.X clientv3-grpc1.7 etcd v3.2 and v3.3 + grpc-go v1.7.X cleintv3-grpc1.23 etcd v3.4 + grpc-go v1.23.X Balancer etcd client load balancer that implements retry and failover mechanism etcd client should automatically balance loads between multiple endpoints Endpoints A list of etcd server endpoints that clients can connect to. 3 or 5 client URLs at an etcd cluster Pinned endpoint When configured with multiple endpoints &amp;lt;= v3.3 Client balancer chooses only one endpoint to establish a TCP connection In v3.4 Balancer round-robin pinned endpoints for every request distribute request more evenly Client Connection TCP connection that has been established to an etcd server, via gRPC Dial Sub connection gRPC SubConn interface Each sub-connection contains a list of addresses. Transient disconnect gRPC server returns a status code of “Code Unavailable” gRPC at etcd etcd provides a gRPC API for client applications to interact with the key-value store. The gRPC API enables clients to perform operations such as reading and writing key-value pairs, watching key-value updates in real-time, and performing atomic transactions. Client Requirements Correctness If there is a failure from server faults, it never violates consistency guarantees. never write corrupted data Guarantee consistency! Liveness Clients should make progress even if the server fails or disconnects briefly. Never deadlock for waiting server Ideally Client detect unavailable servers with HTTP/2 ping and failover to other healthy node with clear error msg Effectiveness Effective with minimum resources Previous TCP connection should be gracefully closed after endpoint switch Failover mechanism should effectively predict the next replica to connect Portability (I’ve heard that portability is one of the cons for gRPC) Easy to connect with other languages Error handling between difference language bindings should be consistent “Since etcd is fully committed to gRPC, … ” In etcd, gRPC is the default protocol used for client-server communication. implementation should be closely aligned with gRPC long-term design goals Client Overview Balancer Establishes gRPC connections to an etcd cluster API Client Sends RPCs to an etcd server Error Handler Decides whether to retry a failed request or switch endpoints Initial connection (how to encode, decode and send protocol buffer message to server, hot to handle stream RPCs..) can be different by languages, BUT errors returned from etcd server will be the sameclientv3-grpc1.0 Overview Multiple TCP connections are maintained when configured with multiple etcd endpoints And pick one address and use it. The pinned address is maintained until the client object is closed. When an error occurs, balancer picks another endpoint and retry. Client receives an error No error handler Limitation Multiple TCP -&amp;gt; fast failover BUT more resources Balancer does not understand node’s health status or cluster membership balancer can get stuck with one failed or partitioned node. clientv3-grpc1.7 Overview Only one TCP connection is maintained to choose etcd server. 1. The client tries to connect to all endpoints. One connection is up, balancer pins the address, closing others When error occurred, it is sent to client error handler The client error handler takes an error from gRPC server, and decides whether to retry on the same endpoint, or to switch to other addresses. (based on error code and messages) Error handler decided and talk to Balancer Stream RPC (watch, keepalive) watch, keep-alive HTTP/2.0 keepalive is a mechanism to keep a connection open between a client and a server to avoid the overhead of opening and closing connections for each request. gRPC’s Watch is a type of streaming RPC that allows the server to send updates to the client when the requested data changes. It is often used with no timeouts to maintain the persistent connection between the client and the server, ensuring real-time updates. Stream RPCs are typically sent with no timeouts But, clients can send periodic HTTP/2 pings to check the status of a pinned endpoint. If ping does not answer, the balancer will switch to other endpoints. Limitation Unable to detect network partitions. Since partitioned gRPC server can still respond to client pings. HTTP/2 keepalive is just a simple ping mechanism and can not know about cluster membership. cluster membership Etcd cluster membership refers to the process by which individual nodes or members join or leave an etcd cluster. The etcd cluster membership is managed by the etcd Raft consensus algorithm, which ensures that all members in the cluster agree on the current state of the system. Balancer may get stuck with a partitioned node. Balancer maintains a list of unhealthy endpoints. And it hard coded as dial timeout with default value 5-second So if unhealthy server returned before 5 sec, it is still unusable. So tightly coupled with old gRPC interface, that every single gRPC dependency upgrade broke client behavior ## clientv3-grpc1.23 Overview Simplify balancer failover logic. maintaining a list of unhealthy endpoints -&amp;gt; stale simply round-robin to the next endpoint It does not assume endpoint status. -&amp;gt; no more complicated status tracking is needed 1) one sub-connection per each endpoint If there is 5 node cluster, balancer would require 5 TCP connections (same with 1.1…) More resource but provide more flexible automatically handles gRPC internal errors Limitation Caching the status of each endpoint Not list up the node’s health status " }, { "title": "[etcd] [Docs Learning] Data Model", "url": "/posts/etcd-learning-data-model/", "categories": "etcd, docs", "tags": "etcd, docs, learning, data model, data structure", "date": "2023-05-20 00:00:00 +0900", "snippet": " etcd 공식 Docs 의 Learning 문서를 보고 공부 및 해석한 내용을 기록합니다.Data Model Docs “A persistent, multi-version, concurrency-control data model”(유지되고, Multi version 이면서, 동시성 컨트롤이 되는 데이터 구조) “etcd is designed to reliably store infrequently updated data and provide reliable watch queries.”(etcd 는 빈번하지 않게 데이터를 업데이트하고 안정석있는 watch 쿼리를 위해 디자인 되었음) etcd exposes previous versions of key-value pairs to support inexpensive snapshots and watch history events (“time travel queries”).etcd data storage methodologies 모든 과거의 key 들은 접근 가능하고 수정 이후에도 확인 가능함Persistent data structure Persistent data structure(or not ephemeral data structure) 는 수정이 되었던 모든 버전의 데이터를 보존하는 자료 구조를 의미함 Persistent 하지 않은 구조는 ephemeral 이라고 부름 ref. https://en.wikipedia.org/wiki/Persistent_data_structureLogical View 저장소의 Logical view 는 binary key space 이다. Key 공간들은 lexically sorting 되어있어서 range 쿼리에 유리함 lexicographic sorting : 사전식 정렬 key 공간은 여러개의 revision을 유지함. 맨처음의 revision은 1이고, 여러개의 연산이 묶인 trasaction (transaction 을 하나의 단위로 봄) 단위로 revsion은 monotonically 하게 증가한다. 삭제되면 0으로 revision이 바뀜 Compaction이 수행되면 compaction revision 전에 종료된 모든 생성이 제거되고, 최신 생성을 제외하고 compaction revision 전에 설정된 값들이 제거됨Physical ViewB+tree vs B Tree B+Tree 는 leaf node에만 데이터를 저장하고 다른 노드에는 key 만 저장함. 반면에 B Tree 는 모든 노드에 데이터를 저장함 → B+Tree 가 key를 더 많이 가지고 있을 수 있다. (파란색 블록이 원래는 B Tree임.) B Tree 는 User interface 임. 사용자 -&amp;gt; B Tree 의 key -&amp;gt; revisions -&amp;gt; B+Tree 의 revision -&amp;gt; value Pointer 구조! B+Tree 를 사용하면 Compaction 에 유리하고, range lookups 가 빠름 (lexical order) 실제 데이터는 B+Tree 에 저장됨 각 저장소의 revision은 이전 revision과의 delta (변경분) 만 저장을 함. single revision은 multiple keys 와 동일할 수 있다. Snapshot Isolation (SSI) =&amp;gt; MVCC (Multi-Version Concurrency Control) Lock 메커니즘 Vertical scaleup 이 아니라 horizontal 하게 scale up 하게 되면서 분산 시스템에서 데이터의 consistency 유지는 정말 중요함. 근데 그 부분은 예전부터 대두되던 문제이다. 그 해결책으로 그나마 최근에 이야기가 나오는 해결 방법! 특정 키에 대한 버전을 여러개 계속 append 함. 그럼 consistency 에 대한 추적과 update 가 필요없음! Read 에 유리하다 SSI Lock 은 어떻게 잡는가? Optimistic Lock k8s 에서만 봐도 명세를 많은 리소스들이 접근함. revision 값 (수정 시간) 을 통해서 데이터의 immutable 을 보장함 Revision 서버마다 time 은 다르기 때문에 monotonical clock 인 revision 을 사용함 " }, { "title": "[Infiniband] NCCL WARN Call to ibv_reg_mr failed 이슈 해결", "url": "/posts/infiniband-nccl-issue/", "categories": "Trouble Shooting, Infiniband, Kubernetes", "tags": "infiniband, kubernetes, k8s, kernel, linux, docker", "date": "2023-05-18 00:00:00 +0900", "snippet": "개요Infiniband 네트워크를 사용하는 클러스터에서 Multi Node 분산 학습을 실행할 때 NCCL WARN Call to ibv_reg_mr failed 에러가 발생하는 경우가 있음.아래와 같이 에러가 발생하면서 학습이 중단되는 케이스가 있다.ibvwrap.c: 106 NCCL WARN Call to ibv_reg_mr failedNCCL INFO ib_plugin.c:448 -&amp;gt; 2NCCL INFO include/net.h:23 -&amp;gt; 2NCCL INFO transport/net.cc:248 -&amp;gt; 2원인 파악NCCL 을 사용할 때 충분한 MEMLOCK 을 필요로 하기 때문에, memory lock limit 을 설정해줘야함.공식 Nvidia Docs 에서도 해당 이슈에 대한 Trouble Shooting 을 이야기해주는데, 장비의 Memory Lock 을 풀어줘야한다는 것이 제안하는 방법Memory Lock 이란? 메모리에 Lock 해둘 수 있는 최대 크기를 의미함. 메모리 Lock 은 메모리가 항상 RAM에 있고 스왑 디스크로 이동하지 않도록 하는 역할ref. https://stackoverflow.com/questions/9818755/why-would-we-need-to-lock-a-processs-address-space-in-ramMemory Lock Size 를 변경하는 법 Linux Setting ulimit 명령어로 확인이 가능함 $ ulimit -a core file size (blocks, -c) 0 data seg size (kbytes, -d) unlimited scheduling priority (-e) 0 file size (blocks, -f) unlimited pending signals (-i) 8207353 max locked memory (kbytes, -l) unlimited max memory size (kbytes, -m) unlimited open files (-n) 65536 pipe size (512 bytes, -p) 8 POSIX message queues (bytes, -q) 819200 real-time priority (-r) 0 stack size (kbytes, -s) 8192 cpu time (seconds, -t) unlimited max user processes (-u) 65536 virtual memory (kbytes, -v) unlimited file locks (-x) unlimited 위의 결과값과 같이 linux setting 자체에서 max memory size 를 설정할 수 있다. 만약 다른 값을 원한다면 /etc/security/limits.conf 파일에서 수정을 진행하면 되겠다. Docker Setting System daemon 으로 docker.daemon 을 실행한다는 가정하에, systemctl 명령어로 해당 설정값 확인이 가능하다. $ sudo systemctl show docker | grep Limit ... LimitMEMLOCK=65536 LimitMEMLOCKSoft=65536 ... 그럼 Container 에서 ulimit 을 확인하면 어떤 값을 overwrite 할까? root@container# ulimit -a | grep lock max locked memory (kbytes, -l) 64 해당 Linux setting 에 위의 Docker config를 가지고 Container 를 만들었을 때 docker config 를 사용하는 것으로 볼 수 있다. Docker setting 을 변경하려면 /etc/sysconfig/docker 파일을 수정하면 되겠다. Kubernetes 구글링을 해본 결과 아직 Kubernetes 상에서는 해당 값을 설정하는 방법은 없어보인다.. 해당 Kubernetes Issue 를 확인해보면 개발이 되는 것으로 보인다. 해결책 위의 3가지 옵션 중 하나를 선택할 수 있다. 하지만 우리 환경은 Kubernets Cluster 기반에서 돌아가고 있기 때문에, 전체적인 Memory Lock Limit 으로 설정하는 것은 매우 위험하다고 판단. 그렇기 때문에 다른 방법을 찾아보았다.IPC_LOCK 권한 설정 IPC_LOCK 권한이란? 컨테이너가 호스트 시스템의 공유 메모리 세그먼트에 대한 잠금을 설정할 수 있는 권한을 나타냄. 이 권한은 컨테이너가 특정 IPC 리소스에 액세스하고 조작할 수 있는지를 제어 학습이 도는 컨테이너에 IPC_LOCK 권한을 부여하게 되면 ulimit을 무시하고 최대의 메모리를 사용할수 있다!# CONTAINERsecurityContext: allowPrivilegeEscalation: false capabilities: add: - IPC_LOCK drop: - all privileged: false# PODsecurityContext: fsGroup: 1000 runAsGroup: 1000 runAsNonRoot: true runAsUser: 1000 위의 부분들인데, 영향을 주는 부분은 아래의 pod 권한과 drop {“all”} 부분임. 다른 것들은 각각 추가해서 테스트해봤는데 정상동작함 따라서 IB 인경우 위의 두개를 빼는 로직을 hotfix 로 추가따라서 우리는 해당 이슈를 해결하기 위해서 노드 혹은 도커의 Memory limit 을 설정하기 보다, 학습이 도는 컨테이너에 IPC_LOCK 권한을 부여하면서 해결하였음." }, { "title": "[K8S] 5.4.0-132 커널의 epoll 버그로 인한 etcd leader election 이슈", "url": "/posts/k8s-kernel-epoll-etcd-leader/", "categories": "Kubernetes", "tags": "kubernetes, k8s, etcd, kernel, linux, leader_election", "date": "2023-05-17 00:00:00 +0900", "snippet": "[k8s] 5.4.0-132 커널의 epoll 버그로 인한 etcd leader election 이슈 사용 중인 클러스터의 Control plane 은 Master[1:3] 노드로 구성되어있고 HA 구성이 되어있는 상태. Kubernetes 는 Control plane 노드에서 etcd 라는 Database 를 Kubernetes 리소스 메타데이터 저장소로 사용하고 있음. 매우 중요한 컴포넌트 ref. https://kubernetes.io/ko/docs/concepts/overview/components/이슈 개요 운영중인 클러스터에서 The API Server is burning too much error budget 이라는 에러가 발생하는 것을 알람으로 받게되었다. 그 후 클러스터에서 EtcdNoLeader 에러가 master2번과 master3번 노드에서 추가적으로 발생 5분 뒤 응답속도 정상으로 돌아오면서 클러스터 정상화 됨 kube-apiserver 접근이 지연되면서 kubectl 사용 시 &quot;Error from server: etcdserver: leader changed&quot; 에러 발생 해당 이슈가 2-30분 간격으로 계속 발생이슈 분석노드의 컨테이너에 접근하여 etcdNoLeader 발생 확인# Master 1번 노드의 etcd container에서 확인# Master 2번 노드가 정상적이지 않은 것  확인master1:~$ sudo docker exec -it 2001sdedaw170 /bin/sh# etcdctl endpoint health{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-11T11:30:25.332Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-4e9097d1-8f27-4dec-befe-932345b3a82c/x.x.x.x:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}https://x.x.x.x:2379 is healthy: successfully committed proposal: took = 11.634432mshttps://x.x.x.x:2379 is healthy: successfully committed proposal: took = 11.725419mshttps://x.x.x.x:2379 is unhealthy: failed to commit proposal: context deadline exceededError: unhealthy cluster# Master 2번 노드의 etcd container에서 확인master2:~$ sudo docker exec -it 2001287ffb70 /bin/sh# etcdctl endpoint health{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-11T11:37:22.498Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-641ebecc-a28e-498a-bdc3-574d44b9f73f/127.0.0.1:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}https://127.0.0.1:2379 is unhealthy: failed to commit proposal: context deadline exceededError: unhealthy clustermaster[1:3] 노드의 etcd docker container 의 로그를 분석함# master 1# etcdctl member list -w table{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-06T09:02:23.846Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-d0ef2028-d62b-4751-9a78-bc8505a540cc/127.0.0.1:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}Error: context deadline exceeded# etcdctl endpoint health{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-06T09:02:55.298Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-114c51ac-a69e-46aa-a718-42ae334175c9/127.0.0.1:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}https://127.0.0.1:2379 is unhealthy: failed to commit proposal: context deadline exceededError: unhealthy cluster# master 2master2:~$ sudo docker ps | grep etcda57237cfc676 quay.io/coreos/etcd:v3.4.13 &quot;/usr/local/bin/etcd&quot; 4 weeks ago Up 4 weeks etcd2master2:~$ sudo docker exec -it a57237cfc676 /bin/sh# etcdctl member list -w table+------------------+---------+-------+---------------------------+---------------------------+------------+| ID | STATUS | NAME | PEER ADDRS | CLIENT ADDRS | IS LEARNER |+------------------+---------+-------+---------------------------+---------------------------+------------+| b9f835148c70c3bb | started | etcd2 | https://x.x.x.x:2380 | https://x.x.x.x:2379 | false || dd20b5740a7a8bfc | started | etcd3 | https://x.x.x.x:2380 | https://x.x.x.x:2379 | false || e2b224392171a306 | started | etcd1 | https://x.x.x.x:2380 | https://x.x.x.x:2379 | false |+------------------+---------+-------+---------------------------+---------------------------+------------+# etcdctl member list | cut -d, -f5 | sed -e &#39;s/ //g&#39; | paste -sd &#39;,&#39;https://x.x.x.x:2379,https://x.x.x.x:2379,https://x.x.x.x:2379# ETCDCTL_ENDPOINTS=&quot;https://x.x.x.x:2379,https://x.x.x.x:2379,https://x.x.x.x:2379&quot;# etcdctl endpoint health{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-06T08:33:21.501Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-9db42de3-861f-4224-a712-ec2dbac5c6be/x.x.x.x:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-06T08:33:21.501Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-a7d064dd-31bd-4ba6-86f6-23f1e220bb17/x.x.x.x:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}{&quot;level&quot;:&quot;warn&quot;,&quot;ts&quot;:&quot;2023-01-06T08:33:21.501Z&quot;,&quot;caller&quot;:&quot;clientv3/retry_interceptor.go:62&quot;,&quot;msg&quot;:&quot;retrying of unary invoker failed&quot;,&quot;target&quot;:&quot;endpoint://client-80ca7865-535b-4b18-bdac-61641a98eccb/x.x.x.x:2379&quot;,&quot;attempt&quot;:0,&quot;error&quot;:&quot;rpc error: code = DeadlineExceeded desc = context deadline exceeded&quot;}https://x.x.x.x:2379 is unhealthy: failed to commit proposal: context deadline exceededhttps://x.x.x.x:2379 is unhealthy: failed to commit proposal: context deadline exceededhttps://x.x.x.x:2379 is unhealthy: failed to commit proposal: context deadline exceededError: unhealthy cluster Master1 에서는 etcdctl 명령어 자체가 안먹힘 “https://127.0.0.1:2379 is unhealthy: failed to commit proposal: context deadline exceeded “ 이런 에러 발생 ETCD Server Log 1번에서 2,3번으로 연결이 안되는 것으로 보임 # Master 12023-01-06 06:37:05.978890 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: read tcp x.x.x.x:56582-&amp;gt;x.x.x.x:2380: i/o timeout2023-01-06 06:37:05.978918 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: read tcp x.x.x.x:35330-&amp;gt;x.x.x.x:2380: i/o timeout2023-01-06 06:37:05.978934 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:37:05.978952 W | rafthttp: the clock difference against peer 90169b554bad10c6 is too high [1m29.239296422s &amp;gt; 1s]2023-01-06 06:37:05.978965 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:37:05.978973 W | rafthttp: the clock difference against peer 19437e9fbebfe6e7 is too high [1m29.243035275s &amp;gt; 1s]raft2023/01/06 06:32:33 INFO: 909af5b6b434f7a9 is starting a new election at term 946raft2023/01/06 06:32:33 INFO: 909af5b6b434f7a9 became candidate at term 947raft2023/01/06 06:32:33 INFO: 909af5b6b434f7a9 received MsgVoteResp from 909af5b6b434f7a9 at term 947raft2023/01/06 06:32:33 INFO: 909af5b6b434f7a9 [logterm: 10, index: 10744605] sent MsgVote request to 19437e9fbebfe6e7 at term 947raft2023/01/06 06:32:33 INFO: 909af5b6b434f7a9 [logterm: 10, index: 10744605] sent MsgVote request to 90169b554bad10c6 at term 9472023-01-06 06:32:35.971784 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:35.971822 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:35.971834 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:35.971847 W | rafthttp: the clock difference against peer 19437e9fbebfe6e7 is too high [1m29.243035275s &amp;gt; 1s]2023-01-06 06:32:35.971855 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:35.971861 W | rafthttp: the clock difference against peer 90169b554bad10c6 is too high [1m29.239296422s &amp;gt; 1s]2023-01-06 06:32:40.971920 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:40.971945 W | rafthttp: the clock difference against peer 90169b554bad10c6 is too high [1m29.239296422s &amp;gt; 1s]2023-01-06 06:32:40.971958 W | rafthttp: health check for peer 90169b554bad10c6 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:40.971964 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:40.971972 W | rafthttp: health check for peer 19437e9fbebfe6e7 could not connect: dial tcp x.x.x.x:2380: i/o timeout2023-01-06 06:32:40.971978 W | rafthttp: the clock difference against peer 19437e9fbebfe6e7 is too high [1m29.243035275s &amp;gt; 1s]위의 로그 반복# Master 22023-01-06 06:23:44.592030 I | embed: rejected connection from &quot;x.x.x.x:58832&quot; (error &quot;read tcp x.x.x.x:2380-&amp;gt;x.x.x.x:58832: i/o timeout&quot;, ServerName &quot;&quot;)2023-01-06 06:23:44.593077 I | embed: rejected connection from &quot;x.x.x.x:58846&quot; (error &quot;read tcp x.x.x.x:2380-&amp;gt;x.x.x.x:58846: i/o timeout&quot;, ServerName &quot;&quot;)2023-01-06 06:23:44.593210 I | embed: rejected connection from &quot;x.x.x.x:58858&quot; (error &quot;read tcp x.x.x.x:2380-&amp;gt;x.x.x.x:58858: i/o timeout&quot;, ServerName &quot;&quot;)2023-01-06 06:27:14.244195 W | etcdserver: cannot get the version of member 909af5b6b434f7a9 (Get https://x.x.x.x:2380/version: net/http: TLS handshake timeout)2023-01-06 06:35:59.199845 W | etcdserver: failed to reach the peerURL(https://x.x.x.x:2380) of member 909af5b6b434f7a9 (Get https://x.x.x.x:2380/version: net/http: TLS handshake timeout)2023-01-06 06:35:59.199863 W | etcdserver: cannot get the version of member 909af5b6b434f7a9 (Get https://x.x.x.x:2380/version: net/http: TLS handshake timeout) 상황 정리 Master1 에서 2,3 번으로의 통신 불가 ETCD 연결을 확인해보면 1번은 조회가 되지 않고 2,3번이 Leader 를 번갈아가면서 가져가고 있음 master1 에서 2,3 으로의 tcp 통신 확인 master1에서 Master2,3 의 etcdserver 로 telnet 도 잘 붙음 CERT 인증 이슈? 간헐적으로 발생하는 것으로 봐서는 인증 이슈는 아닌 것으로 보임 임시 해결 master1의 etcdserver container 재시작 진행master1:~$ sudo docker ps | grep etcde5db1897e6bc quay.io/coreos/etcd:v3.4.13 &quot;/usr/local/bin/etcd&quot; 3 weeks ago Up 3 weeks etcd1master1:~$ sudo docker restart e5db1897e6bce5db1897e6bc 재시작 했더니 정상적으로 ETCD server 통신이 가능한 것 확인# etcdctl endpoint status --write-out table+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+| https://x.x.x.x:2379 | 19437e9fbebfe6e7 | 3.4.13 | 24 MB | true | false | 1975 | 10809508 | 10809508 | || https://x.x.x.x:2379 | 90169b554bad10c6 | 3.4.13 | 24 MB | false | false | 1975 | 10809508 | 10809508 | || https://x.x.x.x:2379 | 909af5b6b434f7a9 | 3.4.13 | 24 MB | false | false | 1975 | 10809508 | 10809508 | |+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+명확안 원인 파악5.4.0-132-generic x86_64 커널 버전에 epoll 버그가 있는 것으로 확인 ref https://lore.kernel.org/lkml/20220823080117.738248512@linuxfoundation.org/ https://bugs.launchpad.net/ubuntu/+source/containerd/+bug/1996678 https://forum.cloudron.io/topic/8101/fix-for-kernel-bug-in-ubuntu-20-04-causing-various-issues 현재 노드의 커널 버전 확인$ kubectl get node -o wide | awk &#39;{print $1, $11}&#39;NAMEmaster1 5.4.0-132-genericmaster2 5.4.0-132-genericmaster3 5.4.0-132-generic...# Linux version 5.4.0-132-generic (buildd@lcy02-amd64-059) (gcc version 9.4.0 (Ubuntu 9.4.0-1ubuntu1~20.04.1))사내 인프라 공지 확인 5.4.0-132 focal release 중 epoll: autoremove wakers even more aggressively 항목이 비슷한 증상을 유발하는 업데이트가 의심된다는 사내 인프라 내용 확인 https://launchpad.net/ubuntu/+source/linux/5.4.0-132.148 해당 epoll 버그는 runc 에도 영향을 주며 etcd 에서 epoll을 사용하기 때문에 접속 에러를 유발하기도 함. 또한 일부 golang 프로그램 중 커넥션 에러를 유발한다고 함해결 해당 버그는 5.4.0-135부터 해결되었음. OS 재설치까지는 필요없고 최신 버전으로 커널 패치를 진행하면 됨. 따라서 해당 버전에 속하는 노드들을 Rolling Update 로 커널 패치 작업 진행해서 해결함! 이후로 해당 이슈는 발생하지 않음." }, { "title": "[K8S] Kuberntes에서 A100 GPU Node 의 MIG 설정하기", "url": "/posts/k8s-gpu-mig/", "categories": "Kubernetes", "tags": "kubernetes, k8s, mig, a100, gpu", "date": "2023-05-15 00:00:00 +0900", "snippet": "개요A100 GPU 의 경우 MIG(Multi-Instance GPU) 사용이 가능합니다. 이를 통해서 1장의 GPU 카드를 작은 용량으로 쪼개어서 사용할 수 있습니다.ref. NVIDIA MIG DOCS예를 들어 A100 40GB GPU 카드 8장이 붙은 Node 를 가정해보겠습니다.nvidia-smi 로 조회 시 아래와 같이 GPU 카드를 조회할 수 있습니다.$ nvidia-smi -i 0Mon May 15 21:35:42 2023+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.103.01 Driver Version: 470.103.01 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA A100-PCI... Off | 00000000:07:00.0 Off | 0 || N/A 32C P0 99W / 400W | 37344MiB / 81251MiB | 100% Default || | | Disabled |+-------------------------------+----------------------+----------------------+만약 MIG 인스턴스가 생성된 GPU 카드의 경우는 아래와 같이 MIG Device 들을 조회할 수 있습니다.$ nvidia-smi -i 0Mon May 15 21:35:15 2023+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.103.01 Driver Version: 470.103.01 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA A100-PCI... Off | 00000000:01:00.0 Off | On || N/A 39C P0 72W / 250W | 24MiB / 40536MiB | N/A Default || | | Enabled |+-------------------------------+----------------------+----------------------++-----------------------------------------------------------------------------+| MIG devices: |+------------------+----------------------+-----------+-----------------------+| GPU GI CI MIG | Memory-Usage | Vol| Shared || ID ID Dev | BAR1-Usage | SM Unc| CE ENC DEC OFA JPG|| | | ECC| ||==================+======================+===========+=======================|| 0 1 0 0 | 10MiB / 20096MiB | 42 0 | 3 0 2 0 0 || | 0MiB / 32767MiB | | |+------------------+----------------------+-----------+-----------------------+| 0 11 0 1 | 3MiB / 4864MiB | 14 0 | 1 0 0 0 0 || | 0MiB / 8191MiB | | |+------------------+----------------------+-----------+-----------------------+| 0 12 0 2 | 3MiB / 4864MiB | 14 0 | 1 0 0 0 0 || | 0MiB / 8191MiB | | |+------------------+----------------------+-----------+-----------------------+| 0 13 0 3 | 3MiB / 4864MiB | 14 0 | 1 0 0 0 0 || | 0MiB / 8191MiB | | |+------------------+----------------------+-----------+-----------------------+| 0 14 0 4 | 3MiB / 4864MiB | 14 0 | 1 0 0 0 0 || | 0MiB / 8191MiB | | |+------------------+----------------------+-----------+-----------------------+Kubernetes 환경에서 A100 Worker Node 의 MIG 사용을 위해서는 몇 가지 설정이 필요한데요, 해당 내용을 아래에서 한번 다뤄보겠습니다.Kubernetes 환경에서의 GPU Node 사용Kubernetes 에서는 GPU 노드의 리소스 사용을 위해서 NVIDIA/k8s-device-plugin 를 사용하고 있습니다.따라서 MIG 설정이 정상적으로 되어있다면, k8s-device-plugin Daemonset Pod가 알아서 해당 노드의 GPU 설정을 읽어와 Kubernetes 환경에서 사용할 수 있도록 적용해주고 있습니다.해당 k8s-device-plugin 컴포넌트는 추후 다른 포스팅에서 다시 한번 다루도록 하겠습니다.MIG 적용 방법MIG 사용법은 NVIDIA MIG DOCS 에서 자세하게 나와있지만, 대략적으로 설명을 해보겠습니다.1.MIG Enable ( 기본은 disable ) : GPU IDs 입력 안할 시 모든 GPU에 적용 nvidia-smi 실행 시 오른쪽 제일 아래에 보이는 것 처럼 MIG M. 은 Disabled 가 default 값입니다.$ nvidia-smi -i 0Mon May 15 21:32:23 2023+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.103.01 Driver Version: 470.103.01 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA A100-SXM... Off | 00000000:07:00.0 Off | 0 || N/A 32C P0 100W / 400W | 37344MiB / 81251MiB | 100% Default || | | Disabled |+-------------------------------+----------------------+----------------------+ 이를 Enabled 로 바꾸기 위해서는 아래와 같은 커맨드 실행이 필요합니다$ sudo nvidia-smi -i 0 -mig 1Enabled MIG Mode for GPU 00000000:36:00.0All done. nvidia-smi 실행 시 아래와 같이 Enabled * 로 * 가 붙어있으면, 해당 장비 재시작을 하여 Enabled 로 바꿔줘야 합니다.$ sudo nvidia-smiMon Dec 5 16:56:16 2022+-----------------------------------------------------------------------------+| NVIDIA-SMI 470.103.01 Driver Version: 470.103.01 CUDA Version: 11.4 ||-------------------------------+----------------------+----------------------+| GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC || Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. || | | MIG M. ||===============================+======================+======================|| 0 NVIDIA A100-PCI... Off | 00000000:01:00.0 Off | On || N/A 37C P0 67W / 250W | 0MiB / 40536MiB | N/A Default || | | Enabled*|+-------------------------------+----------------------+----------------------+| 1 NVIDIA A100-PCI... Off | 00000000:25:00.0 Off | On || N/A 38C P0 71W / 250W | 0MiB / 40536MiB | N/A Default || | | Enabled*|+-------------------------------+----------------------+----------------------+| 2 NVIDIA A100-PCI... Off | 00000000:41:00.0 Off | On || N/A 37C P0 69W / 250W | 0MiB / 40536MiB | N/A Default || | | Enabled*|+-------------------------------+----------------------+----------------------+| 3 NVIDIA A100-PCI... Off | 00000000:61:00.0 Off | On || N/A 36C P0 68W / 250W | 0MiB / 40536MiB | N/A Default || | | Enabled*|+-------------------------------+----------------------+----------------------+$ nvidia-smi -i &amp;lt;GPU IDs&amp;gt; -mig 12.VM 환경에선 Reboot 필요 : 만약 gpu를 사용하는 모니터링 시스템 agent가 동작한다면 먼저 nvsm, dcgm 서비스를 해주어야 합니다.3.제공하는 인스턴스 프로파일 리스트 확인$ sudo nvidia-smi mig -lgip4.현재 GPU 상태에서 프로파일 별 추가 가능한 GPU id 확인$ sudo nvidia-smi mig -lgipp5.인스턴스 생성 및 할당 : 아래 예시는 모든 GPU에 3g.20gb 2개 생성$ sudo nvidia-smi mig -cgi 9,3g.20gb -C6.정상적으로 생성되었는지, 사용 가능한 인스턴스 목록을 확인 해보기 ( MIG devices 확인 )$ sudo nvidia-smi$ sudo nvidia-smi -LMIG Instance 만들기ref. NVIDIA MIG DOCSMIG 인스턴스는 위와 같이 만들 수 있습니다.예를 들어 40GB GPU Memory 를 가진 GPU 카드의 경우 아래와 같습니다. 1개의 7g.40gb 혹은 2개의 3g.20gb 혹은 3개의 2g.10gb 혹은 7개의 1g.5gb 로 MIG 인스턴스 생성이 가능합니다." }, { "title": "[K8S] 같은 노드의 CoreDNS 로의 응답이 없는 이슈 (dns resolution failed)", "url": "/posts/k8s-coredns-debug/", "categories": "Kubernetes", "tags": "k8s, coredns, dns, nslookup, resolv", "date": "2023-05-11 00:00:00 +0900", "snippet": "Description coreDNS pod와 다른 pod가 같은 노드에 떠있으면, 간헐적으로 dns query 응답을 받지 못하는 이슈dns resolution failed tcpdump 확인 결과 coreDNS가 같은 노드의 다른 pod에 대해서 ARP reply를 받지 못함.CoreDNS 란? CoreDNS : reference CoreDNS 는 유연하고 확장가능한 DNS 서버이고, Kubernetes 클러스터의 DNS 서버로 사용할 수 있음. 또한 CNCF 프로젝트임. kubespray를 사용한다면, kubespray에서 coredns를 설치해줄 수 있다.Cause 구체적인 coreDNS 원인에 대해서는 조금 더 알아볼 필요가 있음 이런 이슈들을 해결하기 위해서 coredns 에서 추가된 것들이 있다?? dns lookup 과정이라던가, dns 부하문제라든가 이런것들 완화시키기 위해서 추가된 것들이 있음. 찾아보자 Solution실질적인 coreDNS의 이슈를 해결할 수는 없어서, 두 가지 방법으로 이슈를 해결하였다. (팀원분께서..) 문제가 발생하고 있는 Worker Node에 coreDNS를 띄우지 않도록 한다. dns-autoscaler 를 수정하여 coreDNS 파드의 수를 줄인다.위의 작업을 위해서 CoreDNS에 대해서 조금 더 알아 봐야하기 때문에 설정 방법을 정리해보았다.Add-On Components 란? Kubernetes Components 에서 참고할 수 있듯이, Kubernetes에는 흔히 알고있는 컴포넌트들을 포함하여 애드온 이라는 부분도 존재한다. 애드온은 쿠버네티스 리소스(데몬셋, 디플로이먼트 등)를 이용하여 클러스터 기능을 구현함. 이들은 클러스터 단위의 기능을 제공하기 때문에 애드온에 대한 네임스페이스 리소스는 kube-system 네임스페이스에 속함. 네트워킹과 네트워킹 폴리시 역할을 하는 CNI 컴포넌트, 서비스 검색을 위한 CoreDNS, 시각화 제어 등의 추가적인 컴포넌트가 여기 포함됨# 아래의 파일들이 Add-On 을 위해서 관리되는 파일들 test-master1:/etc/kubernetes$ pwd/etc/kubernetestest-master1:/etc/kubernetes$ ls | grep dnscoredns-clusterrolebinding.ymlcoredns-clusterrole.ymlcoredns-config.ymlcoredns-deployment.ymlcoredns-sa.ymlcoredns-svc.ymldns-autoscaler-clusterrolebinding.ymldns-autoscaler-clusterrole.ymldns-autoscaler-sa.ymldns-autoscaler.ymlCritical Add-On Pods들의 스케줄링을 보장하는 법 Kubernetes 의 코어 컴포넌트인 API-Server, Scheduler 와 Control manager 들은 Control plane 노드에 떠 있다. 하지만 Add-ons 들은 일반적인 클러스터 노드에 떠 있는데, 예를 들어 DNS, UI, Metrics-server 를 담당하는 Add-On 컴포넌트들이 pending 에 걸리거나, evicted가 되면 클러스터는 정상적으로 동작하지 않는다. 따라서 이런 부분들을 해결하기 위해서 해당 컴포넌트들은 Yaml 을 /etc/kubernetes 에 보관하며, priorityClassName 값으로 system-cluster-critical or system-node-critical 를 추가해서 매우 중요한 컴포넌트라고 Kubernetes 에 알린다. 이를 통해서 수동으로 삭제나 변경이 안됨CoreDNS의 설정 방법 CoreDNS도 개별적으로 떠 있는 컴포넌트가 아니라 Kubernetes 클러스터 관리를 위해 떠 있는 클러스터 애드온 컴포넌트임. 따라서 CoreDNS는 단순하게 kubectl을 사용해서 deployment 의 replicas를 변경하거나, node selector를 수정하는 작업이 안됨1) worker node에 core dns 파드가 뜨지 않도록 수정 coreDNS Deployment 는 위에서 언급한 것 처럼 priorityClassName 값으로 system-cluster-critical 가 설정되어 있기 떄문에, kubectl 로 명세 수정이 불가능함. 따라서 위험하긴 하지만 /etc/kubenetes/coredns-deployment.yaml 명세를 직접 수정해주는 방법으로 수행. coreDNS가 워커 노드에 스케줄 되지 않도록 설정. /etc/kubernetes/coredns-deployment.yml 파일 직접 수정. 워커 노드를 제외한 나머지 노드에 labels 생성. /etc/kubernetes/coredns-deployment.yml 파일 직접 수정하여 nodeSelector에 labels 추가 2) dns-autoscaler 의 값을 수정하여 coreDNS Pod 수 감소 deployment.apps/dns-autoscaler 가 coreDNS 수를 설정 하고 있음 dns-autoscaler는 coreDNS의 replica를 아래의 수식 결과로 설정 replicas = max( ceil( cores × 1/coresPerReplica ) , ceil( nodes × 1/nodesPerReplica ) ) 현재 사용하고 있는 Worker node의 core 수가 굉장히 높아서 위의 수식에 따라 계산했을 경우, 값이 높게 나오기 때문에 CoreDNS pod 도 많이 생성되고 있었음 dns-autoscaler configmap의 data 값을 수정 k edit configmap dns-autoscaler -n kube-system linear: &#39;{ &quot;coresPerReplica&quot;:1024, &quot;min&quot;:2, &quot;nodesPerReplica&quot;:16, &quot;preventSinglePointFailure&quot;:true }&#39; coresPerReplica의 기본값이 128로 설정되어 있었고, 1024로 변경 추후에 coreDNS 모니터링해서 적절한 값으로 튜닝이 필요함 Reference 팀원분께서 트러블슈팅해주신 내용을 바탕으로 정리한 글입니다." }, { "title": "[K8S] Kubespray로 만든 Kubernetes 클러스터의 Master 1번 Node 장애 복구", "url": "/posts/k8s-kubespray-master1/", "categories": "Kubernetes", "tags": "k8s, kubespray, failure", "date": "2023-05-07 00:00:00 +0900", "snippet": "Description Kubernetes 클러스터의 Master Node에서 Disk 장애가 발생함[HAWKEYE] kernel:: [22329228.540857] blk_update_request: I/O error, dev sda, sector 795030104 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 0, at 디스크 장애로 인해 sda 영역이 Read only 상태로 바뀌었고, 디스크 교체 및 OS 재설치가 필요한 상황이 발생 해당 Kubernetes 클러스터는 Kube-spray 기반으로 클러스터가 구성되어 있음 Master 노드는 3개의 노드로 HA 구성이 되어있지만, Kube spray 특성 상 Master 1번 노드에 특별한 권한이 부여되어 있어서 1번 노드의 장애 복구는 매우 까다로움 (우리와 똑같은 이슈! )Cause 하드웨어 장애 발생으로 인해 Kubernetes 의 First Control plane Node 가 죽음Solution1번 master 노드의 역할 바꾸기 Kube spary 의 Inventory 파일에서 Control plane의 순서를 바꾼다.[kube_control_plane] node-1 node-2 node-3-&amp;gt; [kube_control_plane] node-2 node-3 node-1 하드웨어 장애가 발생한 1번 장비를 클러스터로부터 Remove 작업 수행 Cluster-info configmap을 수정해준다. old control plane 장비의 IP를 살아있는 장비로 바꿔주고, certificate-authority-data 값을 바꿔준다. kubectl edit cm -n kube-public cluster-info 새로운 Control plane 노드를 추가한다.Kubelet 등 여러 컴포넌트 설치 및 구성# swapoffsudo swapoff -a# systemd-resolved 실행sudo systemctl start systemd-resolved.service# docker 설치sudo apt-get install dockermkdir -p /etc/systemd/system/docker.service.dsudo systemctl daemon-reloadsudo systemctl enable docker.servicesudo systemctl start docker.service# nf conntrack 설치sudo apt-get install -y conntrack# kubeadm 설치chmod 750 kubeadmsudo mv kubeadm /usr/local/bin/# Kubelet 설치sudo mv kubelet /usr/local/bin/ &amp;amp;&amp;amp; sudo chmod 750 /usr/local/bin/kubeletsudo vi /etc/systemd/system/kubelet.servicesudo systemctl enable kubelet.service# copy certificatesmkdir -p /etc/kubernetes/ssl/#copy /etc/kubernetes/sslmkdir -p /etc/ssl/etcd/ssl#copy /etc/ssl/etcd/ssl# CNI 설치sudo mkdir -p /opt/cni/binsudo mv cni-plugins-linux-amd64-v0.9.1.tgz /opt/cni/bintar -xvf cni-plugins-linux-amd64-v0.9.1.tgz# control plane 추가sudo kubeadm token create --print-join-commandsudo kubeadm join X.X.X.X:6443 --token X --discovery-token-ca-cert-hash sha256:X --control-plane --v=5 --skip-phases control-plane-join/etcdetcd 노드 추가하기3개의 Control plane 노드에서 1개 이상의 장비가 죽는다는 것은 Kubernetes 에서 가장 중요한 etcd 내부의 Quorum 이 깨진다는 것을 의미함. 따라서 노드가 Fail 되기 전에 etcd 를 다시 recover 해주어야 한다.Reference https://kubespray.io/#/docs/nodes" }, { "title": "[GPU] uncorrectable ECC 에러로 인한 GPU 카드 인식 불가 현상", "url": "/posts/gpu-uncorrectable-ecc/", "categories": "GPU", "tags": "gpu, uecc, nvidia, memory", "date": "2023-05-06 00:00:00 +0900", "snippet": "Description 수백장의 GPU 카드를 기반으로 한 Kubernetes on-premise 클러스터를 운영하는 도중, 갑자기 특정 장비에 꽂혀있는 8장의 GPU 중 1장의 카드가 인식이 안되는 현상이 발생 아래와 같이 해당 노드를 kubectl describe node 로 확인하였을 때는, 1장이 누락된 7장만 Allocatable 로 나타는 것을 확인할 수 있음 하지만 장비에 들어가서 nvidia-smi 로 확인하였을 때, 따로 GPU 를 사용중인 프로세스는 없었음. Kubernetes 에서 GPU 사용에 필요한 nvidia-device-pulgin Daemonset 로그를 보았을 때 아래와 같은 에러로그가 찍히고 있었음08:08:58 &#39;nvidia.com/gpu&#39; device marked unhealthy: GPU-XX-X-X-XX-XX08:08:58 XidCriticalError: Xid=94 on Device=GPU-XX-X-X-XX-XX, the device will go unhealthy.Cause 해당 GPU 카드를 사용하던 도중 uncorrectable ECC 에러가 발생해서 해당 카드의 Memory에 문제가 생겨, unhealthy 로 marking 이 되는 것이 아닐까라고 인프라쪽에서 추측하심 위의 사진에서 체크된 것처럼 Uncorr. ECC 칼럼에 0이 아니라 23이라는 값이 들어가 있음Uncorrectable ECC 란?Nvidia GPU uecc error Docs ECC : Error Correcting Code 메모리의 기능 중 하나를 나타내는 것인데, ECC는 데이터 저장 장치나 메모리 시스템에서 발생하는 에러를 검출하고 수정하는 기술 즉, Uncorrectable ECC 에러는 ECC 기능이 오류를 검출하고 수정하지 못한 경우를 의미함. 따라서 메모리에서 발생한 오류가 ECC로는 복구할 수 없는 상태라는 것을 의미. Uncorrectable ECC 에러가 발생하는 경우, 일반적으로 해당 GPU 카드를 교체하거나 기술 지원을 받아서 처리해야함Solution 기본적으로 이런 하드웨어 이슈는 인프라쪽에서 서포트를 해주시기 때문에 인프라 팀에서 도움을 주셨다. 먼저 자동 복구될 수 있는 상황인지 확인한 후에, 카드 리셋을 수행해야함. 그리고 리셋으로 복구가 되지 않는 상황이면, 재부팅을 수행해야 함 그래도 해결되지 않으면, 해당 GPU 카드를 교체하는 작업이 필요함 우리는 해당 카드를 교체하여 문제를 해결하였다." }, { "title": "[K8S] Prometheus 의 Disk Pressure 현상", "url": "/posts/k8s-prometheus-disk-pressure/", "categories": "Kubernetes", "tags": "kubernetes, k8s, nginx, ingress", "date": "2023-05-03 00:00:00 +0900", "snippet": "Description Kuberntes 환경에서 모니터링을 위해서 Prometheus를 사용 중. Prometheus는 kube-prometheus-stack 에서 확인할 수 있듯이 Statefulset을 사용하고 있음 Prometheus 는 수집하고 있는 메트릭을 보존할 필요가 있고, Stable 한 네트워크를 가지고 있어야 하며, 저장소가 Persistent 해야하기 때문에 Statefulset 을 사용하고 있음 어느날 아래와 같이 Prometheus 가 떠 있는 노드에서 Disk Pressure 현상이 발생함Lease: HolderIdentity: kiml-test-monitor1 AcquireTime: &amp;lt;unset&amp;gt; RenewTime: Thu, 13 Apr 2023 14:50:18 +0900Conditions: Type Status LastHeartbeatTime LastTransitionTime Reason Message ---- ------ ----------------- ------------------ ------ ------- NetworkUnavailable False Sun, 09 Apr 2023 11:23:34 +0900 Sun, 09 Apr 2023 11:23:34 +0900 FlannelIsUp Flannel is running on this node MemoryPressure False Thu, 13 Apr 2023 14:50:21 +0900 Thu, 06 Apr 2023 14:59:34 +0900 KubeletHasSufficientMemory kubelet has sufficient memory available DiskPressure True Thu, 13 Apr 2023 14:50:21 +0900 Thu, 13 Apr 2023 06:08:51 +0900 KubeletHasDiskPressure kubelet has disk pressure PIDPressure False Thu, 13 Apr 2023 14:50:21 +0900 Thu, 06 Apr 2023 14:59:34 +0900 KubeletHasSufficientPID kubelet has sufficient PID available Ready True Thu, 13 Apr 2023 14:50:21 +0900 Sat, 08 Apr 2023 01:09:51 +0900 KubeletReady kubelet is posting ready status. AppArmor enabled ---------- Allocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 250m (3%) 0 (0%) memory 0 (0%) 0 (0%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FreeDiskSpaceFailed 15m (x397 over 45h) kubelet (combined from similar events): failed to garbage collect required amount of images. Wanted to free 2643082444 bytes, but freed 0 bytes Warning EvictionThresholdMet 5m23s (x8450 over 2d20h) kubelet Attempting to reclaim ephemeral-storageCause Prometheus Pod의 anti-affinity 설정이 되어있지 않아 2개의 Pod 모두 monitor1 노드에 스케줄 됨. monitor1의 OOM 발생 또는 Disk Pressure 현상이 발생 Prometheus 리소스가 사용하는 local-path-retain PV 의 볼륨이 과하게 디스크를 차지 monitor2 노드의 syslog 확인 Disk usage on iage filesystem is over the high threshold, trying to fress bytes down to the low threshold 2개의 Pod 모두 문제가 생기게 되므로 모니터링 기능이 동작하지 않음.Solutionprometheus 파드 spread out Anti-affinity 추가를 통해 같은 Node에 뜨지 않도록 조치affinity: podAntiAffinity: preferredDuringSchedulingIgnoredDuringExecution: - weight: 100 podAffinityTerm: labelSelector: matchExpressions: - key: app operator: In values: - prometheus topologyKey: &quot;kubernetes.io/hostname&quot;Disk Pressure 해결을 위한 Kubelet 재시작파드를 다른 노드로 옮겨주고 해당 노드의 사용하지 않는 PVC directory 를 삭제하였지만 계속 해당 현상 발생하여 Kubelet 을 재시작# kubelet 재시작sudo systemctl restart kubelet.service Github issue : node get DiskPressure when disk space is still plenty #66961 위의 깃헙 이슈에서 볼 수 있듯이 kubelet 과 caadvisor 의 버그로 보임. kubelet 재시작으로 해결" }, { "title": "[K8S] iptables 설정이 80, 443 통신을 막는 이슈", "url": "/posts/k8s-iptable/", "categories": "Kubernetes", "tags": "k8s, iptable, traffic, preroute", "date": "2023-04-13 00:00:00 +0900", "snippet": "Description Kubernetes 클러스터의 특정 worker node 에서 80 과 443 포트로의 통신이 누락되는 경우가 발생. 해당 이슈를 해결하기 위해서 인프라분들과 팀원분들이 디버깅을 수행해주심iptables 시스템 관리자가 리눅스 커널 방화벽이 제공하는 테이블들과 그것을 저장하는 체인, 규칙들을 구성할 수 있게 해주는 사용자 공간 응용 프로그램Cause Storage 작업을 위한 컴포넌트들을 설치한 서버가 자동으로 특정 노드에 iptables 설정을 추가함. 아래와 같이 iptables 을 명령어로 확인할 수 있음test-a100:~$ sudo -iroot@test-a100:~# iptables -L -t nat -vChain PREROUTING (policy ACCEPT 233 packets, 75208 bytes) pkts bytes target prot opt in out source destination 445K 146M KUBE-SERVICES all -- any any anywhere anywhere /* kubernetes service portals */ 0 0 REDIRECT tcp -- any any anywhere anywhere tcp dpt:http redir ports 47080 234 14040 REDIRECT tcp -- any any anywhere anywhere tcp dpt:https redir ports 47443 Chain INPUT (policy ACCEPT 17 packets, 1020 bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 23 packets, 1380 bytes) pkts bytes target prot opt in out source destination81376 4933K KUBE-SERVICES all -- any any anywhere anywhere /* kubernetes service portals */ 0 0 REDIRECT tcp -- any lo anywhere anywhere tcp dpt:http redir ports 47080 193 11580 REDIRECT tcp -- any lo anywhere anywhere tcp dpt:https redir ports 47443...... 아래의 PREROUTING Chain 은 Kubernetes 의 Pod에서 Network 요청을 날리면 해당 rule 이 적용됨 따라서 Pod에서 HTTP(80), HTTPS(443) 외부 통신을 하게 되면 각각 localhost:47080, localhost:47443 으로 redirecting 이 된다. 이 때문에 pod 내의 외부통신이 불가능함Chain PREROUTING (policy ACCEPT 233 packets, 75208 bytes)pkts bytes target prot opt in out source destination445K 146M KUBE-SERVICES all -- any any anywhere anywhere /* kubernetes service portals */ 0 0 REDIRECT tcp -- any any anywhere anywhere tcp dpt:http redir ports 47080 234 14040 REDIRECT tcp -- any any anywhere anywhere tcp dpt:https redir ports 47443 Pod 의 Image pull 같은 경우는 Pod 내부 통신이 아니라 Host 통신이기 때문에 OUTPUT chain의 rule 에 적용됨. OUTPUT 같은 경우는 out 이 lo 즉, localhost 로의 요청인 경우만 redirecting 되기 때문에 영향이 없었음Chain OUTPUT (policy ACCEPT 23 packets, 1380 bytes) pkts bytes target prot opt in out source destination81376 4933K KUBE-SERVICES all -- any any anywhere anywhere /* kubernetes service portals */ 0 0 REDIRECT tcp -- any lo anywhere anywhere tcp dpt:http redir ports 47080 193 11580 REDIRECT tcp -- any lo anywhere anywhere tcp dpt:https redir ports 47443Solution Storage 쪽 관계자 분들께 요청드려서 해당 Rule 을 삭제하여 해결What did I learned? Kubernetes 내부적으로 kube-proxy가 iptable을 건드린다. 해당 부분에 대한 깊은 이해가 필요하다고 느낌. 또한 네트워크 테스트 중 패킷이 유실되는 경우가 있을 때, 어떤 부분이 문제인지 차근 차근 스텝 별로 따져가면서 확인해야함. 이를 위해서 어떤 스텝으로 네트워킹이 이루어지는지 확실하게 알고 있을 필요가 있음." }, { "title": "[K8S] V100 GPU 장비 메모리 Hang 발생 Trouble shooting", "url": "/posts/k8s-v100-memory-hang/", "categories": "Kubernetes", "tags": "kubernetes, k8s, gpu, memory", "date": "2023-04-03 00:00:00 +0900", "snippet": "Description GPU 장비로 Kubernetes 클러스터를 관리하다가 어느 순간부터 여러개의 GPU 장비에서 Memory Hang 현상이 발생Cause 해당 메모리 행이 발생하는 장비의 메모리 값을 분석 kodac_tb_memory_total : 512 GB kodac_memorysize : 527787256 KiB (527787256 kb = 503GiB) root@test-test-100v12:/# cat /proc/meminfo MemTotal: 527787256 kB MemFree: 370712636 kB MemAvailable: 380421124 kB Buffers: 136604 kB Cached: 45594528 kB SwapCached: 0 kB Active: 110750512 kB Inactive: 40200512 kB Active(anon): 104539864 kB Inactive(anon): 34361512 kB Active(file): 6210648 kB Inactive(file): 5839000 kB Unevictable: 0 kB Mlocked: 0 kB SwapTotal: 0 kB SwapFree: 0 kB Dirty: 284 kB Writeback: 0 kB AnonPages: 105219780 kB Mapped: 35780904 kB Shmem: 34363400 kB KReclaimable: 922780 kB Slab: 3007904 kB SReclaimable: 922780 kB SUnreclaim: 2085124 kB KernelStack: 30976 kB PageTables: 951676 kB NFS_Unstable: 0 kB Bounce: 0 kB WritebackTmp: 0 kB CommitLimit: 263893628 kB Committed_AS: 469875780 kB VmallocTotal: 34359738367 kB VmallocUsed: 359952 kB VmallocChunk: 0 kB Percpu: 226304 kB HardwareCorrupted: 0 kB AnonHugePages: 0 kB ShmemHugePages: 0 kB ShmemPmdMapped: 0 kB FileHugePages: 0 kB FilePmdMapped: 0 kB CmaTotal: 0 kB CmaFree: 0 kB HugePages_Total: 0 HugePages_Free: 0 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 2048 kB Hugetlb: 0 kB DirectMap4k: 11855420 kB DirectMap2M: 460660736 kB DirectMap1G: 66060288 kB 실질적으로 해당 GPU Node 가 사용되는 부분은 해당 장비를 기반으로 만들어진 Kubernetes 클러스터 위에 직접 구축한 MLOps 플랫폼 위에서 VM 형태의 Job 이 돌고 있음 해당 Job 에 할당된 리소스는 48 core, 4gpu, 500Gi mem, 32GB GPUmem 가장 큰 리소스 타입 현재 장비에 떠 있는 프로세스들의 리소스 사용량 확인Non-terminated Pods: (11 in total) Namespace Name CPU Requests CPU Limits Memory Requests Memory Limits Age --------- ---- ------------ ---------- --------------- ------------- --- fluentd-log-monitor fluentd-ds-qzvzg 500m (0%) 500m (0%) 1000Mi (0%) 1000Mi (0%) 13d gpu-monitoring gpu-monitoring-dcgm-exporter-srdnc 0 (0%) 0 (0%) 0 (0%) 0 (0%) 19d kube-system kube-flannel-phlw5 150m (0%) 300m (0%) 64M (0%) 500M (0%) 322d kube-system kube-proxy-hwxsj 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d kube-system node-shell-b28804c0-f71d-47aa-b5cb-9754e80f01e2 0 (0%) 0 (0%) 0 (0%) 0 (0%) 12m managed-dynamic-pv managed-dynamic-pv-dvq8r 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d managed-monitoring-stack managed-monitoring-stack-prometheus-node-exporter-xjz24 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d node-feature-discovery nfd-worker-qlt72 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d nvidiadeviceplugin nvidiadeviceplugin-nvidia-device-plugin-tbk8r 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d nvidiafeaturediscovery nvidiafeaturediscovery-gpu-feature-discovery-h87pm 0 (0%) 0 (0%) 0 (0%) 0 (0%) 322d ws-????-??? job-run-?????-worker-5 48 (85%) 48 (85%) 500Gi (99%) 500Gi (99%) 4h15mAllocated resources: (Total limits may be over 100 percent, i.e., overcommitted.) Resource Requests Limits -------- -------- ------ cpu 48650m (87%) 48800m (87%) memory 525374500Ki (99%) 538419488000 (99%) ephemeral-storage 0 (0%) 0 (0%) hugepages-1Gi 0 (0%) 0 (0%) hugepages-2Mi 0 (0%) 0 (0%) nvidia.com/gpu 4 4 현재 V100 장비의 전체 가용 메모리는 527787256 KB 인데, 사용중인 해당 Job의 요구 메모리는 500 GiB 이다. GB와 GiB 는 단위 차이가 있음. 500 GiB는 500 * 1024 * 1024 = 524288000 KB 즉, 노드 전체 memory 의 527787256 에서 job 의 524288000 를 제외하면 3499256 (약 3GiB) 가 남음GB vs GiB Gigabyte 와 Gibibyte 를 의미 십진법 및 이진법의 개념차이 임Job이 없는 기본 프로세스들의 사용량 확인root@test-test-100v1:/# free total used free shared buff/cache availableMem: 527787256 4844120 5621468 61880 517321668 520292100Swap: 0 0 0root@test-test-100v1:/# 따로 job 리소스가 떠있지 않는데도 아래를 보면 사용 가능한 메모리 양은 520292100 뿐이다. 해당 값은 500Gi 보다 적기 때문에 만약 500Gi 를 할당받은 Job이 메모리를 다 써버린다면 노드 OOM은 충분히 일어날 수 있음!Solution 리소스를 할당할 때 GiB가 아닌, GB로 할당하도록 수정하여, 확인하게 쉽게 설정을 바꾸었다." }, { "title": "[K8S] host 에서 Kubeadm 으로 Kubernetes 환경 제거하기", "url": "/posts/k8s-kubeadm-delete/", "categories": "Kubernetes", "tags": "k8s, kubeadm, node, delete", "date": "2023-03-12 00:00:00 +0900", "snippet": "Description Kubernetes 클러스터 환경에서 특정 노드를 Clear 하고 싶은 경우가 있다. 사실 Node OS 재설치가 제일 깔끔하고 쉽지만, 인프라 쪽의 도움을 받지 못하는 경우는 Kubernetes 의 컴포넌트들을 직접 삭제해주는 작업이 필요하다.Solutionkubeadm 으로 reset 후 ipvsadm — clear 작업test-app14:/etc/apt/trusted.gpg.d# kubeadm reset[reset] WARNING: Changes made to this host by &#39;kubeadm init&#39; or &#39;kubeadm join&#39; will be reverted.[reset] Are you sure you want to proceed? [y/N]: y[preflight] Running pre-flight checksW1215 17:32:36.034825 54565 removeetcdmember.go:79] [reset] No kubeadm config, using etcd pod spec to get data directory[reset] No etcd config found. Assuming external etcd[reset] Please, manually reset etcd to prevent further issues[reset] Stopping the kubelet service[reset] Unmounting mounted directories in &quot;/var/lib/kubelet&quot;[reset] Deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki][reset] Deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf][reset] Deleting contents of stateful directories: [/var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes] The reset process does not reset or clean up iptables rules or IPVS tables.If you wish to reset iptables, you must do so manually.For example:iptables -F &amp;amp;&amp;amp; iptables -t nat -F &amp;amp;&amp;amp; iptables -t mangle -F &amp;amp;&amp;amp; iptables -X If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)to reset your system&#39;s IPVS tables. The reset process does not clean your kubeconfig files and you must remove them manually.Please, check the contents of the $HOME/.kube/config file.test-app14:/etc/apt/trusted.gpg.d# ipvsadm --clearDocker 삭제test-app14:/etc/apt/trusted.gpg.d# sudo docker versiontest-app14:/etc/apt/trusted.gpg.d# apt remove docker-ce혹시나 network cni 이슈가 발생한다면 아래의 작업을 수행해야함 CNI 이슈 예시(combined from similar events): Failed to create pod sandbox: rpc error: code = Unknown desc = failed to set up sandbox container &quot;XX&quot; network for pod &quot;gpu-monitoring-dcgm-exporter-jv4t6&quot;: networkPlugin cni failed to set up pod &quot;gpu-monitoring-dcgm-exporter-jv4t6_gpu-monitoring&quot; network: failed to set bridge addr: &quot;cni0&quot; already has an IP address different from X.X.X.X/25 네트워크 설정 작업# sudo -i# ip link set cni0 down &amp;amp;&amp;amp; ip link set flannel.1 down# ip link delete cni0 &amp;amp;&amp;amp; ip link delete flannel.1# systemctl restart docker &amp;amp;&amp;amp; systemctl restart kubeletReference 팀원분께서 트러블슈팅해주신 내용을 바탕으로 정리한 글입니다." }, { "title": "[K8S] Node 에서 Domain Resolving(nslookup) 실패 이슈", "url": "/posts/k8s-dns-failure/", "categories": "Kubernetes", "tags": "k8s, domain, nslookup", "date": "2023-02-10 00:00:00 +0900", "snippet": "Descriptionnslookup 이란? nslookup은 리눅스, 맥OS, 윈도우 등에서 사용할 수 있는 커맨드라인 명령어 중 하나로 DNS 레코드를 조회할 때 사용함. 즉, 도메인이나 IP를 파라미터로 넣었을 때 해당 값이 설정되어있는 DNS 레코드의 조회가 가능함. nslookup 의 모든 기능은 dig 명령어로도 가능함!systemd-resolved.service 란? Linux 시스템에서 domain resolving 을 위해서 사용하는 데몬 시스템Problem systemd-resolved.service 를 켰을 때 아래와 같이 nslookup이 안되는 경우가 발생하여, 해결한 부분을 블로그 글로 작성한다.test-app2:~$ nslookup google.comServer:        127.0.0.53Address:    127.0.0.53 ** server can&#39;t find google.com: SERVFAIL test-app2:~$ cat /etc/resolv.confnameserver 127.0.0.53options         timeout:1 attempts:2 rotateCause systemd-resolved.service를 새로 킬 때 서버 이름으로 dns resolving 파일을 새로 만들어준다. 이 때 dns 서버 주소가 잘못 설정되어 있어서 설정이 잘못 들어간 것 Solution 먼저 systemd-resolved.service 를 stop 해준다. /etc/resolv.conf 파일에 사내 DNS 주소를 넣어준다. (ref. https://askubuntu.com/questions/1370794/systemd-resolved-not-resolving-any-domains) 다시 systemd-resolved.service start 한다.test-app1:~$ sudo systemctl stop systemd-resolved.servicetest-app1:~$ sudo vi /etc/resolv.conf# This file is managed by man:systemd-resolved(8). Do not edit.## This is a dynamic resolv.conf file for connecting local clients directly to# all known uplink DNS servers. This file lists all configured search domains.## Third party programs must not access this file directly, but only through the# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,# replace this symlink by a static file or a different symlink.## See man:systemd-resolved.service(8) for details about the supported modes of# operation for /etc/resolv.conf. nameserver X.X.X.X  test-app2:~$ sudo systemctl start systemd-resolved.servicetest-app1:~$ nslookup google.comServer:     X.X.X.XAddress:    X.X.X.X Non-authoritative answer:Name:   google.comAddress: 216.58.195.142Name:   google.comAddress: 2607:f8b0:4002:c09::64Name:   google.comAddress: 2607:f8b0:4002:c09::66Name:   google.comAddress: 2607:f8b0:4002:c09::71Name:   google.comAddress: 2607:f8b0:4002:c09::65" }, { "title": "[K8S] MongoDB Connection Failed 이슈", "url": "/posts/k8s-connection-failed/", "categories": "Kubernetes", "tags": "k8s, mongodb, failure", "date": "2023-02-09 00:00:00 +0900", "snippet": "Description Kubernetes Cluster의 CPU Worker Node 의 Flavor 를 리사이징(16GB Mem → 32GB Mem) 하면서 Node가 다운되었다가 다시 올라오는 작업이 진행되었음1.이때 MongoDB 가 떠 있는 Node가 다운되면서 MongoDB의 인증 문제 발생# MongoDB를 연결하고 있는 API server에서 발생한 에러 로그panic: (ShardingStateNotInitialized) Encountered non-retryable error during query :: caused by :: Cannot accept sharding commands if sharding state has not been initialized with a shardIdentity document panic: connection() error occurred during connection handshake: auth error: sasl conversation error: unable to authenticate using mechanism &quot;SCRAM-SHA-1&quot;: (AuthenticationFailed) Authentication failed2.MongoDB가 정상적으로 돌아오고 나서 Backup 해두었던 MongoDB를 Restore 하는 작업 중 내부적으로 접근 안되는 에러 발생ubuntu@test-demo-control-plane-test:~/mybackup$ sudo docker run --rm --name mongodb -v $(pwd):/app --net=&quot;host&quot; bitnami/mongodb:latest mongorestore -u root -p $MONGODB_ROOT_PASSWORD /appmongodb 02:39:18.57mongodb 02:39:18.57 Welcome to the Bitnami mongodb containermongodb 02:39:18.57 Subscribe to project updates by watching https://github.com/bitnami/containersmongodb 02:39:18.57 Submit issues and feature requests at https://github.com/bitnami/containers/issues mongodb 02:39:18.582022-10-31T02:39:18.591+0000    WARNING: On some systems, a password provided directly using --password may be visible to system status programs such as `ps` that may be invoked by other users. Consider omitting the password to provide it via stdin, or using the --config option to specify a configuration file with the password.2022-10-31T02:39:48.592+0000    error connecting to host: could not connect to server: server selection error: server selection timeout, current topology: { Type: Single, Servers: [{ Addr: localhost:27017, Type: Unknown, Last error: connection() error occurred during connection handshake: connection(localhost:27017[-13]) incomplete read of message header: read tcp [::1]:59378-&amp;gt;[::1]:27017: i/o timeout }, ] }Cause1. Mongodb Sharded Cluster 는 시작될 때 Data server가 Config server에 접속하여 Shard list 를 획득함. 이 때 key 를 이용한 SCRAM 인증 방식을 이용하는데, bitnami mongodb-sharded helm의 경우 이 key를 k8s의 secret config에 저장함. 이후, Pod가 생성될 때 각 Pod의 로컬 파일(/opt/bitnami/mongodb/conf/keyfile)에 저장함 key file 확인 k exec -it mongodb-mongodb-sharded-configsvr-2 -n mongodb -- cat /opt/bitnami/mongodb/conf/keyfile Argocd를 사용하여 각 컴포넌트들의 Sync를 맞추고 있는데, 이번에 노드 리사이징 작업을 수행하면서 Node가 내려가게 되었고 Argocd는 AutoSync 기능에 의해서 MongoDB를 업데이트하게 됨 이 때 Secret config의 key 값(secrets/mongodb-mongodb-sharded/mongodb-replica-set-key)이 변경되면서, 정상 기동 중인 Pod와 새로 기동된 Pod의 Key 값이 다르기 때문에 Authentication Failed 에러가 발생한 것으로 보임 추후 argocd가 바라보는 mongodb application의 autosync, autoheal 기능을 disable 해두어야 할 필요가 있음 2. 기존 백업 시 27017 port-forwarding 수행중인 kubectl 프로세스가 백그라운드로 떠 있었는데, MongoDB 컴포넌트가 죽고 다시 뜨면서 해당 kubectl 프로세스가 바라보는 곳이 잘못되었지 않을까 추측Solution1. MongoDB의 PVC를 포함한 모든 리소스(mongos, configsvr, data) rollout restart (백업이 꼭 선행되어야 함)2. ps -x 로 기존 27017 포트로 port-forwarding 하고 있는 kubectl 프로세스 kill 후 다시 실행" }, { "title": "[NFS] NFS Mount 시 &#39;Unit rpcbind.socket is masked&#39; 에러 발생", "url": "/posts/nfs-rpcbind-masked/", "categories": "NFS", "tags": "nfs, rpcbind, mount", "date": "2023-02-04 00:00:00 +0900", "snippet": "Description Kubernetes 의 Pod 명세에서 NFS Mount 를 사용하는데, 해당 Pod가 정상적으로 뜨지 못하고 아래와 같은 에러 발생Warning FailedMount 5m (x87 over 3h) kubelet MountVolume.SetUp failed for volume &quot;default-ws-test&quot; : mount failed: exit status 32Mounting command: mountMounting arguments: -t nfs 10.92.25.196:/test/test/test /var/lib/kubelet/pods/testvolumes/kubernetes.io~nfs/default-ws-testOutput: Failed to start rpc-statd.service: Unit rpcbind.socket is masked.mount.nfs: rpc.statd is not running but is required for remote locking.mount.nfs: Either use &#39;-o nolock&#39; to keep locks local, or start statd. 같은 Linux 장비에서 직접 NFS Mount 시에도 아래와 같은 에러 발생Failed to start rpc-statd.service: Unit rpcbind.socket is maskedCause NFS Mount 잘 되는 다른 장비에서 디버깅 수행deploy@test-test-app22:~$ sudo mount -v -t nfs X.X.X.X:/test /home/test/nfs-testmount.nfs: timeout set for Wed Mar 15 15:32:27 2023mount.nfs: trying text-based options &#39;vers=4.2,addr=X.X.X.X,clientaddr=X.X.X.X&#39;mount.nfs: mount(2): Protocol not supportedmount.nfs: trying text-based options &#39;vers=4.1,addr=X.X.X.X,clientaddr=X.X.X.X&#39;mount.nfs: mount(2): No such file or directoryFailed to start rpc-statd.service: Unit rpcbind.socket is masked.mount.nfs: rpc.statd is not running but is required for remote locking.mount.nfs: Either use &#39;-o nolock&#39; to keep locks local, or start statd. 뭔가 설정이 막혀있는 것으로 확인됨. 해당 이슈를 조금 더 서칭해보니, nfs-common 패키지를 설치하고 rpcbind 를 활성화 해야함. rpc bind 는 보안상 취약한 패키지로 꼭 필요한 경우가 아니면 설치를 지양하고 있어서, 기본 설정이 비활성화가 되어 있어서 위와 같은 에러가 발생하는 것! 따라서 rpcbind 활성화를 해주어야함.Solution rpcbind 활성화하기$ systemctl unmask rpcbind.service rpcbind.socket$ systemctl enable rpcbind.service rpcbind.socket" }, { "title": "[CISCO 네트워킹] 7. 라우터만 알면 네트워크 도사? (2)", "url": "/posts/network-study-ch07-2/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-23 00:00:00 +0900", "snippet": "7. 라우터만 알면 네트워크 도사? (2)라우터 셋업 모드 Set up 방식라우터에 명령을 입력하는 두 번째 방법 Configuration 모드 configure terminal 라우터의 구성을 콘솔이나 텔넷을 이용해서 할 때 사용하는 모드 라우터의 구성 변경을 위해서는 항상 privileged 모드에서 구성 모드로 들어가야하는데, privileged 모드에서 구성 모드로 들어가기 위한 명령은 configure 이다. 또 구성을 텔넷이나 콘솔로 하는 경우에는 teminal 이라는 옵션을 사용해서 configure terminal 을 사용한다. 모든 변경을 마치고 빠져 나올 때는 ctrl + z , RAM의 구성 파일을 NVRAM으로 저장할 때는 write memory or copy runningconfig startup-config 명령 입력Static 라우팅을 이용한 Router 구성 Static 라우팅 경로를 직접 넣어주기 때문에 라우팅이 빠르게 가능 라우팅 테이블도 적게 사용함 경로에 문제가 생기면 다른 길을 찾아내지 못하고, 수정해줄 때까지 기다림 → 갈 수 있는 경로가 하나밖에 없는 Stub 라우터 용으로 많이 사용됨 Stub 네트워크 : 오직 하나의 경로만을 통해서 외부 망과 연결된 네트워크를 의미 스태틱 라우팅만 알면 디폴트 라우트는 식은 죽 먹기 Default 라우팅 경로를 찾아내지 못한 모든 네트워크들은 모두 이곳으로 가라고 미리 정해 놓은 길 디폴트 라우트를 잡아놓으면 다른 경로에서 해당 네트워크를 못 찾을 때는 무조건 인터넷 쪽 인터페이스로 가보게 되는 것 Default 라우트 만드는 법 디폴트 네트워크를 이용 스태틱 명령을 이용 &amp;lt;img width=”837” alt=”image” src=”https://user-images.githubusercontent.com/37402136/214020188-a24bc595-d336-4git commit –amend –author=”korband korband78@gmail.com“ecf-9e9f-b1cdfaea3870.png”&amp;gt;Distance Vector 와 Link state 라우터는 스태틱 라우팅 프로토콜 / 다이나믹 라우팅 프로토콜로 나뉘었음 또 하나의 라우팅 프로토콜에 대한 분류가 distance vetor 와 link state 이다Distance vector 거리와 벡터만을 위주로 만들어진 라우팅 프로토콜 한 라우터가 모든 라우팅 정보를 가지고 있을 필요가 없기 때문에 라우팅 테이블을 줄일 수 있어서 메모리를 절약하고, 구성 자체가 간단하여 여러 곳에서 표준으로 사용되고 있음 정해진 시간마다 한 번씩 꼭 라우팅 테이블의 업데이트가 일어나기 때문에 트래픽을 낭비하고 Convergence Time이 너무 느림 → 커다란 네트워크에서는 적용하지 않고 규모가 작은 네트워크에 적용할 경우 장점을 살릴 수 있음 RIP(Routing Information Protocol), IGRP(Interior Gateway Routing Protocol)Link state 한 라우터가 목적지까지의 모든 경로 정보를 다 알고 있음 링크에 대한 정보(어디에 어떤 네트워크가 있고, 거기까지 가려면 어떤 라우터를 통해야 한다는 정보) 를 토폴러지 데이터베이스로 만들게 됨 토폴러지 데이터베이스를 가지고 SPF(Shortest Path First) 라는 알고리즘을 계산 후 SPF 트리를 만듬 한 라우터에서 목적지까지의 모든 경로를 알고 있기 때문에 중간에 링크의 변화가 생겨도 이를 알아내는 데 걸리는 시간이 짧음. 라우팅 테이블 교환이 자주 발생하지 않고 트래픽 발생을 줄여줌 라우팅 정보를 관리해야 하기 때문에 메모리 많이 소모, CPU가 일을 많이 함 → 고용량 라우터에 적용하는 것이 바람직함 OSPF(Open Shortest Path First)Telnet 을 이용한 장비 접속 Telnet 기본적으로 TCP/IP 위에 올라가는 프로그램 Ping 과 Trace 핑과 트레이스는 라우터를 구성한 후 네트워크의 연결에 이상이 없는지를 테스트하기 위해 만든 프로그램 즉, 출발지에서 목적지까지 연결에 이상이 없는지, 그리고 이상이 있다면 어디에서 이상이 발생했는지를 핑과 트레이스를 이용해서 찾아낼 수 있음 Ping 단순 핑의 경우 에코 패킷의 출발지를 정할 수 없기 때문에 PC에 가서 직접 핑을 해보지 않는 이상 확인해볼 수 있는 방법이 없음. 확장형 핑 에코 패킷의 출발지를 지정 에코 패킷의 크기 핑을 몇 번 보낼 것인가 Trace 출발지에서 목적지 뿐만 아니라 중간에 거친 경로에 대한 정보와 소요 시간까지도 확인해볼 수 있는 것이 Trace 명령" }, { "title": "[Clean code] Ch17. Smells and Heuristics", "url": "/posts/cleancode_ch17/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-16 00:00:00 +0900", "snippet": "Ch17. Smells and Heuristics코드에서 나는 나쁜 냄새? 나쁜 습관들에 대한 리스트!클린코드에서 이야기했던 고쳐야하는 부분들을 적어둬서 나중에 참고해도 괜찮을 듯 하다.인상깊었던 부분주석 꼭 필요한 부분만 주석으로 남기자. 남긴다면 단어들을 신중하게 선택해서 최대한 멋지게 남기자 (당연한 소리 반복 X, 주절대지 않고 간결하고 명료하게) 주석으로 처리된 코드는 즉각 지우자! 어짜피 git이 관리해줌함수 인수개수는 최대한 줄이자. 없다면 가장 좋다. 플래그 인수는 함수가 여러 기능을 한다는 증거임. 피해야한다 호출하지 않는 함수는 삭제하자. 테스트 커버리지 검사로 확인 가능 환경 make file일반 중복을 발견할 때마다 추상화할 기회로 간주해라 switch, if 문은 다형성으로 대체할 수 있는 기회임 다형성 객체를 생성해서 switch 문을 대신할 수 있음 서술적인 변수 사용을 통해서 프로그램 가독성을 높이자. 이름은 소프트웨어 가독성의 90%를 결정한다. 이름이 매우 중요하기 때문에 신중하게 고르고 선택한 이름이 적합한지 자주 되돌아 보자 긴 범위는 긴 이름을 사용하자 매직숫자는 명명된 숫자로 바꾸자! (근데 합리적인 숫자들은 괜찮! ) 조건문 캡슐화 및 긍정문으로 바꾸기 함수의 리턴 값을 통해서 시간적인 결합 만들기유비쿼터스 언어 DDD 에서 나오는 단어 특정 프로젝트에 적용할 표준을 고안한 것 단순히 명사 뿐만 아니라 동사까지 용어 사전으로 관리 해야함 테스트 커버리지 도구 사용 " }, { "title": "[Clean code] Ch13. Concurrency", "url": "/posts/cleancode_ch13/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-15 00:00:00 +0900", "snippet": "Ch13. Concurrency동시성과 깔끔한 코드는 양립하기 어렵다!멀티스레드 코드는 시스템이 부하를 받기 전까지는 멀쩡하게 돌아가지만, 소수의 케이스에서 이슈가 발생함 두개의 스레드 기준으로 잠재적인 경로는 최대 12870개 long일 경우 2704156 멀티스레드 프로그래밍에서 어떻게 하면 코드를 깨끗하고 효율적으로 짤 수 있는가에 대한 내용동시성 디커플링 전략 무엇과 언제를 분리 구조적 개선 각 스레드는 다른 스레드와 무관하게 돌아감 응답 시간과 작업 처리량 개선 항상 성능을 높여주는 것은 아님 설계 변할 수 있음 동시성에 대해 이해해야함 복잡함 버그 재현 어려움 설계 전략을 근본적으로 다시 바라봐야함동시성 방어원칙 SRP 반복적으로 나오는 중요한 포인트 동시성과 관련없는 코드는 분리해야함 Critical section 일반적으로 synchronized로 보호함 최대한 줄여라 스레드 테스트 코드 재현 매우 어려움 일회성 오류를 그냥 넘기면 안됨 일단 멀티 스레드랑 관련없는 외부 코드부터 잘 돌아야 함! 다양한 설정에서 돌려봐야함 보조코드 실패하는 경로가 실행될 확률이 극도로 저조함 jiggle(흔들다) 무작위로 nop, sleep 이나 yield를 수행 서비스 환경에 바로 넣을 수는 없기 때문에 같은 이름의 두개의 메서드를 생성 → 이전에 나온 내용대로 코드를 잘 분리하고 잘 짜고, 동시성에 대해 잘 이해하고 있으면 됨!!Golang 예제package mainimport ( &quot;fmt&quot; &quot;time&quot;)func sendHello(ch1 chan string) { time.Sleep(time.Second * 1) ch1 &amp;lt;- &quot;hello&quot;}func sendWorld(ch2 chan string) { time.Sleep(time.Second * 3) ch2 &amp;lt;- &quot; world&quot;}func receive(ch1 chan string, ch2 chan string, done chan string) { var msg string for { select { case msg1 := &amp;lt;-ch1: fmt.Println(&quot;msg: &quot;, msg1) msg += msg1 case msg2 := &amp;lt;-ch2: fmt.Println(&quot;msg: &quot;, msg2) msg += msg2 done &amp;lt;- msg return default: fmt.Println(&quot;default는 계속 호출됩니다.&quot;) time.Sleep(time.Second) } }}func main() { ch1 := make(chan string) ch2 := make(chan string) done := make(chan string) go sendHello(ch1) go sendWorld(ch2) go receive(ch1, ch2, done) result := &amp;lt;-done fmt.Println(&quot;result: &quot;, result)}===outputdefault는 계속 호출됩니다.default는 계속 호출됩니다.msg: hellodefault는 계속 호출됩니다.msg: worldresult: hello world" }, { "title": "[CISCO 네트워킹] 7. 라우터만 알면 네트워크 도사? (1)", "url": "/posts/network-study-ch07-1/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-06 00:00:00 +0900", "snippet": "7. 라우터만 알면 네트워크 도사? (1)라우터란? 지능을 가진 경로 배정기 라우터는 자신이 가야 할 길을 자동으로 찾아서 갈 수 있는 능력이 있음 라우터의 기능 (가장 좋은 길을 찾는 것) 을 위해서 PC 처럼 CPU, Memory, Interface 등을 가지고 있음라우터가 하는 일Path Determination 데이터 패킷이 목적지까지 갈 수 있는 길을 검사하고 어떤 길로 가는 것이 가장 적절한지를 결정함Switching Path determination 을 통해 경로가 결정되면 그 쪽으로 데이터 패킷을 스위칭해줌Routing Protocol &amp;amp; Routed ProtocolRouting Protocol 라우터가 자동차라고 치면, 그 자동차를 안전하고 빠르게 운전하는 운전기사라고 볼 수 있음 라우팅 알고리즘이라고도 함 ex. RIP, IGRP, OSPF, EIGRP 등이 있음Routed Protocol TCP/IP, IPC, AppltTalk 등 우리가 아는 모든 프로토콜은 전부 라우티드 프로토콜이다. 라우터가 라우팅을 해주는 고객을 의미함. 즉, 말 그대로 라우팅을 당하는 주체 라우터라는 자동차에 타는 승객! 즉, 라우터 입장에서는 어떤 라우팅 프로토콜을 채용하는가에 따라서 라우터의 성능이 결정됨. 물론 라우터 자체의 성능 또한 중요함Static Routing Protocol 라우터에 사람이 일일이 경로를 입력해줘야 함 Pros 시키는 대로 하기 떄문에 라우터 자체에 부담이 들지 않아 속도도 빠르고 성능이 좋아짐 다이나믹 라우팅 방식처럼 라우팅 테이블을 교환할 필요가 없음. 따라서 네트워크 대역폭을 그만큼 절약할 수 있음 보안에 강함 Cons 사람이 일일이 경로를 넣어줘야하는 것이 귀찮음 입력해준 경로에 문제가 생기면 경로를 다시 고쳐주기 전까지 문제가 있는 쪽으로 데이터를 보내려고 함 Dynamic Routing Protocol 상황에 따라서 그떄 그떄 변화가 가능한 프로토콜 Pros 여러 상황에 자동으로 대응이 가능함 Cons 라우타가 할 일(Path 계산, 정보 갱신 확인)이 많기 때문에 라우터에 부담을 줌 → 라우팅 성능을 높이기 위해 스태틱을 사용하기도 하고, 라우터가 선택할 수 있는 경로가 오직 하나뿐일 때는 다이나믹 프로토콜을 쓸 필요가 없음Routing Table 라우팅 프로토콜이 있으면서 어떤 길이 가장 좋은 길인지 메모해 두는 일종의 이정표 같은 것 라우팅 알고리즘은 자신의 라우팅 테이블을 가지고 있으면서 자기가 찾아갈 경로에 대해 정보를 이곳에 기억해둔다. 라우팅 테이블은 라우터의 RAM에 저장됨 전원이 껐다 켜지면 날라감 하지만, 라우팅 테이블은 그때 그때 변화하는 다이나믹한 정보이고, 라우팅 테이블을 다 지운 상태에서도 라우터가 이 테이블을 다시 만드는 데는 몇 초 정도 밖에 걸리지 않기 때문에 저장하지 않아도 됨 Q) 그러면 스태틱 라우팅 프로토콜은? AS, 그리고 내부용과 외부용 라우팅 프로토콜Autonomous System (AS) Autonomous System 하나의 네트워크 관리자에 의해서 관리되는 라우터들의 집단. 즉, 한 회사나 기업 또는 단체의 라우터 집단을 의미 AS 라는 그룹으로 묶어주는 이유는 라우터가 가지는 정보를 효율적으로 관리하고 인터넷 서비스를 좀 더 간편하게 제공하기 위함 AS 밖으로 나갈 때는 그 AS에 있는 문지기 라우터 (ASBR: Autonomous System Boundary Router) 에게 정보를 물어봐서 밖으로 (인터넷) 나감 즉, 이런 시스템 때문에 라우터들은 인터넷에 접속하더라도 전 세계의 모든 네트워크에 대한 정보를 다 가지고 있을 필요가 없고, 단지 자신이 속한 AS에 대한 정보만 가지면 됨 Interior Routing Protocol (IGP : Interior Gateway Protocol) AS 내부에서 사용하는 라우팅 프로토콜 RIP, IGRP, EIGRP, OSPFExterior Routing Protocol (EGP : Exterior Gateway Protocol) AS 외부에서 서로 라우팅 정보를 주고 받기 위해 사용하는 프로토콜 EGP, BGP라우터 구성 방법 맨 처음 라우터를 구성할 떄 쓰는 콘솔 케이블을 통한 구성 원격지에서 모뎀을 이용한 구성 IP주소가 세팅된 후 네트워크를 통해서 접속하는 텔넷을 이용한 구성 네트워크 관리 시스템이 있는 곳에서 사용하는 NMS 를 이용한 구성 미리 구성된 파일을 저장했다가 나중에 라우터로 다운로드하는 TFTP 서버를 이용한 구성라우터의 모드RXBOOT 라우터의 패스워드를 모르는 경우나 라우터의 이미지 파일에 문제가 생긴 경우 복구를 위해 사용함Setup 라우터를 처음 구매해서 파워를 켰거나 라우터에 구성 파일이 없는 경우 라우터가 부팅하면서 자동으로 들어가는 모드User 꺽쇠 (&amp;gt;) 로 프롬프트가 나오면 유저모드임 현재상태를 볼 수 있고, 테스트가 가능함 하지만 라우터의 구성 파일을 본다거나 구성 자체 변경은 불가Privileged enable 명령어로 들어갈 수 있음 꺽쇠 (&amp;gt;) 가 아니고 # 으로 프롬프트가 나오면 프리빌리지드 모드임 명령의 제약이 없는 모드이다.Configuration 라우터의 구성파일을 변경하는 경우에 사용하는 모드 프리빌리지드 모드에 있어야 접근 가능함 enable 후 config terminal 명령어로 접근 가능라우터의 구성요소내부 RAM 파워가 켜진 다음에 RAM 위로 운영체제가 올라옴 시스코 라우터는 IOS 라는 운영체제를 사용함 라우팅 테이블이 들어감 구성 파일이 올라감 이외에도 ARP 캐시, 패스트 스위칭에 대한 캐시 등이 들어감 NVRAM RAM 백업을 위해 사용됨 라우터의 구성파일은 매우 중요하기 때문에 백업을 진행 함 Flash Memory IOS 운영체제가 저장됨 NVRAM과 다른 점은 NVRAM에 비해서 용량이 큼 NVRAM은 구성파일 저장용으로만 사용되지만, 플래시 메모리는 주로 IOS 이미지 파일 저장용으로 사용 됨 IOS 는 전원이 켜지면서 라우터의 램으로 뛰어올라옴 ROM 파워가 켜지면 어떤 순서로 라우터 스스로의 상태를 점검하고 또 어디서 운영체제를 가져다가 메모리에 올릴 것인지 등을 적어 둠 외부" }, { "title": "[Clean code] Ch12. Emergence", "url": "/posts/cleancode_ch12/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-04 00:00:00 +0900", "snippet": "Ch12. Emergence테스트모든 테스트를 실행한다리팩터링중복을 없앤다프로그래머 의도를 표현한다클래스와 메서드 수를 최소로 줄인다위의 4가지 규칙을 적용한다면 설계는 단순해진다 테스트 테스트를 철저히 거쳐서 모든 테스트 케이스를 항상 통과하는 시스템은 테스트가 가능한 시스템. 결합도가 높으면 테스트 케이스 작성이 어려움 → 테스트 케이스를 많이 작성할 수록 자연스럽게 DIP, DI, interface, Abstraction 등 을 사용해서 결합도를 낮추게 됨 리팩터링 테스트 케이스가 잘 되어있다면 리팩터링 및 기능 추가 또한 두렵지 않음 중복 없애기 Template Method 패턴 특정 작업을 처리하는 일부분을 서브 클래스로 캡슐화해서 전체적인 구조는 바꾸지 않으면서 특정 단계에서 수행하는 내용만 바꾸는 패턴 객체지향에서는 아주 일반적임 //추상 클래스 선생님 abstract class Teacher{ public void start_class() { inside(); attendance(); teach(); outside(); } // 공통 메서드 public void inside() { System.out.println(&quot;선생님이 강의실로 들어옵니다.&quot;); } public void attendance() { System.out.println(&quot;선생님이 출석을 부릅니다.&quot;); } public void outside() { System.out.println(&quot;선생님이 강의실을 나갑니다.&quot;); } // 추상 메서드 abstract void teach(); } // 국어 선생님 class Korean_Teacher extends Teacher{ @Override public void teach() { System.out.println(&quot;선생님이 국어를 수업합니다.&quot;); } } //수학 선생님 class Math_Teacher extends Teacher{ @Override public void teach() { System.out.println(&quot;선생님이 수학을 수업합니다.&quot;); } } //영어 선생님 class English_Teacher extends Teacher{ @Override public void teach() { System.out.println(&quot;선생님이 영어를 수업합니다.&quot;); } } public class Main { public static void main(String[] args) { Korean_Teacher kr = new Korean_Teacher(); //국어 Math_Teacher mt = new Math_Teacher(); //수학 English_Teacher en = new English_Teacher(); //영어 kr.start_class(); System.out.println(&quot;----------------------------&quot;); mt.start_class(); System.out.println(&quot;----------------------------&quot;); en.start_class(); } } 표현하기 유지보수가 제일 코스트가 큼 코드를 이해하기 쉽게 만들어야함 좋은 이름 함수와 클래스 크기 줄이기 표준명칭 클래스가 표준 패턴을 사용하여 구현된다면 클래스 이름에 패턴 이름을 넣어주기 singleton, factory, facade … 등 유닛 테스트 케이스 꼼꼼히 작성 노오력 클래스와 메서드 수를 최소로 줄여라 중요하긴 하지만, 테스트 케이스 만들고 리펙터링 하는게 더 중요함 목표는 함수와 클래스 크기를 작게 유지하면서 동시에 시스템 크기도 작게 유지하는 데 있음 " }, { "title": "[Clean code] Ch11. System", "url": "/posts/cleancode_ch11/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-04 00:00:00 +0900", "snippet": "Ch11. System 자바를 사용하지 않아서 그런지 아니면 설계를 잘 몰라서 그런지 쉽게 이해하기 어려운 챕터였음 적절한 추상화와 모듈화 매우 중요 시스템 제작(construction)과 시스템 사용(use)를 분리하라 관심사 분리 Main 분리 생성과 관련된 코드는 모두 main or main이 호출하는 모듈로 옮김 나머지 시스템은 모든 객체가 생성되었고 모든 의존성이 연결되었다고 가정 팩토리 패턴 팩토리 interface를 구현해서 한 종류의 객체를 만드는 것 다음 챕터에서 나오는 템플릿 메소드 패턴과 유사? 의존성 주입 하나의 객체가 다른 객체의 의존성을 제공하는 테크닉 스프링 객체 간의 의존성(객체 간의 관례)을 객체 내부에서 직접 해주는 대신, 외부에서 객체를 생성해서 넣어주는 방식 확장 EJB AOP 어떤 로직을 기준으로 핵심적인 관점, 부가적인 관점으로 나누어서 보고 그 관점을 기준으로 각각 모듈화하겠다는 것 aspect 모듈화하고 핵심적인 비즈니스 로직에서 분리해서 재사용하게다는 것이 AOP의 취지 POJOTDD 구축의사 결정 최적화 가능한 마지막 순간까지 결정을 미루는 방법도메인 특화 언어?" }, { "title": "[Clean code] Ch10. Classes", "url": "/posts/cleancode_ch10/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-04 00:00:00 +0900", "snippet": "Ch10. Classes클래스 체계변수 목록 static public var private var private instance var public 공개 변수가 필요한 경우는 거의 없음 function클래스는 작아야함! 첫째 규칙도 작은 크기이고, 두번째 규칙도 작은 크기이다. 어느정도? → 단일 책임 원칙 클래스 설명은 if, and, or, but 없이 25자 내외로 가능해야함. SRP (Single Responsibility Principle) 클래스나 모듈을 변경할 이유가 하나이어야 함. 객체 지향에서 중요한 개념! 우리는 일반적으로 돌아가는 일에 집중을 하고, 다음 단계인 “깨끗하고 체계적인 소프트웨어” 라는 관심사로 전환하지 않음. ㅜㅜ 즉, 개발 이후 리팩토링을 거쳐야만 한다! 클래스는 큰 몇 개가 아니라 작은 클래스 여럿으로 이루어진 시스템이 더 바람직함 작은 클래슨느 각자 맡은 책임이 하나이며, 변경할 이유 또한 하나이어야 한다. 다른 작은 클래스와 협력해서 시스템에 필요한 동작을 수행함.응집도 (cohesion) 인스턴스 변수 수가 작아야함. 메서드가 인스턴스 변수를 많이 사용할 수록 응집도가 높음 → 클래스에 속한 메서드와 변수가 서로 의존하면서 논리적인 단위로 묶인다는 의미 만약 몇몇 메서드만이 사용하는 인스턴스 변수가 많이지면 클래스를 쪼개야함!변경하기 쉬운 클래스 대다수 시스템은 지속적인 변경이 불가피함. 따라서 SRP 따르는 깨끗한 시스템이 필요함 OCP 도 지원하도록 확장에 개방적이고, 수정에 폐쇄적이어야 한다는 원칙 새 기능을 수정하거나 기존 기능을 변경할 때 건드릴 코드가 최소인 시스템 구조!DIP 의존성 역전 원칙 변화하기 쉬운 것 또는 자주 변화하는 것에 의존하기 보다는, 변화하기 어렵고 거의 변화가 없는 것에 의존해라!! → 결합도를 최소로 줄이면 DIP를 따르는 클래스가 나옴 추상화에 의존해라! → 추상 클래스는 테스트가 쉬움 " }, { "title": "[Clean code] Ch09. Unit Tests", "url": "/posts/cleancode_ch09/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2023-01-04 00:00:00 +0900", "snippet": "Ch09. Unit Tests테스트 코드가 중요하다고 듣고 가끔 생각하기만 했지 이렇게 강조되는 부분일 줄은 몰랐다.TDD에 대해서 부정적으로 보는 사람들도 있어서 그렇구나 하고 생각했었는데 클린코드의 철학에서는 TDD가 필수인 것 처럼 보인다!TDD 법칙 실패하는 테스트를 작성할 때까지 코드 작성 X 컴파일은 실패하지 않으면서 실행이 실패하는 정도로만 테스트 작성 현재 실패하는 테스트를 통과할 정도만으로 실제 코드 작성→ 개발 테스트 주기가 30초 정도대애박테스트 코드는 실제 코드 못지 않게 중요함! 실제 코드가 진화하면 테스트 코드도 변화해야한다. 테스트 코드를 깨끗하게 유지하지 않으면 결국을 잃어버림 테스트 코드를 잘 짜두면 변경이 쉬워지고 변화를 통해 발생하는 버그에 대한 공포가 사라짐 가독성, 가독성, 가독성! 이 중요함Build- Operate- Check 패턴 테스트 자료 만들기 테스트 자료 조작 조작한 결과가 올바른지 확인좋은 테스트 규칙 개념 당 assert 문 수를 최소로 줄여라 무조건 1개일 때는 중복 코드가 많아지고, 부모 클래스 적용으로 해결할 수 있긴 하지만,, 굳이 1개를 고집할 필요는 없음. 하지만 최소로 줄이려고 노력은 해야함! 테스트 함수 하나는 개념 하나만 테스트하라Fast, Independent, Repeatable, Self-Validating, Timely 실제 코드 구현 직전에, bool(성공, 실패) 결과를 내며 어떠한 환경에서도 반복 가능 해야 하고 빠르고 의존성이 없이 작성해야함!TDD 매우 중요! 나중에 다시 깊게 공부해볼 필요 있음!근데 왜 애자일에서 TDD를 쓸까?→ 추상적인 레벨에서의 TDD의 핵심 개념 결정과 피드백 사이의 갭에 대한 인식, → 결정과 피드백 사이의 갭을 조절하기 위한 테크닉 즉, 애자일의 철학을 도입한 프로그래밍 기법이 TDD아닐까? 만약 어떤 부분에 대한 코딩을 여러번 해봤고, 결과를 예측할 수 있다면 TDD를 하지 않아도 됨! 처음하는 주제, 불확실성이 있거나 요구조건이 바뀔 수 있을 때! TDD를 진행함 즉, 애자일이 적용되는 플젝에는 TDD가 적합할 가능성이 매우 농후! " }, { "title": "[CISCO 네트워킹] 6. 스위치를 켜라", "url": "/posts/network-study-ch06/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-12-23 00:00:00 +0900", "snippet": "6. 스위치를 켜라!스위치 허로 만들어진 콜리전 도메인 사이를 반으로 나누고 중간에 다리를 놓아서, CSMA/CD 문제를 해결함스패닝 트리 스위치나 브리지에서 발생하는 Looping 을 막아주기 위한 프로토콜 출발지부터 목적지까지의 경로가 2개 이상 존재할 때 1개의 경로만을 남겨두고 나머지는 모두 끊어두었다가, 사용하던 경로에 문제가 발생하면 그 때 끊어두었던 경로를 하나씩 살림 Bridge ID 브리지의 ID Bridge Priority (16bit) + MAC Address (48bit) Bridge Priority 0 ~ 65535 default : 중간 값 32768 == 8000 Path Cost 브리자가 얼마나 가까이 그리고 빠른 링크로 연결되어 있는지를 알아내기 위한 값 기본적으로는 1,000Mbps 를 두 장비 사이의 링크 대역폭으로 나눈 값을 사용했음 소수 점이 나오면서 표준 값이 생김스패닝 트리의 프로토콜 네트워크 당 하나의 Root Bridge 를 가진다 네트워크는 스위치나 브리지로 구성된 하나의 네트워크를 의미함. 따라서 라우터에 의해 나누어지는 브로드캐스트 도메인이 하나의 네트워크라고 볼 수 있음 Root Bridge 는 대장 브리지로 볼 수 있음. 스패닝 트리 프로토콜을 수행할 때 기준이 되는 브리지 Root Bridge 가 아닌 나머지 모든 브리지는 무조건 하나씩의 Root Port 를 가진다 Root Port 는 루트브리지에 가장 빨리갈 수 있는 포트를 말한다. 세그먼트 당 하나씩의 Designated Port 를 가진다 쉽게 말해 지정포트 세그먼트는 브리지 또는 스위치 간에 서로 연결된 링크라고 볼 수 있음 STP 에서 Root Bridge 뽑기 무조건 낮은 BID를 갖는 브리지가 대장이 됨STP 에서 루트포트와 데지그네이티드 포트 정하기 누가 더 작은 Root BID를 가졌는가? Root Bridge 까지의 Path Cost 값은 누가 더 작은가? 누구의 BID(Sender BID)가 더 낮은가? 누구의 Port ID가 더 낮은가? BPDU Bridge Protocol Data Unit 루트 브리지의 BID인 Root BID, Root Path Cost, Sender BID 그리고 Port ID 등의 정보가 실려있음 스패닝 트리 정보를 브리지끼리 주고 받기 위해서 사용하는 특수한 프레임 브리지나 스위치가 부팅을 하면 이들은 각각의 포트로 BPDU를 매 2초마다 내보내면서 서로의 스패닝 트리 정보를 주고 받게 된다. STP의 5가지 상태 변화 Disabled 포트가 고장나서 사용할 수 없거나 네트워크 관리자가 포트를 일부러 닫아놓은 상태 데이터 전송 X MAC 배우기 X BPDU 주고받기 X Blocking 스위치를 맨 처음 켜거나, Disabled 되어 있는 포트를 살렸을 때 데이터 전송은 되지 않고 오직 BPDU 만 주고받을 수 있음 데이터 전송 X MAC 배우기 X BPDU 주고받기 O Listening 블로킹 상태에 있던 스위치 포트가 루트 포트나 데지그네이티드 포트로 선정되면 포트는 바로 리스닝 상태로 넘어감 데이터 전송 X MAC 배우기 X BPDU 주고받기 O Learning 리스닝 상태에 있던 스위치 포트가 포워딩 딜레이 디폴트 시간인 15초 동안 그 상태를 유지하면, 리스닝 상태는 러닝 상태로 넘어감 비로소 MAC 을 배워서 Table을 만듬 데이터 전송 X MAC 배우기 O BPDU 주고받기 O Forwarding 스위치 포트가 러닝 상태에서 다른 상태로 넘어가지 않고, 포워딩 딜레이 디폴트 시간인 15초 동안 유지하면 포워딩 상태로 넘어감 즉, 블로킹에서 포워딩으로 가려면 30초가 소요됨 데이터 전송 O MAC 배우기 O BPDU 주고받기 O 실습스패닝 트리에 변화가 생기면? Hello Time 루트 브리지가 얼마 만에 한번 씩 헬로 BPDU를 보내는지에 대한 시간 디폴트는 2초 Max Age 브리지들이 루트 브리지로부터 헬로 패킷을 받지 못하면 맥스 에이지 시간 동안 기다린 후 스패닝 트리 구조 변경을 시작함 즉, 특정 시간이 지나면 루트 브리지가 죽었다고 생각하고 새로운 트리를 만드는 것임 디폴트 20초 Forwarding Delay 브리지 포트가 블로킹 상태에서 포워딩 상태로 넘어갈 때까지 걸리는 시간 VLAN 스위치는 단순히 콜리전 영역을 나눠주는 역할 뿐만 아니라 라우터 없이 브로드캐스트 도메인을 나눌 수 있는 VLAN 기능을 제공함 즉, 한 대의 스위치를 마치 여러대의 분리된 스위치처럼 사용하고, 또 여러 개의 네트워크 정보를 하나의 포트를 통해 전송할 수 있음 가상 랜을 이용하면 하나의 스위치에 연결된 장비들도 브로드캐스트 도메인이 서로 다를 수 있음 한 스위치에 붙어 있는 2번 3번 VLAN 네트워크끼리 통신하려면 반드시 라우터를 거쳐서만 가능함. 아무리 같은 스위치에 붙어있다고 하더라도 네트워크가 다르기 때문에 통신이 불가능함트렁킹 여러 개의 VLAN 들을 한 번에 전송하는 방식 ISL 방식 CISCO에서 만듬 IEEE 802.1Q 방식 표준 Native VLAN이 존재 이름표가 붙지 않은 VLAN VTP VLAN Trunking Protocol 스위치들 간에 VLAN 정보를 서로 주고받아 스위치들이 가지고 있는 VLAN 정보를 항상 일치시켜 주기 위한 프로토콜 시스코만의 프로토콜VLAN 에 대한 정보 VLAN에 대해 아무것도 세팅하지 않았을 때도 Default VLAN은 이미 세팅이 되어 있음. VLAN 1 이 디폴트 VLAN으로 최초에 모든 포트가 다 VLAN 1에 속해 있음 각 스위치 마다 만들 수 있는 VLAN의 개수는 모두 다름 스위치의 IP 주소 세팅은 VLAN 1에 진행함ref. https://medium.com/humanscape-tech/스위치-스패닝트리-d6c1966aca94https://winyong.tistory.com/47http://trac.gateworks.com/wiki/linux/vlan" }, { "title": "[CISCO 네트워킹] 5. IP 주소로의 여행", "url": "/posts/network-study-ch05/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-12-17 00:00:00 +0900", "snippet": "5. IP 주소로의 여행IP 주소 원래는 이진수 32자리로 되어 있음. 각 8자리 (십진수로 하면 최대 255) 즉, 옥텟 사이에는 점을 찍음 인터넷에서 사용되는 프로토콜이 바로 TCP/IP 이고, TCP/IP 가 사용하는 주소가 바로 IP 주소이므로 제대로 이해할 필요가 있음라우터에서의 IP 주소 보통 쓰는 라우터에 배정해야 하는 IP 주소는 두 개이다. 하나는 이더넷 인터페이스 용. 다른 하나는 시리얼 인터페이스 용 이더넷용은 우리가 부여받은 번호 중에 하나를 쓰는 것 시리얼은 우리가 접속하는 ISP 업체에 따라 다르기 때문에 문의해서 써야함IP 주소의 구성 (네트워크와 호스트) 네트워크는 하나의 브로드캐스트 영역 하나의 PC가 데이터를 뿌렸을 때 그 데이터를 라우터를 거치지 않고 바로 받아볼 수 있는 영역 호스트는 각각의 PC 또는 장비라고 생각할 수 있음 모든 PC가 서로 달라야함 IP 주소의 클래스 A부터 B, C, D, E 로 구분됨. 2개는 빠짐 하나는 0 으로 가득찬 가작 작은 수 ⇒ 네트워크 자체의 주소 다른 하나는 제일 큰 수 ⇒ 해당 네트워크의 브로드 캐스팅을 위한 것 A 클래스 맨 앞이 0으로 시작 하나의 네트워크가 가질 수 있는 호스트 수가 가장 많은 클래스 네트워크 번호가 1-126으로 시작 한 네트워크 안에 들어갈 수 있는 호스트 수 : 16,777,214 B 클래스 맨 앞이 10 으로 시작 네트워크 번호가 128.1 - 191.255 로 시작 호스트 수 : 65.534 C 클래스 맨 앞이 110 으로 시작 네트워크 번호가 192.0.0 - 223.255.255 로 시작 호스트 수 : 254 D 와 E 클래스는 멀티캐스트용과 연구용으로 사용 됨서브넷 마스트 서브넷 마스크는 IP 주소에 있어서 매우 중요 예를 들어 클래스 B 주소를 받았을 경우 하나의 네트워크가 65000여 개의 호스트를 가지기 때문에 바로 구성하게되면 브로드캐스트의 영향이 너무 큼. 따라서 구분을 하기 위해서 서브넷마스크를 사용함 서브넷 마스크는 주어진 IP 주소를 네트워크 환경에 맞게 나누어 주기 위해서 씌워주는 이진수의 조합 → 브로드 캐스트 영역을 나누는 것과 IP 주소를 아끼기 위한 것 각각의 서브넷 간의 통신은 라우터를 통해서만 가능함! 서브넷 마스크를 가지고 나누어서 만들어낸 서브넷도 엄연히 하나의 네트워크니까 서로 간의 통신은 라우터를 통해서만 가능함. 서브넷 마스크를 만들 때는 이진수로 봤을 때 여러개의 1사이에 0이 오면 안됨디폴트 서브넷 마스크 서브넷 마스크는 IP 주소를 가지고 어디까지가 네트워크 부분이고, 또 어디까지가 호스트 부분인지를 나타내는 역할을 함 1인 부분은 네트워크, 0인 부분은 호스트 기존 IP와 AND 연산을 함 주어진 네트워크를 하나도 나누지 않고 그대로 다 쓰는 경우는 디폴트 서브넷 마스크를 사용함. 하지만 주어진 네트워크를 나누어서 쓰는 경우는 디폴트 서브넷 마스크를 수정해서 사용함 하나의 주소를 서브넷 마스크를 씌워서 작은 네트워크로 만드는 것을 서브네팅이라고 함서브넷 만들어보기 예시Q) 공인 IP 주소를 210.100.1.0(서브넷 마스크 255.255.255.0) 네트워크를 받았습니다. 그런데 네트워크 관리자인 여러분은 이 공인 주소를 이용해서 PC가 30대인 네트워크를 최소 4개 이상 만든 후 이들 네트워크를 라우터를 이용해서 서로 통신하게 하려고 합ㄴ디ㅏ. 이 경우 여러분이 서브넷 마스크를 만든다면 어떻게 해야할 까요?⇒ 해당 네트워크는 C 클래스 네트워크임. 이 주소로 네트워크를 최소 4개 이상 만들려면 주어진 디폴트 서브넷 마스크 변경이 필요함 호스트가 30개가 되기 위해서는 이진수가 5자리가 필요함 호스트 부분이 모두 0 이거나 모두 1인 경우는 못쓰니깐 2를 빼줘야함 2^(호스트 비트 수) - 2 즉, 210.100.1.ssshhhhh 가 되어야 함 → 255.255.255.1110 0000(224) 의 서브넷 마스크를 사용해야함 Q) 받은 공인 주소는 201.222.5.0(255.255.255.0) 이고 고객이 원하는 것은 이 주소를 잘라서 20개 이상의 작은 네트워크를 만드는데, 한 네트워크가 최소한 5개의 호스트를 가져야 한다.⇒ 호스트가 5개가 되기 위해서는 최소 3비트가 필요함 255.255.255.1111 1000 " }, { "title": "[CISCO 네트워킹] 4. 네트워크 장비에 관한 이야기", "url": "/posts/network-study-ch04/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-12-07 00:00:00 +0900", "snippet": "4. 네트워크 장비에 관한 이야기랜카드 유저의 데이터를 케이블에 실어서 허브나 스위치, 혹은 라우터 등으로 전달해주고 자신에게 온 데이터를 CPU에게 전달해주는 역할 즉, 랜에 접속하기 위한 카드처럼 생긴 것! 선택 시 생각해야하는 것 종류 (네트워킹 방식에 따라 나뉨) 이더넷용 랜카드 토큰링용 랜카드 FDDI ATM용 PC의 버스 방식 (컴퓨터에서 데이터가 날아다니는 길의 방식을 의미함) PCI ISA EISA 몇년전까지만 해도 10Mbps 용 랜카드가 일반적이었음. 즉 1초에 1MB 하지만 OSI 7 Layer를 봐서 알겠지만 여러 헤더가 붙고 오버헤드가 발생함 IRQ (Interrupt Request) 목적지가 자신의 랜카드 맥어드레스와 일치하는 데이터가 들어왔을 때 랜카드는 컴퓨터의 CPU에 인터럽트를 걸고 요청을 함 Hub Multiport Repeater 허브는 포트가 여러 개 달린 장비인데, 한 포트로 들어온 데이터를 나머지 모든 포트로 뿌려줌! 리피터는 데이터 전송 시 전달을 해주는 역할을 함 UTP 케이블 최대 전송 거리를 넘기기 위해 ~ 등 랜카드가 설치된 각각의 PC들은 케이블을 타고 바로 이 허브에 연결됨. 허브에 연결된 PC끼리는 서로 통신이 가능함. 허브끼리 연결을 하게 되면 마치 1대의 허브처럼 동작이 가능함. 종류 이더넷용 토큰링용 일반 허브 패스트 허브 허브의 동작 방법 1번 PC가 데이터 전송 → 1번 포트를 제외하고 나머지 모든 포트로 데이터 전송 → 각 PC의 랜카드가 자신에게 온 것인지 확인하고 자기 맥어드레스가 아니면 버림 이더넷 허브는 CSMA/CD 의 적용을 받음. → 콜리전 도메인에 있다는 의미. 즉, 하나의 PC가 허브에 데이터를 보내고 있을 때 또 다른 PC가 데이터를 보내려고 하면 콜리전 발생 허브를 계속 연결해 나갈 수록 콜리전 도메인의 크기는 점 점 커짐허브의 한계 Shared Hub 허브에 연결된 모든 PC들은 하나의 콜리전 도메인에 있기 때문에, 어느 한 순간에는 한 PC만 데이터를 보낼 수 있음 허브의 종류 인텔리전트 허브 Network Management System 에서 모든 데이터를 분석하고 제어도 할 수 있음 CSMA/CD 특징 떄문에 문제가 생긴 PC가 계속 이상한 데이터를 보내고 있다면 그것을 탐지해서 isolation 시킬 수 있음. → Auto Partition 기능 더미 허브 세미인텔리전트 허브 더미 허브인데 인텔리전트 허브와 연결하면 인텔리전트가 됨 브리지와 스위치 아무리 빠른 속도의 허브를 쓰더라도 어느 한 순간에는 하나의 PC만 데이터를 보낼 수 있음. 이러한 문제를 해결하기 위해 콜리전 도메인을 나누어줄 수 있는 장비가 브리지와 스위치이다. 스위치가 나오기 전까지는 브리지가 해당 역할을 했지만 이제는 브리지보다 스위치가 더 빠르고 더 트렌드임허브 vs 스위치 허브가 더 쌈 허브가 스위치보다 더 빠름 들어온 데이터에게 해주는 일이 따로 없기 때문에 → 어떠한 데이터가 돌아다니느냐 하는 것을 알고 판단해야함브리지의 역할 허브로 만들어진 콜리전 도메인 사이를 반으로 나누고 중간에 다리를 놓는 것 중간에 서서 브리지 테이블을 보면서 통신이 다리 한쪽에서만 일어나면 다리를 못건너가게 하고, 통신이 다리를 통과해야 가능하면 그때만 다리를 건너게 해준다! 브리지/스위치의 기능 Learning 들어온 데이터가 브리지 테이블(맥 어드레스 테이블)에 없으면 기록한다 Flooding 들어온 포트를 제외한 나머지 모든 포트로 뿌린다. 들어온 프레임이 찾아가는 주소가 만약 브리지가 가지고 있는 브리지 테이블에 없는 주소라면 flooding 을 함 브로트캐스트나 멀티캐스트의 경우에도 발생함 Forwarding 해당 포트로 건네준다. 목적지의 맥 어드레스를 자신의 브리지 테이블에 가지고 있고, 이 목적지가 출발지의 맥 어드레스와 다른 세그먼트에 존재하는 경우에 일어남 . Filtering 브리지를 못 넘어가게 막음 출발지와 목적지가 같으 ㄴ세그먼트에 있는 경우 이러한 기능 때문에 허브와는 다르게 콜리전 도메인을 나누어 줄 수 있는 것 Aging Time To Live 계속 저장할 수 없으니, 일정 시간이 지나면 해당 정보 삭제 다시 들어오면 refresh 진행 브리지 vs 스위치 이름이 다름 스위치가 더 비쌈 스위치가 더 잘나감Looping 프레임이 네트워크 상에서 무한정으로 뱅뱅 돌기 때문에 이더넷의 특성상 네트워크가 조용해야 데이터 전송을 할 수가 있는데, 다른 녀석들이 계속 네트워크가 조용해지기를 기다리기만 할 뿐 데이터 전송을 불가능해지는 상태를 의미함 루핑이 발생하면 통신도 안되고 문제도 찾기 어려움 스위치 끼리 다리가 두개가 놓여있으면 장비 간 통신의 길이 두가지가 되기 때문에 루핑이 발생함스패닝 트리 알고리즘 브리지나 스위치에 목적자까지의 경로가 두 개 이상 존재하면 반드시 루핑이 발생하고, 이를 막는 것이 스패닝 트리 알고리즘임 스패닝 트리는 자동으로 루핑을 검색해서 이런 루핑이 발생할 수 있는 상황을 미리 막아줌. 또한 하나를 제외하고 나머지 경로를 자동으로 막아두었다가 기존 경로에 문제가 생기면 막아놓은 경로를 풀어서 데이터를 전송함 (Fault Toleration)라우팅 vs 스위칭 비교 라우터가 스위치보다 비쌈 스위치가 더 빠름 스위치가 구성이 더 쉬움 하지만 브로트 케스트 도메인 분리를 위해서 라우터 없이는 문제를 해결할 수가 없다. 전 세계에서 하나의 브로트 케스트 도메인을 사용한다면? 하루에도 수 많은 PC들이 ARP 를 사용하는데, 이때마다 계속 브로드케스팅이 발생함 CPU 성능도 낮춤 따라서 브로드케스트 도메인을 나누는 것은 매우 중요함 스위치 만으로는 브로드 케스트 도메인을 나눌 수가 없음. 라우터가 필요! → 스위치냐 라우터냐의 답은! 적당히 사용해야함" }, { "title": "[CISCO 네트워킹] 2. 네트워크와 케이블, 그리고 친구들 ", "url": "/posts/network-study-ch01/", "categories": "Study, Book, CISCO Networking", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-12-03 00:00:00 +0900", "snippet": "2. 네트워크와 케이블, 그리고 친구들LAN vs WAN LAN 어느 한정된 공간에서 네트워크를 구성 WAN 멀리 떨어진 지역을 서로 연결하는 경우 Ethernet 이더넷은 네트워킹의 한 방식 이더넷의 가장 큰 특징은 CSMA/CD 라는 프로토콜을 사용해서 통신한다는 것 우리나라에서 사용하는 대부분의 네트워킹 방식이 이더넷임CSMA/CD Carrier Sencse Multiple Access / Collision Detection 눈치게임! 누군가가 네트워크 상에서 통신을 하고 있으면 자기가 보낼 정보가 있어도 못보내고 기다림 Carrier 네트워크 상에 나타나는 신호 Multiple Access 동시에 2개 이상의 장비에서 통신을 해버리면 Collision이 발생 그렇게 되면 PC 들은 랜덤한 시간동안 기다렸다가 데이터 다시 전송 TokenRing 오직 한 PC가 네트워크에 데이터를 실어 보낼 수 있음 Collision 발생하지 않음. 네트워크 성능 예측도 쉬움.UTP 케이블 TP Twisted-pair UTP Unshielded STP Shielded 더 비싸고 성능이 좋음 MAC Media Access Control 통신을 위해서 서로를 구분할 일종의 주소가 필요함 IP 주소를 가지고 다시 MAC 으로 바꾸는 절차가 필요함 (ARP 를 사용) 같은 네트워크에 있다면 브로드케스트를 받을 수 있음. 하지만 다른 네트워크라면 도착지의 맥 어드레스를 안 다음에 통신을 시작할 수 있음유니캐스트, 브로드캐스트, 멀티캐스트유니캐스트 우리가 가장 많이 사용하는 트래픽 항상 출발지와 목적지의 주소를 알고 있어야 함 이더넷을 사용(로컬 이더넷에 붙어있는 모든 PC들에게 정보를 뿌리는 Shared 방식이라서 네트워크 통신 시 다 뿌려줌 자신의 랜카드 맥 어드레스와 목적지 맥 어드레스가 서로 다른 경우는 바로 그 프레임을 버림 같다면 CPU로 프레임을 올려보내면서 작업을 할 수 있게 함브로드캐스트 로컬 랜에 붙어있는 모든 네트워크 장비들에게 보내는 통신 FFFF.FFFF.FFFF 로 보내주면 랜카드는 비록 자신의 맥 어드레스와 같지는 않지만 브로드캐스트 패킷을 모든 장비가 CPU에 보내줌 이렇게 되면 트래픽이 매우 증가함. 상대편 맥 어드레스를 찾기 위한 ARP 동작을 위해서 브로드캐스트를 사용멀티캐스트 200명 중에 150명만 보내고 싶을 때 사용 멀티캐스트는 보내고자 하는 그룹 멤버들에게만 한 번에 보낼 수 있기 떄문에 유니캐스트처럼 여러 번 보낼 필요도 없고, 브로드캐스트 처럼 받기 싫어하는 사람에게 까지 보낼 필요도 없음. 스위치나 라우터가 이 멀티캐스트 기능을 꼭 지원해야한다는 제약이 있음OSI 7 Layer Open System Interconnection 복잡한 네트워크 → dvide and conquer 층 별 표준화 → 호환성 확보, 비용절감 데이터의 흐름 파악 용이, 학습 도구로 활용 문제진단 해결 용이 통신 편리 L4 - L7 : User space L2 - L3 : Kernel space L1 : Physical spaceProtocol 규약 네트워크에서의 P 는 대부분이 Protocol임" }, { "title": "[K8S] Ingress Nginx 413 http error (payload too large) 이슈 해결", "url": "/posts/k8s-nginx-413-error/", "categories": "Kubernetes", "tags": "kubernetes, k8s, nginx, ingress", "date": "2022-11-02 00:00:00 +0900", "snippet": "DescriptionHTTP 413 Error 란? Payload Too Large 상태를 의미하는 것으로, 요청 Entity 가 서버에서 정의된 제한 크기보다 크다는 것을 의미Error Kubernetes 환경에서 nginx ingress controller 를 사용하여 Ingress 를 사용하는 경우 Request의 Payload 가 큰 경우 아래와 같은 413 에러가 발생DEBUG: &amp;lt;HTTPSocketPoolResponse status=413 headers={&#39;date&#39;: &#39;Wed, 09 Nov 2022 05:26:35 GMT&#39;, &#39;content-type&#39;: &#39;text/html&#39;, &#39;content-length&#39;: &#39;176&#39;, &#39;connection&#39;: &#39;keep-alive&#39;, &#39;x-content-type-options&#39;: &#39;nosniff;&#39;, &#39;x-frame-options&#39;: &#39;sameorigin&#39;, &#39;x-xss-protection&#39;: &#39;1; mode=block&#39;}&amp;gt;Cause 413 http error (payload too large) 에러는 말 그래도 Nginx 에서 해당 Payload 값이 너무 커서 못받아준 것임. 따라서 Nginx 에서 받을 수 있는 body size를 늘려주는 작업이 필요함 해당 값을 설정해주지 않을 경우 Default 값은 1MB 임. 따라서 상황에 맞게 해당 Ingress 의 Proxy-body-size 를 설정해줄 필요가 있다.Solution 아래와 같이 Ingress 에 Nginx Annotation 을 추가해주면 된다. client-body-buffer-size : 클라이언트 요청 본문 읽기를 위한 버퍼 크기 설정. proxy-body-size : 위의 값을 Global 하게 적용하기 위해서는 해당 설정 값을 nginx controller configmap에 설정해주면 됨.apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-test annotations: nginx.ingress.kubernetes.io/proxy-body-size: 20m nginx.ingress.kubernetes.io/client-body-buffer-size: 10m" }, { "title": "[Network] DNS는 어떻게 작동할까?", "url": "/posts/dns/", "categories": "Network, DNS", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-30 00:00:00 +0900", "snippet": "Domain도메인 관련 작업을 하다가 도메인쪽 내부 시스템을 이해하고 있지 않아서 뭔가 막히는 느낌이 있었다.이참에 도메인 관련 내용을 쭉 정리해보고자 한다.우리가 URL을 입력하면 어떻게 해당 IP주소를 받아올까 부터 시작해서, DNS는 어떤식으로 작동하는지에 대해서 알아보자.Domain 접속 과정 브라우저에서 URL을 입력 /etc/host.conf 파일을 조회해 우선순위가 확인 우선순위가 /etc/hosts 파일이면 www.nate.com의 IP 주소가 적혀 있는지 확인 hosts 파일에 www.nate.com의 IP 주소가 적혀 있다면 그대로 IP를 반환 /etc/hosts 파일에 www.nate.com의 IP 주소가 없다면 /etc/resov.conf 파일을 확인해서 ‘nameserver 네임서버 IP’ 부분이 있는지 확인이제 여기 나오는 것들을 하나 하나씩 알아보자!/etc에 위치한 Domain 관련 파일/etc/hosts 도메인명의 IP로 변환을 위한 정적 조회. ex. 10.0.0.1 www.foo.bar 위와 같이 등록해두면 www.foo.bar를 접속 시 10.0.01을 찾아감 /etc/resolv.conf 호스트가 DNS 를 확인하기 위한 DNS 네임서버를 나열 ex. nameserver 164.124.101.2 해당 nameserver는 LG DNS IP인데, 위와 같이 적어두면 LG DNS IP를 통해서 리졸빙을 진행함 /etc/host.conf Domain 검색 순서를 나타냄 없다면 default 값으로 “/etc/hosts → /etc/resolv.conf 의 DNS 사용” 순서로 진행Domain이란?먼저 우리가 흔히 이야기하는 Domain은 무엇일까?Domain을 이야기하려면 먼저 IP 를 알아야한다. (좀 딥해지긴 하지만… 간단하게만 정리하자!)IP란?Internet Protocol 로서 TCP/IP 계층에서 네트워크 층에 해당된다.그전에 또 알아야하는 것이 있는데,,, MAC 주소이다. 간단하게 말해서 디바이스의 물리주소!!통신을 하려면 이 물리주소로 접근해야하는데 그때 IP 주소를 통해서 접근한다.어쨌든 간단하게 이야기하면 우리가 흔히 이야기하는 IP는 IP 주소에 해당되는데, ARP 라는 프로토콜을 사용해서User → IP 주소 → ARP 프로토콜 사용 → MAC 주소이렇게 User가 MAC주소에 접근해서 통신할 수 있다!Domain이란?사람이 일반적으로 196.82.33.22 이러한 숫자로 이루어진 IP 주소를 쉽게 외울 수 없기 때문에 문자로 이름을 부여한 것이다!예를 들면 www.google.com.. 같은 것!오케이 이제 그럼 DNS를 알아보자DNS란?Domain Name System 이다.즉, 우리가 위에서 이야기한 Domain 이름을 어떤 IP 주소로 접근할지 해당 도메인 네임에 매칭된 IP 주소를 찾아주는 것이다!근데 우리가 궁금한 것은.. 그래서 DNS가 IP 주소를 어떻게 찾는데? 즉, DNS의 작동원리이다.맨 처음에 알아봤듯이, 우리가 Domain Name을 입력했을 때 /etc/ 파일에 해당 Domain이 등록되어 있지 않다면 Domain이 가리키는 IP를 찾기 위해서 DNS 서버에게 질의한다.기본적으로 우리 컴퓨터가 LAN선을 통해 인터넷이 연결되면, 해당 인터넷의 통신사(STK, KT, LG 등)의 DNS서버가 등록된다.# /etc/resolv.confnameserver 164.124.102.2nameserver 168.126.63.1우리가 Domain name으로 foo.bar.com 을 입력했을 때 우리에게 저장된 DNS 서버는 두 가지 응답을 줄 수 있다. 찾는 도메인 서버가 있을 때 해당 Domain이 가리키는 IP주소를 리턴해줌. 이렇게 Domain Name에 대한 정보와 그에 매칭되는 IP 주소를 가지고 있는 서버를 **Authoritative DNS Server** 라고 함 찾는 도메인 서버가 없을 때 Root DNS 서버에 질의를 넘겨줌 이렇게 Domain Name에 대한 정보와 그에 매칭되는 IP 주소 중 하나만 알고 있는 서버를 **Non-Authoritative DNS Server** 라고 함 중요한 점 중 하나는 DNS가 트리 구조로 되어있다는 것이다.Root DNS Server Root DNS는 최상위 DNS 서버이다. 아래에 딸린 Node DNS 서버에게로 차례대로 물어보면서 내려옴Recursive DNS Query 예를 들어서 download.beta.example.com 을 질의했다고 가정하자. Local DNS에 물어봄 Root Server로 물어봄 .com Top-Level Domain 에게 물어봄 .example.com Authoritative DNS Server에 물어봄 beta.example.com Authoritative DNS Server에 물어봄 A record와 CNAME의 차이점 DNS의 각 레코드는 A, AAAA, CNAME, NS, MX 등으로 이루어져 있다. A record type 간단하게 Domain Name에 IP 주소를 매핑하는 방법을 의미한다. CNAME Canonical Name의 줄임말로 하나의 도메인에 도메인 별칭 (Aliaas) 를 부여하는 방식이다. A record는 직접적으로 IP가 할당되어 있어서 IP가 변경되면 직접적으로 도메인에 영향을 받지만, CNAME은 그렇지 않음! nslookup 예시 $ nslookup www.naver.com Server: 10.20.30.60 Address: 10.20.30.60#53 Non-authoritative answer: www.naver.com canonical name = www.naver.com.nheos.com. Name: www.naver.com.nheos.com Address: 223.130.200.104 Name: www.naver.com.nheos.com Address: 223.130.195.95 Naver의 경우를 보자! CNAME (canonical name) 이 등록되어있다. (이게 naver.com 을 들어가도 접속이 된다는 것을 의미하는 걸까..?) ref .https://hinos.tistory.com/175https://hwan-shell.tistory.com/320 (여기서 DNS 구조에 대해서 엄청 잘 설명해주셨다)https://coding-start.tistory.com/348" }, { "title": "[Clean code] Ch07,08. Error Handling", "url": "/posts/cleancode_ch07/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-30 00:00:00 +0900", "snippet": "Ch07. Error Handling오류 코드보다 예외를 사용하라 오류 status 를 하나 하나 지정해서 하기보다는 예외를 사용하는 것이 맞음 try - catch 와 같이 전체적인 맥락에서 오류를 잡을 수 있음 try 문이 하나의 트랜잭션 역할을 한다. catch 블록은 프로그램 상태를 일관성 있게 유지함 미확인 에외를 사용하라 오류 코드를 싹 다 지정하는 것처럼 예외를 모두 확인된 예외로 사용할 수 있지만, 미확인 예외를 사용하는 것도 분명히 필요함 확인된 예외는 OCP를 위반함 Open Closed Principle 기능이 추가될 때 소프트웨어는 확장을 하고, 변경은 최소화하자 시스템이 확장하기 쉬우면서 너무 많은 영향을 받지 않도록하기 위함 함수를 호출하다보면 call stack depth 가 깊어질 수 밖에 없는데, 확인된 예외만 사용한다면 영향받는 함수가 너무 많아짐. 우리 예시 CLI → SDK → API-server API-Server Layer 별로 추상화 수준을 다르게 해서 예외 종류를 나눴음 → 아래에서 나오는 예외 클래스 정의가 하나의 해결책이 될 수 있음 (감싸기 기법 == 래핑) Null 반환 , Null 전달 노노!Ch08. Boundaries외부 코드 사용하기 오픈소스가 매우 매우 대중화 된 지금 오픈소스 사용은 거의 불가피함 거의 대부분의 기능을 가진 것들이 오픈소스화 되어 있기 때문에 요즘은 먼저 구현하기 보다 구글링을 먼저함 경계 살피고 익히기 오픈소스를 사용할 때 곧바로 우리 코드에서 외부 코드를 호출하기 보단, 외부 코드에 대한 테스트 케이스 작성 및 익히는 방법도 괜찮음 학습 테스트 더 빠르게 익힐 수 있고, 독자적인 클래스로 캡슐화할 수 있음 학습 테스트는 공짜 이상이고, 노력보다 얻는 성과가 더 큼 테스트 케이스 필요! 코드를 불러오는 경우가 아니라 컴포넌트 자체를 가져와서 우리 플랫폼에 붙이는 작업을 했었는데, 해당 외부 플랫폼에 대한 이해도를 쌓는 행위가 선행되는 것이 매우 중요했음아직 존재하지 않는 코드를 사용하기 아는 코드와 모르는 코드를 분리하는 경계 인터페이스 정의 ADAPTER 패턴 전기 콘센트 한국 표준 플러그를 일본 전원 소켓이 끼우기 위해서는 어댑터를 끼워야함. 어댑터는 소캣의 인터페이스를 플러그에서 필요로 하는 인터페이스로 바꿔줌 → 현재 사용하고 있는 라이브러리가 더 이상 요구에 부합하지 않아 재작성하거나, 다른 라이브러리를 사용해야할 때가 있음. adapter 패턴을 이용하면 기존 코드를 가능한 적게 변경하면서 교체할 수 있다. " }, { "title": "[Clean code] Ch05,06. Fomatting &amp; Objects and Data Structures", "url": "/posts/cleancode_ch05/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-24 00:00:00 +0900", "snippet": "Ch05. 형식맞추기목적코드 형식은 의사소통의 일환임.‘돌아가는 코드’를 짜는 것이 우리의 목적이 아님.다음 버전에서 기능은 바뀔 가능성이 매우 높지만, 스타일과 가독성 수준은 계속 영향을 미친다.행 길이200 줄 미만 신문 기사 처럼 작성하기 고차원 개념 → 저차원 개념 함수 호출 순서 A A’ B B’ 순서대로 읽을 수 있게 가로 형식 20-60자 IDE의 무조건적인 코드 형식 맞추는 것이 막 좋지는 않다. 가로 정렬이 가독성이 떨어질 수도 있음 들여쓰기 무시하기 if, while 문 같은 것 한 문장에 끝내고 싶긴한데, 이게 가독성에 부정적일 수 있음. 팀 규칙 개인이 선호하는 규칙은 따로 있지만, 팀에 속한다면 자신이 선호해야 할 규칙은 바로 팀 규칙이다.Ch06. 객체와 자료구조 구현을 감추기 위해서는 추상화가 필요함 추상 인터페이스를 제공해 사용자가 구현을 모른 채 자료의 핵심을 조작할 수 있어야 함 객체 동작을 공개하고 자료를 숨긴다 기존 동작을 변경하지 않으면서 새 객체 타임을 추가하기는 쉬움 하지만 기존 객체에 새 동작을 추가하기는 어렵다자료구조 별다른 동작없이 자료를 노출함 기존 자료 구조에 새 동작을 추가하기는 쉬움 하지만 기존 함수에 새 자료 구조를 추가하기는 어려움→ 새로운 자료 타입을 추가하는 유연성이 필요한 시스템일 경우 객체가 더 적합→ 새로운 동작을 추가하는 유연성이 필요하면 자료구조와 절차적인 코드가 더 적합객체와 자료구조는 근본적으로 양분된다.객체 지향 코드에서 어려운 변경은 절차적인 코드에서 쉽고, 절차적인 코드에서 어려운 변경은 객체 지향코드에서 쉽다.DTO (Data Transfer Object)자료 전달 객체자료 구조체의 전형적인 형태는 공개 변수만 있고 함수가 없는 클래스다.DB와 통신하거나 소켓에서 받은 메시지의 구문을 분석할 때 유용" }, { "title": "[Clean code] Ch04. Annotation", "url": "/posts/cleancode_ch04/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-23 00:00:00 +0900", "snippet": "Ch03 Annotation주석은 필요악이다. 순수하게 선하지 못하다. 프로그래밍 언어로 치밀하게 의도를 표현할 수 있다면 주석은 거의 필요하지 않다. 프로그래머들이 주석을 유지하고 보수하기란 현실적으로 불가능하다.근데 요새 잘 짜여진 거의 대부분의 오픈소스들은 엄청난 양의 주석을 가지고 있음.근데 이게 코드에 대한 설명이라기 보다는 전체적인 오픈소스에 대한 설명이긴 하다 .코드로 의도를 표현하라.좋은 주석 법적인 주석 정보를 제공하는 주석 의도 설명 …→ 주석이 올바른지 검증하기가 쉽지 않음. TODO 주석나쁜 주석 거의 대부분 코드를 정당화하는 주석도 아니고, 의도나 근거를 설명하는 주석도 아니며, 코드보다 읽기 쉽지않음. 의무적인 주석 javadocs python 도 있음. 있으나 마나 한 주석을 달려는 유혹에서 벗어나 코드를 정리해라 주석으로 처리한 코드는 매우 밉살스러운 관행 소스코드 관리 시스템을 사용해야함 주석을 달아야 한다면 근처에 있는 코드만 기술해라" }, { "title": "[Clean code] Ch03. Functions", "url": "/posts/cleancode_ch03/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-16 00:00:00 +0900", "snippet": "Ch03 FunctionsFitnesse 란 Fitnesse는 입력 값 과 결과 값을 WIKI페이지에 입력하고 검증을 수행하므로 항상 결과에 대한 기대 값은 동일해야 한다는 제약사항이 존재한다.Small! 작아야함 더 작아야함스크린에 가득차면 안됨.20줄정도Blocks and Indentingif, while 등의 블록의 인덴트는 하나여야함. 즉, 중첩이면 안된다.Do one ThingFunctions should do one thing. They should do it well, They should do it onlyOne thing 이라는 것을 판단하는 기준 - 추상화 레벨. 다른 이름으로 함수를 추출해낼 수 있다면 한가지 일이 아님Keep It Simple Stupid추상화 수준이 섞이면 특정 표현이 근본 개념인지, 세부사항인지 구분하기 어려움!내려가기 규칙A A’ B B’—A B A’ B’ 위의 내용이 더 맞다고 주장함. 스타일 차이?Switch 문인스턴스를 생성만하는 factory 패턴으로는 가능!그렇지 않으면 case가 늘어날때마다 service logic의 switch문이 무한정 길어짐서술적인 이름 사용함수 인수무항이 이상적임3,4개는 피하는게 좋음플래그 인수는 추함. → 플래그를 통해서 여러가지 작업을 하겠다는 것을 의미한다는 것이 모순인수가 2,3개 필요하다면 클래스의 변수로 선언하는 것이 더 나음.출력 인수appendFooter(s);s가 출력으로 사용되면 안됨! 좀 어색함오류 코드보다 예외를 사용하라!차라리 예외를 써라" }, { "title": "[Clean code] Ch02. Meaningful Names", "url": "/posts/cleancode_ch02/", "categories": "Study, Book, Clean Code", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-10-15 00:00:00 +0900", "snippet": "Ch02. Meaningful NamesChoosing good names takes time but saves more than it takes. 의도를 확실하게 나타내도록 네이밍해야함 네임이 코멘트를 필요로한다면 제대로 네이밍 안된것임 하지만 코멘트에 대해서는 견해차이가 있다. List가 아닐 경우가 있을 수 있기 때문에 변수 명에 List를 붙일 때는 확실할때만 붙여야 함 info, data 둘 다 a, an 처럼 의미 없는 구분점임 근데 local 과 global 을 구분하는데 the, a 를 쓰는건 괜찮음 Noise words are redundant 변수 명에 var 이런것은 noise하며 redundant하다 Customer, CustomerObject 의 차이가 애매하다? Object는 안써도 된다? 발음할 수 있는 이름 This matters because programming is a social activity. 검색할 수 있는 이름 prefix 로 customer 이런것 붙이면 안됨! 자바 같은 경우는 변수 명에 타입을 입력할 필요가 없음 Avoid Mental Mapping (기억력을 믿지마라) 명료함이 최고! 한개념에 한 단어 get, retrieve 애매하게 섞어 쓰면 안됨! 변수 명에 기술 개념 사용 타겟 유저가 확실하게 기술직군 개발자이기 때문에 기술 개념(FIFO .. ) 을 쓰는 것이 괜찮다. " }, { "title": "[Golang] map, slice, list 정리 ", "url": "/posts/golang-basic/", "categories": "Golang", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-07-23 00:00:00 +0900", "snippet": "[Golang] map, slice, list 정리Array 고정길이 배열 배열 크기를 동적으로 증가시키거나 부분 배열을 잘라내는 기능 없음. 배열의 크기를 데이터 타입 앞에 써줘야 함package mainfunc main() { var a [3] int a[0] = 1 a[1] = 2 a[2] = 3 println(a[1]) 초기화 값을 지정해주지 않으면 0으로 초기화가 됨. var a = [3]int{1,2,3}var b = [...]int{1,2,3}Slice 가변길이 배열. Python 의 List와 유사! Array와는 다르게 고정된 크기를 미리 지정하지 않을 수 있고, 동적으로 변경 가능. 부분 잘라내기 가능 선언 var v []T make(slice_type, slice_length, capacity) 함수 capacity : 내부 배열의 최대 길이 생략 시 capacity는 length와 동일 모든 요소가 zero value인 슬라이스를 생성함. package mainfunc main() { var a []int a = []int{1,2,3} a[1] = 10 println(a)}---func main() { s := make([]int, 5, 10) println(len(s), cap(s))}---ss := s[0:11]// 가변길이보다 더 큰 값을 주면 에러 발생 -&amp;gt; panic: runtime error: slice bounds out of range---s := make([]int, 5, 10)ss := s[0:10]fmt.Println(s)fmt.Println(ss)ss = append(ss, 1)sss := ss[0:11]fmt.Println(sss) Sub-slice func main(){ s := []int{0,1,2,3,4,5} s = s[2:5] } Append &amp;amp; Copy func main() { s := []int{0,1} s = append(s,2) s = append(s,3,4,5) } Map Key-value 쌍으로 된 데이터 타입. Python의 Dictionary와 유사var sMap map[int]string---sMap = make(map[int]string)---tickers := map[string]string{ &quot;GOOG&quot;: &quot;Google Inc&quot;, &quot;MSFT&quot;: &quot;Microsoft&quot;, &quot;FB&quot;: &quot;FaceBook&quot;,}" }, { "title": "[번역] Pid 네임스페이스에 대한 궁금증 ", "url": "/posts/eng04_pid-namespace/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-07-16 00:00:00 +0900", "snippet": "The Curious Case of Pid Namespaces[원본링크] https://hackernoon.com/the-curious-case-of-pid-namespaces-1ce86b6bc900Pid 네임스페이스에 대한 궁금증컨테이너들은 어떻게 pid들을 공유할까?네임스페이스들은 리눅스 컨테이너들의 가장 중요한 컴포넌트 중 하나이다.네임스페이스들은 공유 자원에 대해서 격리를 지원하는데, 각각의 어플리케이션이 자기 자신의 유니크한 시스템 뷰를 가질 수 있게 한다. 네임스페이스 덕분에 각각의 도커 컨테이너들은 자신의 파일시스템과 네트워크를 가질 수 있는 것이다. 리눅스는 수많은 배포를 통해서 네임스페이스를 더해왔다. 이러한 점진적인 변화 떄문에 네임스페이스의 각 타입들은 자신의 유니크한 도전들을 제공한다. 그 중에서도 Pid 네임스페이스는 특히 멀티 프로세스가 포함될 때 특별한 처리를 요구한다.리눅스에서의 Pid리눅스에서의 프로세스는 트리구조를 가지게 된다. 커널의 각 프로세스들은 유니크한 프로세스 식별자를 가지고 있는데, pid라고도 불린다. 각 프로세스의 발자취에 대한 기록은 직계 부모 프로세스로부터 물려받는다. Pid는 fork 시스템 콜로 생성되었을 때도 부모로부터 전달받는다. 커널은 새로운 자식 pid 를 생성하고 식별자를 호출한 프로세스로 반환해준다. 하지만 자식 pid를 수동으로 추적하는 것은 부모 프로세스에게 달려있다.커널에 의해 시작되는 첫번째 프로세스는 pid를 1로 가진다. 이 프로세스는 init process라고도 불리며 쉽게 init이라 불린다. init 의 부모 프로세스의 pid는 0이며 그것이 커널을 의미한다는 것을 알 수 있다. Pid 1은 유저스페이스 프로세스 트리의 루트이다. 리눅스 시스템에서는 어떤 프로세스든 반복적으로 각자의 부모 프로세스를 따라가다 보면 pid 1 을 찾을 수 있다. 만약 pid 1 이 죽게되면, 커널은 panic에 빠지고 서버를 재부팅해야만 한다.네임스페이스 훑어보기리눅스 네임스페이스들은 unshare 시스템 콜에 어떤 네임스페이스를 생성할 것인지에 대한 플래그를 전달해서 만들어진다. 대부분의 케이스에서 unshare 는 새로운 네임스페이스로 당신을 던져버린다. 예를 들어, 프로세스가 네트워크 네임스페이스를 생성하자마자 어떠한 디바이스도 연결되지 않은 빈 네트워크를 즉각적으로 확인할 수 있다.Pid 네임스페이스는 약간 다른데, 당신이 pid 네임스페이스를 unshare 했을때 프로세스는 즉각적으로 새로운 네임스페이스에 들어가지 않는다. 대신, fork를 해줘야한다. 자식 프로세스는 pid 네임스페이스에 들어가고 pid 1 이 된다. 이것을 통해서 특별한 성격이 가득 채워진다.또한 pid 네임스페이스는 프로세스 계층에 대해서 분리되어진다는 것을 주목해야한다. 다시말해서 fork 된 프로세스는 실제로는 두개의 Pid를 가지고 있다는 것이다. 새로운 네임스페이스에서의 pid 1, 네임스페이스 바깥에서 봤을 때의 pid.네임스페이스에서의 Pid 1네임스페이스 안에서 init (Pid 1)은 다른 프로세스와 비교했을 때 3개의 특별한 특성을 가진다. 자동으로 디폴트 시그널 핸들러를 가지지 않기 때문에 해당 시그널에 대해서 시그널 핸들러를 등록해두지 않았다면 시그널을 받았을 때 무시한다. (왜 도커라이즈 된 수 많은 프로세스들이 ctrl+c 를 했을 때 강제 종료되지 않고 docker kill 을 해야만 죽는 이유이다!) 만약 네임스페이스에서 다른 프로세스가 자식보다 먼저 죽었다면, 그 프로세스의 자식은 pid 1을 부모로 다시 가지게 된다. init 이 exit 상태인 프로세스들을 거두면서 커널이 프로세스 테이블에서 지울 수 있게 된다. 만약 프로세스가 죽는다면, pid 네임스페이스에서의 모든 프로세스들은 강제로 종료가 되며 네임스페이스는 정리가 될 것이다.init 프로세스는 컨테이너의 라이프타임과 매우 밀접하게 연관되어 있음을 알 수 있다.Docker의 “실수”도커(그리고 runc)는 새로운 pid 네임스페이스에서 지정된 프로세스를 컨테이너 엔트리 포인트로 pid 1 으로 실행시킨다. pid 1 로 실행하도록 디자인 된 부분은 어플리케이션 프로세스에 예기치 못한 행동을 불러일으킨다. 위에서 설명한 것처럼 pid 1로 프로세스가 실행되었을 때 만약 자신의 시그널 핸들러를 등록하지 않으면, 시그널이 먹히지 않을 것이다. 만약 포트한 자식 프로세스가 자식의 자식 프로세스가 죽기전에 먼저 죽어버리면, 좀비 프로세스가 컨테이너에 생성될 수 있고 잠재적으로 프로세스 테이블을 계속 채워갈 것이다.도커는 이 내용에 대해서 거의 손을 떼어 왔다. 대신에 컨테이너에서 특별한 init 프로세스를 실행할 수 있는데 어플리케이션 프로세스가 fork-exec 이 되는 것이다.(pid 1이 되는 것이 아니라, 1의 자식 프로세스로 해당 어플리케이션 프로세스가 생성된다) 많은 컨테이너들이 위의 문제를 피하기 위해서 이렇게 수행해왔다. 이 방법에는 한가지 불운한 영향이 있는데 컨테이너가 좀 더 복잡해진다는 것이다. 컨테이너가 진짜 init 시스템을 가지고 있을 때는 사람들이 의존성 격리에 대한 이점을 희생시키는 멀티 프로세스를 컨테이너에 내장하는 경향이 있었다. 도커의 pod 에 대한 네이티브 지원 부족은 이 문제를 계속해서 악화시킬 뿐이다.The Rkt “Solution”Rkt 는 이 문제에 대해서 좀 더 깔끔한 해결책을 제시한다. 당신이 시작한 프로세스가 init 프로세스가 아니라고 가정하고 사용자(systemd)를 위한 init 프로세스를 생성하는데 이 때 systemd는 컨테이너 프로세스에 대한 파일 시스템 네임스페이스를 만들고 실행한다. Systemd는 네임스페이스에서 pid 1이 되고 컨테이너 프로세스는 pid 2로 실행된다. 즉, 컨테이너가 init 프로세스를 제공할 경우 pid 2로 실행될 것이지만, 실제로는 거의 문제가 발생하지 않는다는 것이다.더 간단한 대안책단일 프로세스에 대해서" }, { "title": "[Linux System Programming] Ch10 시그널 ", "url": "/posts/linux_ch10/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-07-10 00:00:00 +0900", "snippet": "[Ch10 시그널]Ch10 시그널리눅스 환경에서 Robustness Test (강건성 테스트) 나 디버깅을 진행하다보면 여러가지 오류로 인해서 프로그램이 종료되는데, 이 때 Core dump가 있으면 디버깅에 유용하지만 로그만 남아 있는 경우도 있음.이 때 단서가 되는 부분이 시그널임 시그널은 비동기 이벤트 처리를 위한 메커니즘을 제공하는 소프트웨어 인터럽트이다.Hardware interrupt : 외부에서 전기적 신호(이벤트)가 발생했을 때Software interrupt : CPU가 연산중에 어떠한 조건에 맞는 이벤트가 발생했을 때 유저가 Ctrl+C 를 눌러 시스템 외부에서 발생시키거나, 프로세스가 0으로 나누는 연산을 수행한 경우처럼 프로그램이나 커널 내부 작업 중에 발생할 수도 있다. 또는 IPC(Inter-Process Communication) 기법으로 프로세스간 시그널 송수신도 가능하다. 중요한 점은 이벤트가 비동기적으로 발생할 뿐만 아니라 해당 프로그램도 시그널을 비동기적으로 처리할 수 있다는 점. 시그널 처리 함수는 커널에 등록되어 시그널이 전달되었을 때 그 함수가 비동기식으로 호출된다. → 프로세스 입장에서 일을 하고 있는 도중에 시그널이 오면 잠시 일을 멈추고 시그널에 대한 처리를 한 뒤 다시 본래의 일로 돌아온다는 의미 시그널 전달 가능 흐름 Kernel → Process Process → Process Thread → Thread 10.1 시그널 개념시그널의 생명 주기 시그널 발생 → 커널은 해당 시그널을 전달 가능할 때까지 쌓아둠 → 커널은 가능한 시점에 적절하게 시그널을 처리 커널은 프로세스 요청에 따라 세 가지 동작 중 하나를 수행함.시그널을 무시한다. 무시할 수 없는 시그널은 SIGKILL 과 SIGSTOP 두 가지 시스템 관리자가 프로세스를 종료하거나 멈출 수 있어야 하기 떄문 시그널을 붙잡아 처리한다. 커널은 프로세스의 현재 코드 실행을 중단하고 이전에 등록했던 함수로 건너뛰어서 해당 함수를 실행함. SIGINT 와 SIGTERM 은 가장 흔하게 잡을 수 있는 시그널. ex. 터미널 프로그램은 시그널을 잡아서 프롬프트로 다시 돌아간다. ex. 프로그램이 종료되기 전에 SIGTERM을 붙잡아서 네트워크 연결을 끊거나, 임시파일 삭제 등 종료와 관련된 작업을 수행할 수 있음 SIGKILL 과 SIGSTOP은 잡을 수 없다.기본 동작을 수행한다. 기본 동작은 시그널에 따라 다름. 대부분은 프로세스 종료 시그널을 전달 받게 되면 진행중인 테스크를 잠시 중단하고, Signal Handler를 수행한 후 다시 프로세스로 돌아옴 내부적으로는 조금 더 복잡하게 동작함 Signal을 처리하는 것은 Kernel 이지만, handler를 등록했다면 signal handler를 수행하기 위해서 다시 user 영역으로 돌아옴. handler를 호출하고 다시 Kernel 영역으로 돌아가서 본래 Task의 context를 이용해서 signal이 불린 시점으로 돌아감.10.1.1 시그널 식별자시그널은 모두 &amp;lt;signal.h&amp;gt; 파일에 정의되어 있음 시그널은 단순 양의 정수를 나타내는 선행처리기의 정의이다.시그널 번호는 1(보통 SIGHUP)에서 시작해서 선형적으로 증가하고, 전체 시그널이 대략 31개지만 대다수 프로그램은 몇 개만 처리함.10.1.2 리눅스에서 지원하는 시그널Table 10-1. SignalsSIGABRT abort() 함수를 호출한 프로세스에 이 시그널을 보낸다. 프로세스는 종료되고 코어 파일을 생성함. 리눅스에서는 assert() 호출이 실패할 경우 abort()를 호출함 abort() : 현재 상태를 core dump 하고 프로세스를 비정상적으로 종료하는 함수 exit() : 정상적으로 종료하는 함수 **core dump** : UNIX 계열에서 프로그램이 비정상적으로 종료되는 경우에 프로그램이 종료될 당시의 메모리 상태를 기록하여 생성된 파일. 디버깅 용도로 사용됨 SIGBUS 프로세스가 메모리 보호 이외에 다른 하드웨어 장애를 유발한 경우 커널에서 이 시그널을 보냄. 프로세스가 mmap() 으로 만든 메모리 영역에 부적절하게 접근할 때 커널에서 이 시그널을 보냄 SIGHUP 제어터미널 상에서 부모 프로세스가 죽거나 멈춘 게(hangup) 감지되면 SIGHUP 시그널을 보냄. 세션의 터미널 접속이 끊어질 때마다 커널에서 해당 세션 리더에게 이 시그널을 보냄. 또한, 커널은 세션 리더가 종료될 때 foreground process 그룹에 속한 모든 프로세스에 이 시그널을 보냄. 기본동작 이 시그널은 사용자의 로그아웃을 의미 → 프로세스 종료 데몬프로세스일 경우 자신의 설정을 다시 읽도록 하는 의미 ex. 아파치에 SIGHUP 을 보내면 httpd.conf를 다시 읽음. 데몬 프로세스는 제어 터미널이 없어서 정상적인 상황에서는 이 시그널을 절대 받을 수 없음. SIGINT 사용자가 인터럽트 문자(보통 Ctrl + C)를 입력했을 때 커널은 포어그라운드 프로세스 그룹에 속한 모든 프로세스에 이 시그널을 보냄. 기본동작 프로세스 종료. 하지만 프로세스에서 이 시그널을 붙잡아 처리할 수 있고, 일반적으로 종료 직전에 마무리 목적으로 사용 SIGKILL kill() 시스템 콜에서 보냄 시스템 관리자가 프로세스를 무조건 종료하도록 만드는 방법을 제공 잡거나 무시할 수 없으며 결과는 항상 해당 프로세스의 종료SIGSEGV 세그멘테이션 위반(Segmentation Violation) 에서 유래된 이름 유효하지 않은 메모리 접근을 시도하는 프로세스에 보냄 맵핑되지 않은 메모리에 접근하거나, 읽기를 허용하지 않는 메모리를 읽거나, 메모리에서 실행 가능하지 않은 코드를 실행하거나, 쓰기를 허용하지 않는 메모리에 쓰는 경우 기본동작 프로세스의 종료와 코어 덤프 생성 SIGSTOP kill() 에서만 보낸다. 무조건 프로세스를 정지시키며 잡을 수도 무시할 수도 없음.SIGWINCH 터미널 윈도우 크기가 변한 경우, 커널에서 포어그라운드 프로세스 그룹에 속한 모든 프로세스에 이 시그널을 보냄. 기본적으로는 무시하지만, 붙잡아 처리할 수 있음.10.2 시그널 관리 기초시그널 관리를 위한 가장 단순하면서도 오래된 인터페이스는 signal() 함수이다.#include &amp;lt;signal.h&amp;gt;typedef void (*sighandler_t)(int);sighandler_t signal (int signo, sighandler_t handler); signal() 호출이 성공되면 signo 시그널을 받았을 때 수행할 현재 핸들러를 handler로 명시된 새로운 시그널 핸들러로 옮겨 해당 시그널을 처리한다. handler 함수는 일반 함수와는 달리 이 함수의 반환값을 받아 처리할 수 있는 곳이 없기 때문에 반드시 void 를 반환해야 한다. 유일한 인자는 처리될 시그널의 시그널 식별자(ex. SIGUSR2) 를 나타내는 정수이다. void my_handler (int signo); 현재 프로세스에 대해 시그널을 무시하게 하거나 시그널을 기본 동작으로 재설정하는 용도로도 커널에 signal() 함수를 사용할 수 있음. (handler 위치에 넣어줄 수 있음 ) SIG_DFL signo로 지정한 시그널에 대한 동작을 기본값으로 설정한다. SIG_IGN signo로 지정한 시그널을 무시한다. return 해당 시그널의 이전 동작인 시그널 핸들러에 대한 포인터 or SIG_DFL, SIG_IGN 을 반환 에러 발생 시 SIG_ERR 반환 10.2.1 모든 시그널 기다리기pause() 시스템 콜은 프로세스를 종료시키는 시그널을 받을 때까지 해당 프로세스를 잠재운다.(테스트와 디버깅에 유용함)#include &amp;lt;unistd.h&amp;gt;int pause (void); pause() 는 붙잡을 수 있는 시그널을 받았을 때만 반환되며 -1을 반환. 리눅스 커널에서 가장 단순한 시스템 콜 중 하나이다. 해당 프로세스를 인터럽트 가능한 잠들기 상태로 만듬 실행 가능한 다른 프로세스를 찾기 위해 schedule() 을 호출하여 리눅스 프로세스 스케줄러를 실행한다. 예시#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;signal.h&amp;gt;/* handler for SIGINT and SIGTERM */static void signal_handler (int signo){ if (signo == SIGINT) printf (&quot;Caught SIGINT!\\n&quot;); else if (signo == SIGTERM) printf (&quot;Caught SIGTERM!\\n&quot;); else { /* this should never happen */ fprintf (stderr, &quot;Unexpected signal!\\n&quot;); exit (EXIT_FAILURE); } exit (EXIT_SUCCESS);}int main (void){ /* * Register signal_handler as our signal handler * for SIGINT. */ if (signal (SIGINT, signal_handler) == SIG_ERR) { fprintf (stderr, &quot;Cannot handle SIGINT!\\n&quot;); exit (EXIT_FAILURE); } /* * Register signal_handler as our signal handler * for SIGTERM. */ if (signal (SIGTERM, signal_handler) == SIG_ERR) { fprintf (stderr, &quot;Cannot handle SIGTERM!\\n&quot;); exit (EXIT_FAILURE); } /* Reset SIGPROF&#39;s behavior to the default. */ if (signal (SIGPROF, SIG_DFL) == SIG_ERR) { fprintf (stderr, &quot;Cannot reset SIGPROF!\\n&quot;); exit (EXIT_FAILURE); } /* Ignore SIGHUP. */ if (signal (SIGHUP, SIG_IGN) == SIG_ERR) { fprintf (stderr, &quot;Cannot ignore SIGHUP!\\n&quot;); exit (EXIT_FAILURE); } for (;;) pause (); return 0;}10.2.3 실행과 상속 fork() 시스템 콜을 통해서 프로세스가 생성되면 자식 프로세스는 부모 프로세스의 시그널에 대한 동작을 상속받는다. 대기 중인 시그널은 상속되지 않는데, 대기 중인 시그널은 특정 pid로 보낸 것이지, 자식 프로세스로 보낸 것이 아니기 때문. exec 시스템 콜 을 통해서 프로세스가 처음 생성되면 모든 시그널은 부모 프로세스가 이를 무시하는 경우를 제외하고 모두 기본 동작으로 설정 됨실행과 상속10.2.4 시그널 번호를 문자열에 맵핑하기시그널 이름으로 코드를 작성하면 힘듦→ 시그널 번호를 시그널 이름의 문자열로 변환할 수 있음sys_siglistextern const char * const sys_siglist[];static void signal_handler (int signo){ printf (&quot;Caught %s\\n&quot;, sys_siglist[signo]);} 최선의 선택 시스템에서 지원하는 시그널 이름을 담고 있는 문자열의 배열 시그널 번호를 색인으로 이용함BSD에서 정의된 psignal() 인터페이스#include &amp;lt;signal.h&amp;gt;void psignal (int signo, const char *msg); msg 인자로 전달한 문자열을 stderr에 출력하는데, 콜론과 공백 그리고 signo로 지정한 시그널 이름이 따라옴더 나은 인더페이스 strsignal()#define _GNU_SOURCE#include &amp;lt;string.h&amp;gt;char * strsignal (int signo); signo로 지정한 시그널의 설명을 가리키는 포인터를 반환함 하지만 반환된 문자열은 다음에 strsignal() 을 호출하기 전까지만 유효하기 때문에 Thread-safe 하지 않다. strsignal() uses a static buffer and is not thread safe. Use bsd_strsignal() for thread safety. 10.3 시그널 보내기kill() 시스템 콜은 특정 프로세스에서 다른 프로세스로 시그널을 보낸다.#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;signal.h&amp;gt;int kill (pid_t pid, int signo); pid가 0보다 큰 경우 (일반적) pid가 가리키는 프로세스에 signo 시그널을 보냄 pid 0 호출한 프로세스의 프로세스 그룹에 속한 모든 프로세스에 signo 시그널을 보냄 pid -1 호출한 프로세스가 시그널을 보낼 권한이 있는 모든 프로세스에 signo를 보냄 호출한 프로세스 자신과 init은 제외 pid &amp;lt; -1 프로세스 그룹 -pid 에 signo를 보냄 10.3.1 권한다른 프로세스에 시그널을 보내기 위해서는 보내는 프로세스가 적절한 권한을 가지고 있어야함 CAP_KILL 기능이 있는 (root process) 프로세스는 모든 프로세스에 시그널을 보낼 수 있음 이 기능이 없을 경우 프로세스의 유효 사용자 ID 나 실제 사용자 ID는 반드시 시그널을 받는 프로세스의 실제 사용자 ID나 저장된 사용자 ID와 유효해야함 → 즉, 사용자는 자신이 소유하고 있는 프로세스에만 시그널을 보낼 수 있음 SIGCONT(프로세스 정지 후 계속 수행) 에 대한 예외를 정의함. signo가 0 (null 시그널) 이라면 시그널을 보내진 않지만, 에러 검사는 수행하기 때문에 권한 체크가 가능함!int ret;ret = kill (1722, 0);if (ret) ; /* we lack permission */else ; /* we have permission */10.3.3 자신에게 시그널 보내기raise() 함수는 자기 자신에게 시그널을 보낼 수 있는 간단한 방법을 제공함#include &amp;lt;signal.h&amp;gt;int raise (int signo);raise (signo);===kill (getpid(), signo);10.3.4 프로세스 그룹 전체에 시그널 보내기프로세스 그룹 ID를 음수로 바꿔서 kill() 을 사용하는 것이 아니라 프로세스 그룹에 속한 모든 프로세스에 시그널을 보낼 수 있는 함수도 있음#include &amp;lt;signal.h&amp;gt;int killpg (int pgrp, int signo); killpg (pgrp, signo);===kill (-pgrp, signo);10.4 재진입성 (Reenterancy) Reenterant 함수 둘 이상의 스레드에 의해서 호출되었을 때, 호출된 순서에 상관없이 하나가 수행되고 난 다음 다른 함수 호출이 수행된 것처럼 제대로 된 결과를 반환해주는 함수를 의미 interrupt handler 와 signal handler에서 찾아볼 수 있음 특성 no static (or global) non-constant data not return the address to static (or global) non-constant data … 예시 function_a()가 호출되고 있는 도중 interrupt가 발생 interrupt_handler() 가 수행 interrupt_handler() 내부에서 function_a()를 다시 사용해도, 기존에 수행중이던 function_a() 의 수행 결과에 영향을 주면 안된다는 것이 Reenterancy 이다. Thread-safe VS Reenterancy Thread-safe : A Function that may be safely invoker soncurrently by multiple threads 즉, 멀티 스레드 환경에서 올바른 결과를 내어주는 함수를 의미 모든 reenterant 함수는 thread-safe 하지만, 모든 thread-safe 함수가 reenterant 함수인 것은 아니다. 커널이 시그널을 보낼 때, 프로세스는 코드 어디선가에서 실행 중인 상태이다.해당 시그널의 핸들러는 어떤 작업 도중에도 실행이 가능하다. 따라서 프로세스에 설정된 시그널 핸들러는 자신이 실행하는 작업과 자신이 손대는 데이터(특히 Global Data 를 수정할 때)를 아주 조심스럽게 다뤄야 함 !10.5 시그널 모음여러개의 시그널을 간편하게 다루기 위해서는 시그널을 집합으로 표시하는 자료 형식이 필요int 형식을 한 비트마다 하나의 신호로 대응시켜서 표시할 수 있지만 signal은 32개보다 많음→ sigset_t 라는 자료 형식이 만들어짐.sigset_t 와 같은 신호 집합을 사용하는 이유는 많은 신호를 간편하게 다루기 위함모든 신호를 막는다거나 (BLOCK), 막은 신호를 다시 푼다거나(UNBLOCK), 신호가 발생했지만 Block 되어서 대기(PENDING) 중인 신호가 무엇이 있는가 를 쉽게 파악할 수 있음 SIGSTOP과 SIGKILL은 절대 제어할 수 없음#include &amp;lt;signal.h&amp;gt;int sigemptyset (sigset_t *set);int sigfillset (sigset_t *set);int sigaddset (sigset_t *set, int signo);int sigdelset (sigset_t *set, int signo);int sigismember (const sigset_t *set, int signo); sigemptyset() set으로 지정된 시그널 모음을 비어있다고 표시하며 초기화 함 sigfillset() set으로 지정된 시그널 모음을 가득 차 있다고 표시하며 초기화 함 sigaddset() set으로 지정된 시그널 집합에 signo를 추가함 sigdelset() set으로 지정한 시그널 모음에서 signo를 제거함 sigismember() set으로 지정한 시그널 모음에서 signo가 있으면 1을 반환, 그렇지 않아면 0을 반환 10.5.1 추가적인 시그널 모음 함수 이 함수들은 유용하지만 POSIX 호환이 중요한 프로그램에서는 사용하면 안됨#define _GNU_SOURCE#define &amp;lt;signal.h&amp;gt;int sigisemptyset (sigset_t *set);int sigorset (sigset_t *dest, sigset_t *left, sigset_t *right);int sigandset (sigset_t *dest, sigset_t *left, sigset_t *right); sigisemptyset() set으로 지정된 시그널 모음이 비어있는 경우에는 1, 그렇지 않으면 0을 반환 sigorset() 시그널 모음인 left와 right의 합집합을 dest에 넣음 sigandset() 시그널 모음인 left와 right의 교집합을 dest에 넣음 10.6 시그널 블록시그널 핸들러와 프로그램의 다른 부분이 데이터를 공유해야 할 필요가 있다면 어떻게 할까?일시적으로 시그널 전달을 보류하여 이 영역을 보호한다.→ 시그널은 블록 되었다고 표현함 블록되는 동안 발생하는 어떤 시그널도 블록이 해제될 때까지는 처리되지 않음. 프로세스는 여러 시그널을 블록할 수 있으며 프로세스가 블록한 시그널 모음을 해당 프로세스의 시그널 마스크 라고 한다. sigprocmask() 는 how값에 따라 다르게 동작하며 프로세스 시그널 마스크를 관리한다.#include &amp;lt;signal.h&amp;gt;int sigprocmask (int how, const sigset_t *set, sigset_t *oldset);how SIG_SETMASK : 호출한 프로세스의 시그널 마스크를 set으로 변경함 SIG_BLOCK : 호출한 프로세스의 시그널 마스크를 현재 마스크와 set의 합집합으로 변경 SIG_UNBLOCK : 기존의 블록된 시그널에서 set의 시그널을 제거 oldset 이 null이 아니라면 이전 시그널 모음을 oldset에 넣는다. set 이 null인 경우 how를 무시하고 시그널 마스크를 변경하지 않지만, 시그널 마스크를 oldset에 넣는다 → set에 null 값을 넣어 전달하면 현재 시그널 마스크를 조회할 수 있음 예시#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;signal.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int main(){ sigset_t set, oldset; // set과 oldset을 깨끗이 비워줌 sigemptyset(&amp;amp;set); sigemptyset(&amp;amp;oldset); // sigaddset으로 set에 SIGINT와 SIGQUIT을 추가 sigaddset(&amp;amp;set,SIGINT); sigaddset(&amp;amp;set,SIGQUIT); // set에 있는 시그널들을 block 시키기 위해서 sigprocmask를 호출하는데 how의 인자는 SIG_BLOCK sigprocmask(SIG_BLOCK,&amp;amp;set,NULL); printf(&quot;SIGINT와 SIGQUIT는 블록되었습니다.\\n&quot;); printf(&quot;Ctrl+C와 Ctrl+\\\\ 눌러도 반응이 없습니다.\\n&quot;); //만약 Ctrl + \\(SIGQUIT)을 눌렀다면 5초후 Coredump가 생기고 종료 //SIGQUIT의 기본동작은 Coredump + 종료 sleep(5); //현재 set에서 SIGINT를 뺌. set에는 SIGQUIT만 있는 상태 //중요한것은 프로세스에 적용하지 않은 상태 sigdelset(&amp;amp;set,SIGINT); //프로세스에 Unblock을 set에 적용. SIGQUIT은 이제 Block되지 않음 sigprocmask(SIG_UNBLOCK,&amp;amp;set,&amp;amp;oldset); printf(&quot;만약 Ctrl+\\\\을 눌렀다면 종료합니다.\\n&quot;); printf(&quot;현재 남은 시그널은 SIGINT입니다.\\n&quot;); printf(&quot;Ctrl+C를 눌러도 반응이 없습니다.\\n&quot;); sleep(5); set=oldset; sigprocmask(SIG_SETMASK,&amp;amp;set,NULL); printf(&quot;다시 SIGINT와 SIGQUIT이 블록됩니다.\\n&quot;); printf(&quot;Ctrl+C와 Ctrl+\\\\ 눌러도 반응이 없습니다.\\n&quot;); sleep(5); sigprocmask(SIG_UNBLOCK,&amp;amp;set,NULL); //아무 시그널(Cntl +C 혹은 Cntl+\\)을 주지 않았다면 아래의 메시지가 출력되고 종료 printf(&quot;모든 시그널이 해제되었습니다.\\n&quot;);}10.6.1 대기 중인 시그널 조회하기#include &amp;lt;signal.h&amp;gt;int sigpending (sigset_t *set); 커널에서 블록된 시그널이 발생할 경우, 이 시그널은 전달되지 않는다. sigpending() 는 대기 중인 시그널 모음을 조회할 수 있음 호출이 성공하면 대기 중인 시그널 모음을 set에 넣고 0을 반환함. 실패하면 -1을 반환10.6.2 여러 시그널 기다리기 프로세스가 자신의 시그널 마스크를 일시적으로 변경하고, 자신을 종료시키거나 자신이 처리할 시그널이 발생할 때까지 기다리게 만든다.#include &amp;lt;signal.h&amp;gt;int sigsuspend (const sigset_t *set); 시그널을 BLOCK시킴과 동시에 대기함. sigprocmask같은 경우 how를 SIG_BLOCK이나 SIG_SETMASK로 설정하면 블록하기만 할뿐 대기하지는 않는데, sigsuspend는 블록과 대기를 동시에 할 수 있음. 성공시 0, 실패시 -1을 반환 활용 방법 프로그램이 critical section에 머물러 있을 때 도착해서 블록되었던 시그널을 조회할 수 있음. 10.7 고급 시그널 관리 signal()은 시그널 핸들러를 구축하는 오래되고 가장 간단한 방법이지만, 매우 기초적이고 시그널 관리를 위한 최소한의 부분만 제공함 sigaction() 시스템 콜이 훨씬 더 훌륭한 시그널 관리 능력을 제공한다.#include &amp;lt;signal.h&amp;gt;int sigaction (int signo, const struct sigaction *act, struct sigaction *oldact); sigaction() 을 호출하면 signo로 지정한 시그널의 동작 방식을 변경한다. SIGKILL, SIGSTOP을 제외 act 가 NULL이 아닌 경우 시스템 콜은 해당 시그널의 현재 동작 방식을 act가 지정한 내용으로 변경함 oldact 가 NULL이 아닌 경우 해당 호출은 이전의 동작 방식( act가 NULL인 경우에는 현재의 방식) 을 oldact에 저장한다. sigaction 구조체는 시그널을 세세하게 제어할 수 있게 함struct sigaction { void (*sa_handler)(int); /* signal handler or action */ void (*sa_sigaction)(int, siginfo_t *, void *); sigset_t sa_mask; /* signals to block */ int sa_flags; /* flags */ void (*sa_restorer)(void); /* obsolete and non-POSIX */}; sa_handler 와 sa_sigaction이 유니언이라는 사실을 주의하고, 두 필드 모두에 값을 할당하지 않도록 해야 함 sa_handler 해당 시그널을 받았을 때 수행할 동작을 지정함. signal()과 마찬가지로 SIG_DFL, SIG_IGN, 시그널을 처리하는 함수를 가리키는 포인터가 들어올 수 있음 sa_sigaction void my_handler (int signo, siginfo_t *si, void *ucontext); 시그널 번호, siginfo_t 구조체, ucontext_t 구조체를 void 포인터로 타입 변환하여 받음. sa_flags 0개 혹은 하나 이상의 플래그에 대한 비트마스크. 해당 플래그들은 signo로 지정한 시그널의 처리를 변경함. (p.456 플래그 설명) SA_SIGINFO 를 설정하면 sa_handler 가 아니라 sa_signaction이 시그널을 처리하는 함수를 명시한다. SA_NODEFER 를 설정하지 않으면 현재 처리 중인 시그널도 블록됨. sa_mask 시그널 핸들러를 실행하는 동안 시스템이 블록해야 할 시그널 모음을 제공함 이 필드를 사용해서 여러 시그널 핸들러 간의 재진입을 적절하게 막을 수 있다. 10.7.1 siginfo_t 구조체siginfo_t 구조체에는 sa_handler 대신 sa_sigaction 을 이용하는 경우 시그널 핸들러로 전달할 정보가 가득함시그널을 보낸 프로세스에 대한 정보와 시그널을 일으킨 원인에 대한 정보를 포함하여 흥미로운 데이터가 많음.(p.458 각 필드 설명)typedef struct siginfo_t { int si_signo; /* signal number */ int si_errno; /* errno value */ int si_code; /* signal code */ pid_t si_pid; /* sending process&#39;s PID */ uid_t si_uid; /* sending process&#39;s real UID */ int si_status; /* exit value or signal */ clock_t si_utime; /* user time consumed */ clock_t si_stime; /* system time consumed */ sigval_t si_value; /* signal payload value */ int si_int; /* POSIX.1b signal */ void *si_ptr; /* POSIX.1b signal */ void *si_addr; /* memory location that caused fault */ int si_band; /* band event */ int si_fd; /* file descriptor */}; si_code 프로세스가 왜 그리고 어디서부터 시그널을 받았는지에 대한 설명이 있음 si_addr SIGBUS, SIGFPE, SIGILL, SIGSEGV, SIGTRAP 의 경우 이 void 포인터는 장애를 일으킨 주소를 저장함. si_value si_int와 si_ptr 의 유니언이다. 10.7.2 si_code 의 멋진 세계 si_code 필드는 시그널을 일으킨 원인을 알려주는데, 커널이 시그널을 보냈을 때 이 필드를 보면 왜 시그널을 보냈는지 알 수 있다. 모든 시그널에 대해서 유효한 값이 있고, 각 시그널에 대해서 유효한 값들도 존재한다. (p.459 참고)10.8 페이로드와 함께 시그널 보내기앞에서 확인했듯이 sigaction에서 SA_SIGINFO 플래그가 함께 등록된 시그널 핸들러는 siginfo_t 인자를 전달하는데, siginfo_t 구조체는 si_value 라는 필드를 포함하고있다.si_value 필드는 시그널을 생성한 곳에서 시그널을 받는 곳까지 전달되는 선택적인 페이로드이다.sigqueue() 함수를 이용해서 페이로드와 함께 시그널을 보낼 수 있다.#include &amp;lt;signal.h&amp;gt;int sigqueue (pid_t pid, int signo, const union sigval value); sigqueue() 는 kill()과 유사하게 동작 호출이 성공하면 signo 시그널은 pid 프로세스나 프로세스 그룹 큐에 들어가고 0을 반환 value int 와 void 포인터의 유니언임 union sigval { int sival_int; void *sival_ptr; }; 10.8.1 시그널 페이로드 예제sigval value;int ret;value.sival_int = 404;ret = sigqueue (1722, SIGUSR2, value);if (ret) perror (&quot;sigqueue&quot;); pid가 1722 인 프로세스에 404라는 정수 값을 페이로드에 담아 SIGUSR2 시그널과 함께 보낸다.10.9 시그널은 미운 오리 새끼? 시그널은 커널과 사용자 간 통신을 위한 구식 메커니즘이며 IPC의 원시적인 형태로 볼 수 있다. 멀티스레딩 프로그램과 이벤트 루프 세계에서 시그널은 적절하지 않음. 하지만 시그널은 커널에서 수많은 통보를 수신할 수 있는 유일한 방법이다!ref. https://d-yong.tistory.com/10 / http://slideplayer.com/slide/10812592/https://reakwon.tistory.com/53" }, { "title": "[번역] The Code Review Pyramid", "url": "/posts/eng03-code/", "categories": "Study, Translate, codereview", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-07-09 00:00:00 +0900", "snippet": " Code review Pyramid에 대한 글 번역원본 링크The Code Review Pyramid코드리뷰에 대해서 이야기할 때 코드 포매팅이나 스타일에 관한 재미없는 논의들이 길고 지루하게 초점을 맞추는 경향이 있는 반면 중요한 측면(코드의 변경이 의도와 맞는지, 성능저하는 없는지, 기존 클라이언트와 호환은 잘 되는지 등)들은 덜 주목을 받는다.이 이슈에 대한 인지도를 높이고 초점을 맞춰야하는 측면에 대한 가이드를 제공하기 위해서 트위터에 Code Review Pyrimid라고 내가 부르는 하나의 이미지를 올렸다. 이 트윗의 의도는 코드리뷰를 하는 동안 어떤 중요한 부분에 초점을 맞춰야하는지, 어떤 부분이 자동화가 될 수 있고 되어야하는지에 대해서 알려주기 위함이다.몇몇 분들이 이 자료에 대해서 영구적이고 참조할 수 있는 포스팅과 고화질 사진을 원해서 여기다가 다시 올려둔다 :)Code Style (코드 스타일) 프로젝트의 포매팅 스타일이 적용되었나요? 네이밍 컨벤션을 잘 적용했나요? DRY(Don’t Repeat Yourself) 한가요? 중복이 없나요? 코드가 충분히 읽을만 한가요? (메서드 길이 등…)Tests (테스트) 모든 테스트를 패스했나요? 새로은 기능이 합리적으로 테스트되었나요? 코너 케이스들도 테스트되었나요? 유닛테스트가 가능한 곳이라면 유닛테스트를, 통합테스트가 필요한 곳이라면 통합테스트를 사용하고 있나요? NFR(non-functional requirement, 비기능요구사항) 들에 대한 테스트도 있나요? e.g. 성능Documentation (문서화) 새로운 기능이 합리적으로 문서화되어있나요? 관련 문서들 (README, API 문서, 사용자 가이드, 참조문서 등)이 잘 업데이트 되었나요? 문서들이 이해가 잘 되고 오타나 문법적인 실수는 없나요?Implemetaion Semantics 기존 요구사항을 잘 만족하나요? 논리적으로 정확한가요? 불필요한 복잡성이 없나요? 견고한가요? (동시성 이슈, 적절한 에러 핸들링 등) 성능 저하 문제는 없나요? 안전한가요? (SQL 인젝션 등) 관측 가능한가요? (메트릭, 로그, 추적 등) 덩치를 키우는 의존성이 새롭게 추가되진 않았나요? 라이센스는 적절한가요?API Semantics API가 줄일 수 있는 것은 잘 줄이고 필요한 만큼만의 크기로 잘 짜여졌나요? 여러가지 일을 수행하는 것이 아니라 한 가지 일만 잘 수행하고 있나요? 일관성이 있나요? 최소한의 원칙을 따르고 있나요? API 내부에서 구멍 뚫린 곳 없이 깔끔하게 나누어져 있나요? 유저가 직접적으로 겪는 변화는 없나요? (API 클래스들, 설정, 메트릭, 로그 포맷 등) 새로운 API가 일반적으로 유용하고 오버스펙이진 않나요?FAQ 왜 피라미드인가요?-&amp;gt; 피라미드의 하위 부분은 코드리뷰의 기초가 되어야하고 대부분을 차지해야 하기 떄문 에이, 저거 삼각형이잖아요! -&amp;gt; 그렇게 생각할 수도 있지, 근데 저거는 피라미드의 측면이야 저 그림 그리는데 어떤 툴 사용했나요?-&amp;gt; Excalidraw" }, { "title": "[Code Review] 코드리뷰 참고자료 정리", "url": "/posts/cr-01/", "categories": "Code Review", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages, codereview", "date": "2022-07-08 00:00:00 +0900", "snippet": "Code Review 란소프트웨어를 실행하지 않고 사람이 직접 검토하는 과정을 통해서, 잠재된 결함을 찾아내고 이를 개선해나가면서 전반적인 소프트웨어의 품질을 높이고자 하는 활동Programmer’s Ego개발자는 자신의 코드를 자신의 것으로 여기고 비판을 수용하지 않는 경우가 있다. 이를 Programmer&#39;s Ego라고 한다.The Ten Commandments of Egoless Programming ref. Understand and accept that you will make mistakes. (당신이 실수했다는 것을 이해하고 받아들여라) You are not your code. (당신은 당신의 코드가 아니다) No matter how much “karate” you know, someone else will always know more. (‘Karate’를 당신이 얼마나 알던지간에, 누군가는 당신보다 더 잘 알 수 있다. ) Don’t rewrite code without consultation (협의없이 코드를 다시 작성하지마라) Treat people who know less than you with respect, deference, and patience. (당신보다 많이 알지 못하는 사람이라 해도 존중과 인내로 대해라) The only constant in the world is change. (이 세상의 유일한 상수는 세계가 변화한다는 것이다.) The only true authority stems from knowledge, not from position. (권위는 지위에서 오는 것이아니라 오직 지식으로부터 나온다. ) Fight for what you believe, but gracefully accept defeat. (당신이 믿는 것에 대해서 투쟁하되, 우아하게 패배를 받아들여라) Don’t be “the guy in the room.” (방 안에 혼자 박혀있는 사람이 되지 마라) Critique code instead of people – be kind to the coder, not to the code. (사람이 아니라 코드를 비판해라 - 코드에게는 친절하지 안되, 코더에게는 친절해라 )Code review에 대한 공부최근 코드리뷰에 대해서 엄청난 필요성을 느끼고 관심을 가져보고 있다. (늦었다고 생각할 때가 제일 빠를때.. ㅎㅎ)이전부터 팀원 분들과의 협업 문화에 많은 갈증을 느끼고 있던 나였다. 하지만 협업문화를 누가 나에게 떠먹여주지 않는다는 것을 다시 한번 깨닫게 된 요즘.내가 열심히 공부해서 팀원분들과 문화를 만들어가고 함께 배워야한다고 생각했다.아래는 한번씩 보면 좋을 내용들을 정리해봤다.읽을 서적 읽기 좋은 코드가 좋은 코드다 좋은 코드, 나쁜 코드(Good Code, Bad Code) 구글 엔지니어는 이렇게 일한다(9장, 코드리뷰) 클린 코드기사/글 코드 리뷰 in 뱅크샐러드 개발 문화 (라인) 효과적인 코드 리뷰를 위해서 구글의 코드 리뷰 가이드 소스코드 리뷰에 대한 짧은 이야기… (카카오) 코드리뷰를 시작하려는 그대에게" }, { "title": "[Movie] Contact (with.스포) (WIP) ", "url": "/posts/movie/", "categories": "Movie", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-06-19 00:00:00 +0900", "snippet": "[Movie] ContactOcakham’s Razor All things being equal, the simplest solution tends to be the best one.전능하고 신비한 신이 우주를 창조하고 자신의 존재를 증명할 여지를 남기지 않은 것All powerful mysterious god created universe and decided not to give any proof to existencevs혹은 신은 원래 없었고, 우리가 나약하고 외로워서 만들어낸 존재라는 것Or god doesn’t exist and we created them어떤 것이 더 간단한가?주인공 앨리는 신이 없는 세상을 상상할 수 없다는 팔머에게 신의 존재가 착각이 아니라면 증거를 달라고 한다. 이때 팔머는 앨리에게 되묻는다.“Did you love your father?”“Yes, very much”“Prove it”아버지를 사랑했다면 증명해봐요. 라는 팔머의 질문에 대답을 하지 못하는 앨리.마음으로서 느끼는 감정과 신의 존재를 동등하게 볼 수 있을까?차원 이동을 경험하고 온 앨리는 청문회를 가게 된다.“멋지네요, 그들이 증거를 주길 거부해서 당신이 증거가 없군요.정신과에선 그런 걸 과대 망상증이라고 합니다. “여기서 과연 종교적인 것 그리고 과학적인 것은 이 자가 말하는 과대 망상증과 다른 것이 무엇일까. 우리는 어떤 것을 믿고 어떻게 살아가야 할까 ?“박사님, 혹시 과학 이론 중에 오컴의 면도날을 아시오?”“네, 모든 조건이 동일하면 가장 단순한 설명이 답일 수 있다.”“바로 그거에요. 이제 말해봐요 어떤게 더 말이 되죠?외계인으로부터 메시지가 와서 마법의 기계를 타고, 은하계의 중심에 갔다가 아버지와 윈드 서핑을 한 후 한 순간 후에 증거 하나 없이 집으로 돌아온 것? 아니면 당신의 경험은 해든의 마지막 퍼포먼스에 자신도 모르게 출연한 결과였다는 것? “박사는 말한다.“이 일이 일어나지 않을 가능성이 있냐구요? 네 있습니다.과학자로서 그 가능성도 인정해야만 하죠.하지만 그럴 수 없어요. 저는 경험했습니다.증거도 없고 설명할 수도 없지만, 인간으로서 제가 아는 모든 것이 그것이 실제였다고 말하니까요.저를 완전히 바꿔준 멋진 경험을 선물받은거에요.저는 봤습니다.우주에서 우리가 얼마나 작고 하찮고 얼마나 드물고도 소중한지요.우리는 우리보다 더 큰 뭔가에 속해 있고, 우리 중 누구도 혼자가 아니라는 것을요.그걸 모두와 나눌 수 있었으면 좋겠어요.제 소망은,, 모두가 한 순간만이라도 그 경외심과 겸허함과 희망을 느낄 수 있었으면 하는 겁니다.하지만 그저 제 소망일 뿐이겠죠.”증거가 없다고 믿지 못하면 종교는 어떻게 믿는단 말인가?이 장면을 보고 나는 그런 생각이 들었다.가끔 종교인들 중 신을 보았고 대화를 했다는 사람이 있다. 나는 그런 사람들을 믿지 못했다. 하지만 위의 장면과 다를 것이 무엇인가? 그는 보았고 그가 느낀 경외심, 희망을 나누고 싶었던 것 뿐이다.마지막 장면에서 기자들은 팔머에게 질문을 한다.“당신은 무엇을 믿습니까?”“신앙인으로서 저는 애로웨이 박사와는 다른 서약을 따르지만, 우리의 목표는 같습니다. 진실을 추구하는 것. 저는 그녀를 믿습니다.”너희는 단절되고 외롭다고 생각해. 하지만 그렇지 않아.그거 아니? 우리가 찾아낸 것 중에 이 공허함을 견딜 수 있게 하는 유일한 것은 서로밖에 없어." }, { "title": "[Golang] go module &#39;connection refuesd&#39; 오류 해결", "url": "/posts/tb_golang/", "categories": "Trouble Shooting, Golang", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-06-03 00:00:00 +0900", "snippet": "Go module ‘connection refuesd’ 오류 해결에러 go mod tidy 로 module 설치 시 connection refused에러 발생github.in/misc@v0.0.0-20220602070448-f6621fa768b4: verifying go.mod: github.in/misc@v0.0.0-20220602070448-f6621fa768b4/go.mod: reading https://sum.golang.org/lookup/github.in/misc@v0.0.0-20220602070448-f6621fa768b4: 410 Gone server response: not found: github.in/misc@v0.0.0-20220602070448-f6621fa768b4: unrecognized import path &quot;github.in/misc&quot;: https fetch: Get &quot;https://github.in/misc?go-get=1&quot;: dial tcp 10.182.235.107:443: connect: connection refusedmake: *** [mod_tidy] Error 1해결법$ export GOSUMDB=off$ go get -uor# Private repo 의 특정 commit 을 패키지로 설치하려면 commit hash 값을 넣어주고 go mod tidy 를 수행하자!# go.mod -&amp;gt; github.in/misc 399aadab40d5f49539d80310ff133f6c31fe51f8$ go mod tidy# -&amp;gt; 자동으로 해당 패키지를 get 해 옴참고 : https://lejewk.github.io/go-mod/" }, { "title": "[Linux] User에 Sudo 권한 부여하기 (feat. Jupyter notebook)", "url": "/posts/tb_linux/", "categories": "Trouble Shooting, Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-05-12 00:00:00 +0900", "snippet": "User에 Sudo 권한 부여하기 (feat. Jupyter notebook)Su Superuser 라는 뜻 Linux에서 모든 것들을 접근하고 수정할 수 있다.Sudo SuperUser DO 에서 유래하였으나 Substitute User Do (다른 사용자의 권한으로 실행) 의 줄임말로 해석됨. 기본적으로 사용자 비밀번호를 요구하지만 Nopassword 옵션을 줄 수 도 있다.사용법 /etc/sudoers 에 sudo 정보가 저장이 되어있음. readonly이기 때문에 visudo 로 열어야 수정 가능# User privilege specificationroot ALL=(ALL:ALL) ALL root 는 이렇게 되어있음.새로운 유저 정보를 추가하기jovyan ALL=(ALL:ALL) ALL 요런식으로 /etc/sudoers 파일에 추가하면 됨. 근데 이렇게 하면 Password 요구하는데 password 없이 접속할 수 있게 하려면 이렇게 하면 됨.jovyan ALL=(ALL:ALL) NOPASSWD: ALL 나는 Dockerfile의 command Line 상에서 해당 user의 sudo 권한을 주고 싶었기 때문에$ root# echo &quot;jovyan ALL=(ALL:ALL) NOPASSWD: ALL&quot; &amp;gt;&amp;gt; /etc/sudoers&amp;gt;&amp;gt;# Dockerfile# Give sudo privilege to jovyanRUN echo &quot;jovyan ALL=(ALL:ALL) NOPASSWD: ALL&quot; &amp;gt;&amp;gt; /etc/sudoers 이런식으로 root 권한으로 해당 문구를 넣어주었다.참고 : https://ko.wikipedia.org/wiki/Sudo , https://info-lab.tistory.com/163" }, { "title": "[System Design Interview] 14. 유튜브 설계 ", "url": "/posts/system_arch_14/", "categories": "System Design Interview", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-05-02 00:00:00 +0900", "snippet": "14. 유튜브 설계 유튜브 시스템은 언뜻 보기에는 간단 (창작자가 비디오를 올리고, 시청자는 재생 버튼을 누름) 하지만 이면에는 매우 복잡함. DAU : 20억 매일 재생되는 비디오 수 : 50억 미국 성인 73% 사용 5천만명의 크리에이터 광고수입은 19년 기준 150억 달러 모바일 인터넷 트래픽 중 37%를 유튜브가 점유 80개 언어로 이용 가능1단계. 문제 이해 및 설계 범위 확정 댓글 비디오 공유 좋아요 버튼 재생목록 채널 구독 → 설계 범위를 좁혀야함. 빠른 비디오 업로드 원활한 비디오 재생 재생 품질 선택 기능 Infrastructure cost 높은 가용성과 규모확장성, 안정성 모바일 앱, 웹, 스마트 TV개략적 규모 추정 DAU : 5백만 한 사용자는 하루에 평균 5개의 비디오 시청 10%의 사용자가 하루에 1개의 비디오 업로드 비디오 평균 크기 300MB 비디오 매일 저장 용량 → 5백만 _ 10% _ 300MB = 150TB CDN (Content Delivery Network) aws 사용 5백만 _ 5비디오 _ 0.3GB * $0.02 = $150,000 2단계. 개략적 설계안 제시 및 동의 구하기 BLOB 이나 CDN 상세설계는 지나침 Client 컴퓨터, 모바일, 스마트 TV CDN 비디오는 CDN에 저장. 재생누르면 CDN으로부터 스트리밍 API Server 비디오 스트리밍을 제외한 모든 요청을 처리함. 피드 추천, 비디오 업로드 URL 생성, 메타데이터 데이터베이스와 캐시 갱신, 사용자 가입 등 비디오 업로드 절차 사용자 컴퓨터나 모바일 폰 을 통해 유튜브를 시청하는 이용자 로드 밸런서 API 서버 각각으로 고르게 분산 API 서버 비디오 스트리밍을 제외한 모든 요청 처리 메타데이터 데이터베이스 비디오의 메타데이터를 보관 샤딩과 다중화를 적용해서 성능 및 가용성 충족 메타데이터 캐시 성능을 위해 비디오 메타데이터와 사용자 객체 캐싱 Original Storage 원본 비디오를 보관한 대형 BLOB 시스템 BLOB 이진 데이터를 하나의 개체로 보관하는 DB관리 시스템 트랜스 코딩 서버 비디오 트랜스 코딩은 비디오 인코딩이라 불리는 절차 비디오의 포맷(MPEG, HLS) 을 변환하는 절차 단말이나 대역폭 요구사항에 맞는 최적의 비디오 스티림을 제공 트랜스 코딩 비디오 저장소 트랜스 코딩이 완료된 비디오를 저장하는 BLOB 저장소 CDN 비디오를 캐싱 Completion Queue 비디오 트랜스 코딩 완료 이벤트들 보관 메시지 큐 Competion Handler 트랜스 코딩 완료 큐에서 이벤트 데이터를 꺼내어 메타데이터 캐시와 디비를 갱신할 작업 서버들 프로세스 A. 비디오 업로드 비디오를 원본 저장소에 업로드 트랜스 코딩 서버는 원본 저장소에서 해당 비디오를 가져와 트랜스코딩을 시작 트랜스 코딩 완료되면 아래의 절차 병렬 수행 완료된 비디오를 트랜스 코딩 비디오 저장소로 업로드 완료 핸들러가 이벤트 데이터를 큐에서 꺼냄 완료 핸들러가 메타데이터 디비와 캐시를 갱신 API 서버가 단말에게 비디오 업로드가 끝나서 스트리밍 준비가 되었음을 알린다.프로세스 B. 비디오 메타데이터 갱신. (비디오 URL, 크기, 해상도, 포맷, 사용자 정보가 포함) 원본 저장소에 파일이 업로드되는 동안, 단말은 병렬적으로 비디오 메타데이터에 갱신 요청을 API서버에 보낸다.비디오 스트리밍 절차 스트리밍 프로토콜 비디오 스트리밍을 위해 데이터를 전송할 때 쓰이는 표준화 된 통신 방법 MPEG-DASH Moving Picture Experts Group / Dynamic Adaptive Streaming over HTTP HLS HTTP Live Streaming Microsoft Smooth Streaming Adobe HTTP Dynamic Streaming. HDS 프로토콜마다 지원하는 비디오 인코딩이 다르고 플레이어도 다름. CDN Edge Server가 비디오 전송을 담당할 것이기 때문에 latency가 아주 낮음.3단계. 상세 설계비디오 트랜스 코딩 비디오가 다른 단말에서도 순조롭게 재생되려면 다른 단말과 호환되는 Bitrate와 Format으로 저장되어야 함. Bitrate 비디오를 구성하는 비트가 얼마나 빨리 처리되어야 하는지를 나타내는 단위 높으면 일반적으로 고화질 비디오 트랜스코딩의 중요성 Raw video는 사이즈가 큼. 호환성 문제 네트워크 대역폭에 따라 화질이 달라져야 사용자에게 끊김이 없음. 모바일 단말의 경우 네트워크 상황에 따라 비디오 화질을 자동 변경 인코딩 포맷 컨테이너 .avi, .mov, .mp4 코덱 압축 및 압축 해제 알고리즘 H.264, VP9, HEVC 유향 비순환 그래프 (DAG) 모델 크리에이터 각자 자기만의 비디오 프로세싱 요구사항이 있는데 (썸네일, 워터마크, 화질… ) 이 부분을 파이프라인을 지원함으로써 클라이언트가 직접 task 정의가 가능 원본 비디오 비디오 검사 비디오 인코딩 : 해상도, 코덱, 비트레이트 인코딩 조합 섬네일 워터마크 : 오버레이 형태로 표시 오디오 오디오 인코딩 메타데이터 병합비디오 트랜스 코딩 아키텍쳐 전처리기 비디오 분할 : 비디오 스트림을 Group Of Pictures라고 불리는 단위로 쪼갬. DAG 생성 : 클라이언트 프로그래머가 작성한 설정 파일에 따라 DAG를 만듬. 데이터 캐시 : 안정성을 높이기 위해 GOP와 메타데이터를 임시 저장소에 보관 DAG 스케줄러 DAG 그래프를 stage로 분할한 다음 각각 자원 관리자의 Queue에 넣음. 자원 관리자 자원 배분을 효과적으로 수행 세 개의 큐와 작업 스케줄러 작업 큐 실행할 작업이 보관되어 있는 우선순위 큐 작업 서버 큐 실행 큐 작업 스케줄러 작업 서버 DAG에 정의된 작업을 수행 임시 저장소 메타데이터는 캐시 / 비디오,오디오는 BLOB 인코딩된 비디오 인코딩 파이프라인의 최종 결과물 시스템 최적화 속도 최적화 : 비디오 병렬 업로드 하나의 비디오는 작은 GOP들로 분할 가능. 분할한 GOP를 병렬적으로 업로드 속도 최적화 : 업로드 센터를 사용자 근거리에 지정 Region Center 이용 속도 최적화 : 모든 절차를 병렬화 느슨하게 결합된 시스템을 만들어서 병렬성을 높이는 것 각 작업 사이에 메시지 큐 도임 Async하게 진행 가능 안전성 최적화 : 미리 사인된 업로드 URL Pre-signed 업로드 URL을 사용해서 authorized 사용자만 업로드할 수 있게 수행 POST 요청시 미리 사인된 URL을 받음. 클라이언트는 해당 URL으로 비디오 업로드 진행 안전성 최적화 : 비디오 보호 Digital Rights Management 도입 AES 암호화 Watermark 비용 최적화 CDN 은 비쌈. 비디오 스트리밍은 롱케일 분호를 따름. 인기있는 비디오는 빈번, 나머지는 거의 보는 사람이 없음. 1. 인기비디오는 CDN, 다른 비디오는 비디오 서버 2. 인기없으면 인코딩 안함 3. 지역별로 구분 4. CDN직접 구축 오류 처리 Highly Fault Tolerant 시스템을 만들어야 함. Recoverable Error 비디오 세그먼트 트랜스코딩 실패 → retry 혹은 오류 코드 반환 Non Recoverable Error 비디오 포맷이 잘못됨. 중단 후 오류 코드 반환 4단계. 마무리 API 서버 규모 확장성 : 수평적 스케일 아웃 디비 규모 확장성 : 다중화 , 샤딩 라이브 스트리밍 레이턴시 낮아야함. 스트리밍 프로토콜 비디오 삭제" }, { "title": "[K8S] Pod 스케줄링 에러 scheduler 0/5 nodes are available", "url": "/posts/k8s_taint/", "categories": "Kubernetes", "tags": "kubernetes, k8s, blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-28 00:00:00 +0900", "snippet": "Pod 스케줄링 에러 scheduler 0/5 nodes are available쿠버네티스에서 Pod 배포를 했을 때Warning FailedScheduling 40s (x5 over 3m50s) default-scheduler 0/5 nodes are available: 1 Too many pods, 1 node(s) had taint {node-role.kubernetes.io/master: }, that the pod didn&#39;t tolerate, 3 node(s) didn&#39;t match Pod&#39;s node affinity/selector.요런 에러 발생.정확하게 읽어보지도 않고 구글링했더니 명확한 답이 안나왔음.근데 하나 하나 제대로 읽어보니 Warning 에 답이 있었다… 1개의 노드는 Pod가 너무 많이 떠있고, 1개의 노드는 Taint (pod가 못뜨게 “얼룩”이 묻은 것임. master) 가 있고, Taint는 얼룩이라는 뜻. Node에 얼룩을 묻힐 수 있는데 얼룩이 있는 Node에는 pod가 마음대로 못뜸. 만약 Taint 가 있는 Node에 Pod를 띄우고 싶으면 Tolerance 값을 줘야함 (얼룩을 참는다는 의미!) 3개의 노드는 node affinity (node 친화도. 즉, 붙고 싶은 node가 이미 있다.)가 맞지 않다. Node Affinity 는 노드 친화도이다. Node selector와 거의 유사. 하지만 조금 더 유연하게 사용가능. 그러니깐 node affinity에다가 node 4 해놓으면 4번 노드에만 붙으려고 한다! 전체 노드는 5개이니 위의 케이스 중 하나가 잘못되어있다!!확인해보니 master 1개, affinity 는 4번 node!! 즉, 1개의 노드에 너무 많은 Pod가 떠 있어서 새로운 pod가 뜨지 못하는 스케줄링 이슈 발생.k get nodes -o yaml | grep pods로 각 노드의 max pod 수를 확인할 수 있다.해당 노드의 pod 하나를 삭제하니 정상적으로 스케줄링이 되는 것을 확인!" }, { "title": "[K8S] Kubectx 사용 시 선택한 kubeconfig 를 제대로 못 불러오는 문제 해결", "url": "/posts/k8s-kubeconfig/", "categories": "Kubernetes", "tags": "kubernetes, k8s, blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-21 00:00:00 +0900", "snippet": "Kubectx 사용 시 선택한 kubeconfig 를 제대로 못 불러오는 문제 해결 DOCS 여기에 거의 모든 정보가 나와있다.나는 Kubectx 라는 툴을 사용해서 다중 클러스터를 선택하고 있다. (매우 유용함!!)근데 클러스터 몇 개를 추가했더니, 선택한 새로운 클러스터 정보를 가져오는 것이 아니라 계속 똑같은 정보만 가져오는 것이다…좀 살펴봤더니 Kubectl의 Config 정보에 문제가 있었다.kubectl config view위의 명령어를 치면 저장된 kubectl config 값이 출력된다. k config viewapiVersion: v1clusters:- cluster: certificate-authority-data: DATA+OMITTED server: foo1 name: bar1- cluster: certificate-authority-data: DATA+OMITTED server: foo2 name: bar2contexts:- context: cluster: cluster.local user: foobaruser1 name: foobar1- context: cluster: cluster.local2 user: foobaruser2 name: foobar2- context: cluster: cluster.local user: foobaruser2 name: foobar3current-context: foobaruser1kind: Configpreferences: {}users:- name: foobaruser1 user: client-certificate-data: REDACTED client-key-data: REDACTED- name: foobaruser2 user: client-certificate-data: REDACTED client-key-data: REDACTEDconfig 에 대해서 좀 읽을 수 있어야 하는데, 요건 위에 첨부한 docs를 참고하자.쉽게 말해서current-context 값을 참조해서, 해당 cluster를 해당 user로 로그인해서 접근하는 흐름이다.근데 내가 겪었던 문제는 context의 값이 동일한 경우이다.무조건 각각 다른 cluster Name 과 User Name을 가지고 있어야 각자의 secret으로 다른 클러스터에 접근할 수 있다.근데 나는 전달받은 kubeconfig 를 사용하다보니 위와 같이 동일한 clusterName을 가지고 접근하고 있었다. 정리하자면, kubectl config view 명령어로 저장된 config 를 확인해보고 current context가 정확하게 내가 지정한 cluster, user 를 사용하고 있는지! 중복된 값을 사용하고 있지는 않는지 확인해볼 필요가 있겠다!" }, { "title": "[Linux System Programming] Ch08 파일과 디렉터리 관리 ", "url": "/posts/linux_ch08/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-19 00:00:00 +0900", "snippet": "[Ch08 파일과 디렉터리 관리]File Discripter VS Inode 같은 파일을 open() 으로 두번 열었을 때의 구조 프로세스에서 파일 입출력은 open 함수로 연 작업을 구분하는 것이며, 실제 물리적인 파일이 같은지는 구분하지 않음. 각각의 fd를 부여하고 커널에서도 각각의 파일의 상태와 현재 작업 위치를 별도로 갖음 inode는 파일 시스템에 저장됨 모든 파일 혹은 디렉토리는 유니크한 inode 값을 가지고 있다. fd는 파일 시스템 안에 저장되지 않음 커널에 의해 만들어지고 커널 안 메모리에 있는 fd 테이블에 저장됨. 8.1 파일과 메타데이터 inode 번호는 파일시스템에서 유일한 숫자 값인데 파일은 inode를 참조한다.$ ls -i1689459 Kconfig 1689461 main.c 1680144 process.c 1689464 swsusp.c1680137 Makefile 1680141 pm.c 1680145 smp.c 1680149 user.c1680138 console.c 1689462 power.h 1689463 snapshot.c1689460 disk.c 1680143 poweroff.c 1680147 swap.c inode는 파일의 접근 권한, 마지막 접근 시간, 소유자, 그룹, 크기 그리고 파일의 데이터 위치와 같은 메타데이터를 저장함.8.1.1. stat 함수#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int stat (const char *path, struct stat *buf);int fstat (int fd, struct stat *buf);int lstat (const char *path, struct stat *buf); 메타데이터를 얻을 수 있음. stat path로 지정한 파일의 정보 반환 fstat fd로 지정한 파일의 정보 반환 lstat stat과 동일한데 심벌릭 링크일 경우 링크가 가리키고 있는 파일이 아닌 링크 그 자체의 정보를 반환한다 struct stat { dev_t st_dev; /* ID of device containing file */ ino_t st_ino; /* inode number */ mode_t st_mode; /* permissions */ nlink_t st_nlink; /* number of hard links */ uid_t st_uid; /* user ID of owner */ gid_t st_gid; /* group ID of owner */ dev_t st_rdev; /* device ID (if special file) */ off_t st_size; /* total size in bytes */ blksize_t st_blksize; /* blocksize for filesystem I/O */ blkcnt_t st_blocks; /* number of blocks allocated */ time_t st_atime; /* last access time */ time_t st_mtime; /* last modification time */ time_t st_ctime; /* last status change time */}; 정보는 stat 구조체에 저장됨.8.1.2 권한 주어진 파일의 권한 값을 변경#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;int chmod (const char *path, mode_t mode);int fchmod (int fd, mode_t mode); 불투명한 mode_t 정수타입으로 표현되는 mode의 유효한 값은 stat 구조체의 st_mode 필드에서 반환하는 값과 동일 파일 권한을 변경하려면 chmod() 나 fchmod() 를 호출하는 프로세스의 유효 ID가 파일의 소유자와 일치하거나 해당 프로세스에 CAP_FOWNER 기능을 사용할 수 있어야 함.8.1.3. 소유권#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int chown (const char *path, uid_t owner, gid_t group);int lchown (const char *path, uid_t owner, gid_t group);int fchown (int fd, uid_t owner, gid_t group); lchown 은 심벌릭 링크를 따라가지 않고 그 자체의 소유권을 변경함. 호출이 성공하면 파일 소유자를 owner로, 그룹을 group으로 변경한 다음 0을 반환. CAP_CHOWN 기능이 있는 프로세스 (보통은 Root 프로세스임.) 만이 파일의 소유자를 변경할 수 있다. uid, gid 가 모두 0 이면 root 를 뜻함. -1 을 넘기면 바뀌지 않음.8.1.4 확장 속성 xattrs 라고 불리기도 하는 확장 속성은 파일과 관련한 키/값을 연권짓는 메커니즘을 제공한다. 확장 속성은 보안을 위한 필수 접근 제어처럼 원래 설계에는 포함되지 않은 새로운 기능을 지원함. 사용자 영역 애플리케이션이 임의로 키/값을 생성하고 읽고 쓸 수 있다는 점키와 값 확장 속성은 유일한 키로 구분된다. 키는 ‘namespace.attrb’ 형태를 취함.확장 속성 네임스페이스 커널은 네임스페이스에 따라 접근 정책을 다르게 적용한다. 리눅스는 현재 4가지 확장 속성 네임스페이스를 정의하고 있음. system security trusted user8.1.5 확장 속성 연산" }, { "title": "[Linux System Programming] Ch07 스레딩 ", "url": "/posts/linux_ch07/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-09 00:00:00 +0900", "snippet": "[Ch07 스레딩] 스레딩은 단일 프로세스 내에서 실행 유닛을 여러 개 생성하고 관리하는 작업을 뜻한다. 스레딩은 Data-race condition과 Deadlock을 통해 어마어마한 프로그래밍 에러를 발생시키는 원인이다.7.1 바이너리, 프로세스, 스레드 바이너리 저장장치에 기록되어 있는 프로그램 특정 OS와 머신 아키텍처에서 접근할 수 있는 형식으로 컴파일되어 아직 실행되지 않은 프로그램. 프로세스 실행된 바이너리를 표현하기 위한 OS의 추상 개념 메모리에 적재되고 가상화된 메모리와 열린 fd, 연관된 사용자와 같은 커널 리소스 등을 포함 스레드 프로세스 내의 실행 단위로 가상화된 프로세서, 스택, 프로그램 상태 등을 포함. 프로세스는 실행 중인 바이너리이고, 스레드는 OS의 프로세스 스케줄러에 의해 스케줄링될 수 있는 최소한의 실행 단위를 뜻함. 하나의 프로세스는 스레드를 하나 이상 포함한다. 가상 메모리 프로세스가 실제 물리적인 RAM이나 디스크 저장장치에 맵핑된 메모리의 고유한 뷰를 사용할 수 있도록 함. 스레드가 아니라 프로세스와 관련 있음. 각 프로세스는 메모리에 대한 하나의 유일한 뷰를 갖지만 한 프로세스 내의 모든 스레드는 메모리를 서로 공유한다. 가상 프로세서 여러 프로세스 상에서 많은 프로세스가 멀티태스킹 중이라는 사실을 숨김으로써 프로세스가 혼자 실행되고 있다고 착각하게 만든다. 스레드와 관련이 있음. 각각의 스레드는 스케줄이 가능한 독립적인 요소이며, 단일 프로세스가 한번에 여러 가지 일을 할 수 있게 해준다. → 스레드는 프로세스와 마찬가지로 시스템의 프로세서 모두 소유했다는 환상을 가지지만, 가상 메모리의 모든 메모리를 소유했다는 환상을 가지고 있진 않다. 또한 프로세스 내의 모든 스레드는 메모리 주소 공간 전체를 공유한다.7.2 멀티 스레딩 멀티 스레딩의 장점 프로그래밍 추상화 작업을 나누고 각각 실행 단위로 할당하는 것은 좋은 접근 방법 병렬성 프로세서가 여러 개인 머신에서 효과적으로 병렬성을 구현할 수 있음 응답속도 향상 멀티스레딩을 이용해 오래 실행되는 작업을 워커 스레드에 맡기고 최소한 하나의 스레드는 사용자 입력에 대응하는 작업을 수행 입출력 블록 스레드를 사용하지 않으면 입출력을 블록하면서 전체 프로세스를 멈추게 만든다. 컨텍스트 스위칭 스레드의 전환보다 프로세스 단위의 전환이 훨씬 비싸다. 메모리 절약 스레드는 여러개의 실행단위를 활용하면서도 메모리를 공유하는 방법을 제공함. → 스레드는 멀티 프로세스의 대안이기도 하다. 7.2.1 멀티스레딩 비용 같은 프로세스에 속한 스레드는 리소스를 공유하는데 같은 데이터를 읽거나 쓰는 것이다. 따라서 스레드 동기화는 매우 중요하고 어떻게 동작하는지 이해하고 있어야 한다. 설계 시작부터 반드시 스레딩 모델과 동기화 전략을 고려해야함.7.2.2 멀티스레딩 대안 지연시간과 입출력상의 장점이 스레드 사용 이유라면? → 다중입출력, 논블록 입출력, 비동기식 입출력을 조합해서 사용 가능 입출력 작업이 프로세스를 블록하지 않도록 함 제대로 된 병렬화가 목표라면? N개의 프로세스를 N개의 스레드처럼 프로세서를 이용하도록 하고 리소스 사용, 컨텍스트 스위칭 비용의 오버헤드를 감수해서 해결 가능 메모리 절약? → 스레드보다 더 제한된 방식으로 메모리를 공유할 수 있는 도구를 리눅스는 제공함. 7.3 스레딩 모델 리눅스에서는 1:1스레딩을 사용한다.7.3.1 사용자 레벨 스레딩 N:1 스레딩 스레드가 N개인 프로세스 하나는 단일 커널 프로세스에 맵핑됨. 장점 애플리케이션이 커널의 관여 없이 스스로 어떤 스레드를 언제 실행할지 결정할 수 있으므로 Context switching 비용이 거의 안듬. 단점 하나의 커널 요소가 N개의 스레드를 떠받치고 있기 때문에 여러개의 프로세스 활용을 못하고, 제대로 된 병렬성 제공이 안됨. 사용자 스레드에서 I/O가 하나라도 발생하면 해당 프로세스는 I/O가 풀릴 때 까지 영원히 Block됨.7.3.2 하이브리드 스레딩 N:M7.3.3 코루틴과 파이버 코루틴과 파이버는 스레드보다 더 작은 실행 단위를 제공함. 코루틴 : 프로그래밍 언어에서 사용 파이버 : 시스템에서 사용되는 용어 리눅스는 이미 빠른 컨텍스트 스위칭 속도로 인해 커널 스레드의 성능을 극한까지 다듬어야 할 필요가 없어서, 코루틴이나 파이버에 대한 네이티브 지원이 없다.7.4 스레딩 패턴 스레드를 사용하는 애플리케이션을 작성할 때 가장 중요하면서 제일 먼저 해야하는 일은 애플리케이션의 처리과정과 입출력 모델을 결정짓는 스레딩 패턴을 결정하는 것!7.4.1 연결별 스레드 하나의 작업 단위가 스레드 하나에 할당되는 프로그래밍 패턴. 작업 단위가 실행되는 동안 많아 봐야 하나의 작업이 스레드 하나에 할당됨. 즉, 작업이 완료될 떄까지 실행 하는 패턴 스레드는 연결이나 요청을 받아서 완료될 때 까지 처리, 작업을 완료하면 다른 요청을 받아서 다시 처리 연결별 스레드 모델에서는 연결이 스레드를 소유하기 때문에 입출력 블록킹이 허용됨. 프로세스단에서 생각하면 fork 모델이 이와 같은 패턴을 따름7.4.2 이벤트 드리븐 스레딩 대부분의 스레드는 많은 시간을 그저 대기 중임. 방법 모든 입출력을 비동기식으로 처리하고 다중 입출력을 사용해서 서버 내 제어 흐름을 관리한다. 입출력 요청이 반환되면 이벤트 루프는 해당 콜백을 대기 중인 스레드로 넘김. 7.5 동시성, 병렬성, 경쟁 상태 동시성 둘 이상의 스레드가 특정 시간에 함께 실행되는 것을 의미한다. 병렬성 둘 이상의 스레드가 동시에 실행되는 것을 의미 동시성은 병렬성 없이 이루어질 수도 있음. 단일 프로세서를 가진 시스템에서의 멀티태스킹 병렬성은 다중 프로세서를 필요로 하는 동시성의 특수한 예7.5.1 경쟁 상태 (Race Condition) 스레드는 순차적으로 실행되지 않고 실행이 겹치기도 하므로 각 스레드의 실행 순서를 예측할 수 없다. 스레드가 서로 리소스를 공유한다면 문제가 된다. 경쟁 상태 공유 리소스에 동기화되지 않은 둘 이상의 스레드가 접근하려 프로그램의 오동작을 유발하는 상황을 뜻함. 크리티컬 섹션(Critical Section) 경쟁상태가 발생할 수 있기 때문에 반드시 동기화가 되어야 하는 영역 경쟁 상태의 실제 사례 은행 입출금 예제 X++ 의 예제 동시에 진행하면 잘못된 결과를 초래할 수 있음. 7.6 동기화 경쟁 상태를 예방하려면 이 Critical Section의 접근을 Mutual Exclusion (상호 배제) 하는 방식으로 접근을 동기화해야 함. Atomic 다른 연산(혹은 연산의 집합)에 끼어들 여지가 없다는 것 7.6.1 뮤텍스 크리티컬 섹션을 원자적으로 만들기 위한 가장 평범한 기법은 크리티컬 섹션 안에서 상호 배제를 구현해서 원자적으로 만들어주는 Lock이다. 락이 있어서 상호 배제를 구현할 수 있으며 Pthread 및 여러 곳에서 뮤텍스 라고 불린다. Mutex. == Binary Semaphore 락은 동시성이라는 스레딩의 장점을 포기하기 때문에 크리티컬 섹션은 가능한 한 최소한으로 잡는 편이 좋다. Notice 락은 코드가 아니라 데이터에 걸어야 한다. 7.6.2 데드락 스레드를 사용하는 이유는 동시성 떄문. → 동시성이 경쟁 상태 유발 → 뮤텍스 사용 → 데드락 유발 데드락 두 스레드가 서로 상대방이 끝나기를 기다리고 있어서 결국엔 둘 다 끝나지 못하는 상태를 의미. 데드락 피하기7.7 Pthread 리눅스 커널의 스레딩 지원은 clone() 시스템 콜 같은 원시적인 수준뿐임. 하지만 사용자 영역에서 스레딩 라이브러리를 많이 제공한다.7.7.1 리눅스 스레딩 구현7.7.2 Pthread API스레드 관리 스레드 생성, 종료, 조인, 디태치 함수동기화 뮤텍스와 조건 변수, 배리어를 포함하는 스레드 동기화 함수7.7.3 Pthread 링크하기 glic에서 Pthread를 제공하지만 libpthread 라이브러리는 분리되어 있으므로 링크해줄 필요가 있다.gcc -Wall -Werror -pthread beard.c -o beard7.7.4 스레드 생성하기 Pthread 는 새로운 스레드를 생성하는 함수인 pthread_create()를 제공한다.#include &amp;lt;pthread.h&amp;gt;int pthread_create (pthread_t *thread, const pthread_attr_t *attr, void *(*start_routine) (void *), void *arg); 호출이 성공하면 새로운 스레드가 생성되고 start_routine 인자로 명시한 함수에 arg로 명시한 인자를 넘겨서 실행을 시작한다. pthread_t 포인터인 thread가 NULL이 아니라면 여기에 새로 만든 스레드를 나타내기 위해 사용하는 스레드ID를 저장한다. pthread_attr_t 포인터인 attr에는 새로 생성된 스레드의 기본 속성을 변경하기 위한 값을 넘긴다. attr에 NULL을 넘기면 기본 속성 스레드 속성은 스택 크기, 스케줄링 인자, 최초 디태치 상태 등의 특성 결정 start_routine 은 다음과 같은 형태를 갖춰야함 void * start_thread (void *arg); fork와 유사하게 새로 생성된 스레드는 부모 스레드로부터 대부분의 속성과 기능 그리고 상태를 상속받음. 부모 프로세스 리소스의 복사본을 가지는 fork와는 다르게 스레드는 부모 스레드의 리소스를 공유한다. 프로세스의 주소 공간. 시그널 핸들러와 열린 파일을 공유 gcc 로 컴파일 할 때 -pthread 플래그를 넘겨야함. 예제 pthread_t tread;int ret;ret = pthread_create (&amp;amp;thread, NULL, start_routine, NULL);if (!ret) { errno = ret; perror(&quot;pthread_create&quot;); return -1;}/* a new thread is created and running start_routine concurrently ... */ 7.7.5 스레드 ID 스레드 ID (TID) 는 프로세스 ID(PID)와 유사하다. PID를 리눅스 커널에서 할당한다면 TID는 Pthread 라이브러리에서 할당한다. #include &amp;lt;pthread.h&amp;gt;pthread_t pthread_self (void); pthread_create() 호출이 성공하면 thread 인자에 저장된다. pthread_self() 함수를 이용해서 자신의 TID를 얻어올 수 있음.스레드 ID 비교하기 Pthread 표준은 pthread_t 가 산술 타입이기를 강제하지 않으므로 == 연산자가 동작하리라 보장할 수 없음. 따라서 비교 하려면 특수한 인터페이스를 사용해야한다.#include &amp;lt;pthread.h&amp;gt;int pthread_equal (pthread_t t1, pthread_t t2); 예제 int ret;ret = pthread_equal(thing1, thing2);if (ret != 0) printf(&quot;The TIDs are equal!\\n&quot;);else printf(&quot;The TIDs are unequal!\\n&quot;); 7.7.6 스레드 종료하기 스레드 종료는 한 스레드가 종료되도 그 프로세스 내의 다른 스레드는 계속 실행된다는 점만 제외하면 프로세스 종료와 비슷함. 스레드가 종료되는 상황 start_routine 함수가 반환한 경우. 이는 main() 함수가 끝까지 실행된 상황과 비슷 pthread_exit() 함수를 호출한 경우 pthread_cancel() 함수를 통해 다른 스레드에서 중지시킨 경우 위는 관계된 스레드 하나만 종료됨. 아래는 프로세스 내 모든 스레드가 종료되어 그 프로세스도 종료되는 경우 프로세스의 main() 함수가 반환 프로세스가 exit() 호출 프로세스가 execve() 호출로 새로운 바이너리를 실행한 경우 스스로 종료하기 start_routine 을 끝까지 실행하면 스레드 스스로 종료함. start_routine 함수가 몇 번의 호출을 타고 들어간 콜 스택 깊숙한 곳에서 스레드를 종료시켜야 할 때는 아래의 시스템 콜 사용#include &amp;lt;pthread.h&amp;gt;void pthread_exit (void *retval);다른 스레드 종료하기 pthread_cancel 함수를 통해 다른 스레드를 취소시켜 종료할 수 있음.#include &amp;lt;pthread.h&amp;gt;int pthread_cancel (pthread_t thread); 실제 종료는 비동기적으로 일어남. 스레드가 취소될지, 또 언제 실행될지는 조금 복잡함. 스레드의 취소 상태는 가능일 수도 있고, 불가능일 수도 있다. 기본값은 취소 가능임. 스레드의 취소 상태는 pthread_setcancelstate() 를 통해 변경할 수 있음 #include &amp;lt;pthread.h&amp;gt;int pthread_setcancelstate (int state, int *oldstate); 스레드의 취소상태가 state 값으로 설정되고 이정 상태는 oldstate에 저장됨. state PTHREAD_CANCEL_ENABLE PTHREAD_CANCEL_DISABLE 스레드의 취소 타입 비동기 취소 요청이 들어온 이후에 언제든지 스레드를 종료시킬 수 있음 특정한 상황에서만 유용함 프로세스를 정의되지 않은 상태로 남겨두기 때문. 스레드가 공유 리소스를 사용하지 않고 시그널 세이프한 함수를 호출한 경우에만 사용해야함 유예 기본 값 취소 타입 변경 함수 #include &amp;lt;pthread.h&amp;gt;int pthread_setcanceltype (int type, int *oldtype); type PTHREAD_CANCEL_ASYNCHRONOUS PTHREAD_CANCEL_DEFERRED 종료 예제 int unused;int ret;ret = pthread_setcancelstate (PTHREAD_CANCEL_ENABLE, &amp;amp;unused);if (ret) { errno = ret; perror (&quot;pthread_setcancelstate&quot;); return -1;}ret = pthread_setcanceltype (PTHREAD_CANCEL_DEFERRED, &amp;amp;unused);if (ret) { errno = ret; perror (&quot;pthread_setcanceltype&quot;); return -1;} int ret;/* `thread&#39; is the thread ID of the to-terminate thread */ret = pthread_cancel (thread);if (ret) { errno = ret; perror (&quot;pthread_cancel&quot;); return -1;} 취소 상태를 가능으로 바꾸고, 취소 타입은 취소 유예로 설정하는 예제 7.7.7 스레드 조인과 디태치 스레드 생성과 종료는 쉽지만, wait() 함수와 마찬가지로 모든 스레드의 종료를 동기화 해야한다. 이를 스레드 조인이라고 함.스레드 조인 조인은 스레드가 다른 스레드가 종료될 때까지 블록되도록 한다.#include &amp;lt;pthread.h&amp;gt;int pthread_join (pthread_t thread, void **retval); 호출한 스레드는 thread 로 명시한 스레드가 종료될 때까지 블록한다. 해당 스레드가 이미 종료되었다면 pthread_join() 은 즉시 반환된다. 스레드가 종료되면 호출한 스레드가 깨어나고 retval이 NULL이 아니라면 그 값은 종료된 pthread_exit()에 넘긴 값이거나 start_routine 에서 반환한 값이다. 스레드 조인은 다른 스레드의 라이프 사이클에 맞춰 스레드의 실행을 동기화하는 것이다. Pthread 의 모든 스레드는 서로 동등하므로 어떤 스레도도 조인 가능하다. 하나의 스레드는 여러 스레드를 조인할 수 있지만, 하나의 스레드만 다른 스레드에 조인을 시도해야 한다. 예제 int ret;/* join with `thread&#39; and we don&#39;t care about its return value */ret = pthread_join (thread, NULL);if (ret) { errno = ret; perror (&quot;pthread_join&quot;); return -1;} 스레드 디태치 부모 프로세스에서 wait() 을 호출하기전까지 자식 프로세스가 시스템 리소스를 잡아먹는 것과 마찬가지로 스레드도 조인이 되기 전까지 시스템 리소스를 잡아먹고 있으므로 조인을 할 생각이 없는 스레드는 디태치해두어야 한다.#include &amp;lt;pthread.h&amp;gt;int pthread_detach (pthread_t thread);7.7.9 Pthread 뮤텍스뮤텍스 초기화하기 뮤텍스는 pthread_mutex_t 객체로 표현된다.pthread_mutex_t mutex = PTHREAD_MUTEX_INITIALIZER;뮤텍스 락 걸기#include &amp;lt;pthread.h&amp;gt;int pthread_mutex_lock (pthread_mutex_t *mutex); 호출이 성공하면 mutex로 지정한 뮤텍스의 사용이 가능해질 때까지 호출한 스레드를 블록한다. 해당 뮤텍스가 사용 가능한 상태가 되면 호출한 스레드가 깨어나고, 이 함수는 0을 반환한다.뮤텍스 해제하기#include &amp;lt;pthread.h&amp;gt;int pthread_mutex_unlock (pthread_mutex_t *mutex);" }, { "title": "[미라클모닝][MKYU] 514 프로젝트 7일차 ", "url": "/posts/miracle07/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-07 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 7일차4/7 당신의 빅팬은 누구입니까?내가 나의 빅팬이 되어야 다른 사람을 사랑할 수 있다.탄생! 나 스스로 평생 내 팬이 되겠다는 약속→ 자녀 교육의 베이스→ 탄생! 너 스스로 평생 너의 팬이 되겠다는 약속을 할 수 있게 해라내가 나를 응원하는 시스템 구축경제적 지원, 정서적 지원, 환경적 지원→ 이런 지원적인 부분들은 내가 구축해야한다.내가 팬이 되면 벌어지는 일!덕질을 한다. 계속 나에 대해 물어보고 질문을 던진다.사랑해요, 힘내요! 걱정마 남은 나의 모든 일을 공감하지 못한다. 남이 하기 전에 내가 먼저 해야한다. 자신있게 이런 말을 하기 위해서는 내가 진정한 나의 빅 팬이 되어야 할 수 있음. 후원한다.영어공부#10Well, It made my dad proudNow, Don’t you feel better remy?You’ve helped the noble cause.Noble? We are thieves dad.And what we’re stealing is, Let’s face it, garbage.It isn’t stealing if no one wants itIf no one wants it, why are we stealing it!#11Let’s just say, we have a different points of view.This much I knew.If we are what you eat, then I only want to eat the good stuff.But to my dad,Food is fuel, you get picky about what you put it the tank, your engine is gonna die.Now shut up and eat your garbage." }, { "title": "[미라클모닝][MKYU] 514 프로젝트 5일차 ", "url": "/posts/miracle05/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-05 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 5일차4/5 디지털 에셋을 만드는 법디지털에셋 돈을 쓰면서 배워야함.디지털 세상에서 나의 자산을 거래처음 번 돈의 의미. 1,000배로 돌아온다. 디지털 에셋은 린하게 출시하고 유저의 피드백으로 커가는 것이다.무료와 유료의 경험차이 내가 돈 내고 배운 것이 유료 경험.영어공부#6And second, I have a highly developed sense of testing smellFlour, eggs, sugar, vanila bean, oh small twist of lemon.Oh you can smell all that?You have a gift#7This is emilie. My brother.He is easiliy impressed.So you can smell ingredients. So what!This is my dad. He is never impressed.He also happens to be the leader of our clan.So what’s wrong with having a highly developed senses?#8Don’t eat that!What’s going on here?Turns out that funny smell was rat poison.Suddenly dad didn’t think my talent was useless.I was feeling pretty good about my gift.#9Until dad gave me a job.That’s right. Poison checker.Close to godliness. Which means clean.You know cleanliness close to godliness.Never mind. Move on" }, { "title": "[미라클모닝][MKYU] 514 프로젝트 4일차 ", "url": "/posts/miracle04/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-04 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 4일차온 동네가 알 때 까지 꾸준해라 경제가 되려면 일정 규모가 되어야함.내 꾸준함이 남들에게 도움이 되게 해라" }, { "title": "[미라클모닝][MKYU] 514 프로젝트 2일차 ", "url": "/posts/miracle02/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-02 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 2일차4/2 세상을 사는 3가지 지혜의 말사랑해 시작할 때 많이 표현하라. 사랑을 표현하면 인간관계의 시작하기 쉬움미안해 유지할 때고마워 정리할 때영어공부#3But not everyone celebrates its success.Amusing title anyone can cook, what’s even more amusing is that Gusteu actually seems to believe it.I, on the other hand, take cooking seriously, No I don’t think anyone can do it.#5This is me.I think it aparent I need to rethink my life little bit.What’s my problem, firtst of all, I’m a rat.Which means life is hard." }, { "title": "[미라클모닝][MKYU] 514 프로젝트 1일차 ", "url": "/posts/miracle01/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-04-01 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 1일차4/1 누군가를 응원하는 법언제나 한결같이 그의 편이 되라. 내가 결혼을 안해봐서 모르지만,,, 왜 부부는 서로에게 한결같이 상대의 편이 될 수 없을까? 나는 될 수 있다고 생각한다. 부모라는 것은 한결같이 자식의 편이 되어주는 것대화가 통할 정도로 공부해라 사랑하는 사람을 응원하려면 말이 통해야한다. 말이 통하기 위해서는 공부해야한다. 자녀와 어느순간 말이 안통할 수가 있다. 공부를 해야한다.솔직하게 털어 놓고 친해져라영어공부 진행#1Althogh each of the world’s country will like to dispute this fact, we french know the truth.The best food in the world is made in France.The best food in France is made in Paris.And the Best food, some say, is made by Chef August#2Gusto’s restaurant is the Toast of Paris, booked 5 months in advance.And his dazzling ascent to the top of fine french cuisine has made his competitors envious.He is the youngest chef ever to acheive a five star rating.Chef Gusteau’s cook book ‘Any one can cook’ climbed to the top of best seller list." }, { "title": "[미라클모닝][MKYU] 514 서약서 ", "url": "/posts/miracle00/", "categories": "Miracle Morning, GOOD JJACK World", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-31 00:00:00 +0900", "snippet": "[미라클모닝][mkyu] 514 서약서서약서" }, { "title": "[Linux System Programming] Ch06 고급 프로세스 관리 ", "url": "/posts/linux_ch06/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-30 00:00:00 +0900", "snippet": "[Ch06 고급 프로세스 관리] 5장에서는 프로세스가 무엇인지, 생성, 제어, 종료에 관한 시스템 콜과 관련된 시스템에 대해 알아봤음. 이번 장에서는 리눅스 프로세스 스케줄러와 스케줄링 알고리즘을 알아본다.6.1 프로세스 스케줄링 프로세스 스케줄러는 커널의 서브 시스템으로써 유한한 리소스인 프로세서의 시간을 시스템 내의 프로세스에 나눠준다. 즉, 다음에 실행할 프로세스를 선택하기 위한 커널의 구성요소임. 실행 가능한 프로세스 블록되지 않은 프로세스 블록된 프로세스 자고 있거나, 커널로부터 입출력을 기다리고 있는 프로세스 프로세서의 개수보다 실행 가능한 프로세스가 더 많이 존재할 때는 스케줄러가 필요함 선점형 멀티태스킹 리눅스 다른 프로세스를 위해 실행 중인 프로세스를 멈추는 행위를 선점이라고 함. 스케줄러가 선점하기 전까지 프로세스에 허락된 실행 시간을 프로세스 타임 슬라이스라고 한다. 비선점형 멀티태스킹 프로세스가 스스로 실행을 멈추기 전까지 계속 실행함. 자발적으로 실행을 잠시 쉬는 것을 양보라고 함 너무 오래 실행되는 그런 단점때문에 최신 OS는 거의 선점형 을 사용한다. 6.1.1 타임 슬라이스 스케줄러가 각 프로세스에 할당하는 타임 슬라이스는 시스템 전반의 동작 방식과 성능에 관한 중요한 변수이다. 타임 슬라이스가 너무 크다면 프로세스는 다음 실행 시간까지 오래 기다려야 하며, 동시 수행 능력을 떨어트림. 반대로 너무 작으면 잦은 프로세스 전환으로 인해 일시적인 지역성과 같은 장점을 잃게 됨. 목적에 따라 크기를 다르게한다. 시스템이 처리할 수 있는 용량을 극대화하여 성능 향상을 하려고 큰 슬라이스를 사용함. 혹은 빠른 응답속도를 확보하기 위해서 작은 슬라이스를 사용함. → 이상적인 사이즈를 결정하기가 어려움! 6.1.2 입출력 위주 프로세스와 CPU 위주 프로세스 사용 가능한 타임 슬라이스를 끊임없이 계속 사용하는 프로세스를 CPU 위주 프로세스라고 한다. 스케줄러에서 허락하는 시간을 모두 사용함 예시 무한루프 과학계산, 수학연산, 이미지 처리 등 // 100% processor-boundwhile (1) ; 실행 시간보다 리소스를 사용하기 위해 기다리는 시간을 더 많이 사용하는 프로세스를 IO 위주 프로세스 라고 한다. 네트워크 입출력, 파일 입출력 기다림. cp, mv 같은 파일 유틸리티 , User GUI 등. 각각의 특성에 맞게 스케줄러를 선택해야함. CPU 위주 일시적인 지역성을 통해 캐시 적중률을 최대화하고 작업을 빨리 끝내기 위해 큰 타임 슬라이스를 사용 IO 위주 입출력 요청을 보내기까지 아주 짧은 시간이면 충분하고 대부분의 시간을 커널 리소스를 얻기 위해 블록되기 때문에 큰 슬라이스가 필요없음. 현실에서는 CPU와 입출력을 같이 사용함6.1.3 선점형 스케줄링 전통적인 유닉스 프로세스 스케줄링에서 모든 실행 가능한 프로세스는 타임 슬라이스를 할당받음 프로세스가 주어진 타임 슬라이스를 다 소진하면 커널은 그 프로세스를 잠시 멈추고 다른 프로세스를 실행. → 모든 프로세스는 자신보다 우선순위가 높은 프로세스가 있을지라도 그 프로세스가 타임 슬라이스를 모두 소진하거나 블록된다면 결국 실행될 기회를 얻게됨. → 모든 프로세스는 반드시 계속 진행되어야 한다는 규칙을 만들어냄6.2 CFS 스케줄러 Completely Fair Scheduler CFS 전의 전통 유닉스 시스템들은 우선순위와 타임 슬라이스 라는 변수를 사용해서 스케줄링을 구현했음 CFS는 타임 슬라이스 대신 CPU 시간의 일부를 각 프로세스에 할당한다. N개의 프로세스에 각각 1/N 만큼의 CPU 시간을 할당한다. 그리고 이를 각 프로세스의 nice 값에 따라 가중치를 준다. nice 값이 기본값인 0을 그대로 사용하는 프로세스의 가중치는 1이며, 따라서 할당받는 CPU 시간에는 변화가 없다. 기본값보다 적은 nice 값을 사용하는 (우선순위가 높은) 프로세스는 CPU 시간을 더 많이 할당 받음 각 프로세스가 수행될 시간을 결정하기 위해 CFS 스케줄러는 프로세스 별로 가중치가 적용된 값을 한정된 시간(Target Latency) 로 나눈다. 시스템의 스케줄링 레이턴시를 나타냄. 예시 Target Latency : 20 밀리초 5개의 프로세스 → 각 4밀리초 But, 프로세스가 많다면 Context Switching Overhead 때문에 일시적인 지역성의 효과는 줄어들고 시스템 전체 처리 성능은 매우 지장받음 →이런 상황을 피하기 위해 CFS는 최소단위 (Minimum Granularity) 를 사용함 최소 단위 프로세스가 실행되는 최저 시간 단위 모든 프로세스는 할당된 CPU 시간과 관계없이 최소 단위만큼은 실행된다는 의미 근데 이를 사용하면 공정성이 무너짐! → 납득할 수 있는 만큼의 평범한 상황에서는 최소 단위가 적용되지 않고, Target latency 만으로 공정성을 유지할 수 있다. 6.3 프로세서 양보하기 리눅스는 선점형 멀티태스킹 운영체제지만, 프로세스가 명시적으로 실행을 양보해서 스케줄러가 새로운 프로세스를 실행하도록 하는 시스템 콜을 제공한다.#include &amp;lt;sched.h&amp;gt;int sched_yield (void); sched_yield() 를 호출하면 현재 실행 중인 프로세스를 잠시 멈춘 다음 스케줄러가 다음에 실행할 새로운 프로세스를 선택하도록 한다. 다른 실행 가능한 프로세스가 없으면 sched_yield() 를 호출한 프로세스의 실행이 즉시 재개됨. → 이런 불확실성과 일반적으로 더 나은 대안에 대한 믿음 때문에 이 시스템 콜은 잘 안씀. 6.3.1 적당한 사용법 리눅스 커널은 가장 효율적이고 최적의 스케줄링 결정을 내리기에 부족함이 없음 → 일개 어플리케이션이 무엇을 언제 선점해야 할지 결정하는 것보다 커널이 훨씬 더 나은 결정을 할 수 있음 !! 쓰는 경우가 있긴 하지만 잘 안씀.6.4 프로세스 우선 순위 nice 값은 프로세스가 be nice 하기를 바라면서 만들어짐. -20 ~ 19 의 값을 가지고 있다. 값이 높을 수록 Nice 하기때문에 우선순위가 낮음.6.4.1 nice()#include &amp;lt;unistd.h&amp;gt;int nice (int inc); 호출이 성공하면 프로세스의 nice 값을 inc로 지정하고 새롭게 업데이트 된 값을 리턴한다. 오직 CAP_SYS_NICE 값을 가진 프로세스만 negative 값을 받을 수 있다. 보통 root 만 이럼. 결과적으로 non root 프로세스는 오직 우선순위를 낮출 수만 있음. 예시 int ret;errno = 0;ret = nice (10); /* increase our nice by 10 */if (ret == −1 &amp;amp;&amp;amp; errno != 0) perror (&quot;nice&quot;);else printf (&quot;nice value is now %d\\n&quot;, ret); inc 값에 0을 넣으면 프로세스의 현재 nice 값을 알 수 있다.6.4.2 getpriority() 와 setpriority()#include &amp;lt;sys/time.h&amp;gt;#include &amp;lt;sys/resource.h&amp;gt;int getpriority (int which, int who);int setpriority (int which, int who, int prio); which PRIO_PROCESS PRIO_PGRP PRIO_USER 각각 who 값이 pid, pgid, uid를 판단한다. 만약 who 가 0이면 현재 pid,pgid, uid를 받아옴. getpriority() 는 지정된 프로세스들 중에서 가장 높은 우선순위를 리턴함. setpriority() 는 prio 값으로 특정된 프로세스들 전부를 지정함. 예시 현재 프로세스의 우선순위를 출력한다.```cint ret; ret = getpriority (PRIO_PROCESS, 0);printf (“nice value is %d\\n”, ret); - 현재 프로세스 그룹의 전체 프로세스의 우선순위를 10으로 지정한다.```cint ret;ret = setpriority (PRIO_PGRP, 0, 10);if (ret == −1) perror (&quot;setpriority&quot;); 6.4.3 I/O 우선순위 기본적으로 I/O 스케줄러는 프로세스의 nice 값을 기준으로 I/O 우선순위를 결정한다.int ioprio_get (int which, int who)int ioprio_set (int which, int who, int ioprio) glibc 는 지원하지 않음.6.5 프로세서 친화 리눅스는 싱글 시스템에서 멀티프로세서를 지원함. 멀티프로세싱 머신에서 프로세스 스케줄러는 어떤 프로세스가 각 CPU에서 돌 것인지 판단해야만 한다. 프로세스가 놀 동안 CPU가 놀고 있으면 안됨. 프로세스가 한 CPU에 스케줄링 되고 다음에 다시 스케줄링 될 때 같은 CPU에 되어야함. migrating 부분에서 이점이 있음 가장 큰 비용은 migration 할 때 캐쉬의 영향이다. 프로세스 스케줄러는 프로세스를 특정 CPU에 오랫동안 두어야함.6.5.1 sched_getaffinity() 와 sched_setaffinity()#define _GNU_SOURCE#include &amp;lt;sched.h&amp;gt;typedef struct cpu_set_t;size_t CPU_SETSIZE;void CPU_SET (unsigned long cpu, cpu_set_t *set);void CPU_CLR (unsigned long cpu, cpu_set_t *set);int CPU_ISSET (unsigned long cpu, cpu_set_t *set);void CPU_ZERO (cpu_set_t *set);int sched_setaffinity (pid_t pid, size_t setsize, const cpu_set_t *set);int sched_getaffinity (pid_t pid, size_t setsize, cpu_set_t *set); sched_getaffinity() 는 프로세스 pid의 CPU 친밀도를 얻고 cpu_set_t 타입의 값에다가 저장한다. 만약 pid가 0이면 호출은 현재 프로세스의 affinity를 얻어옴. setsize 는 cpu_set_t 타입인데 타입의 사이즈 변화에 사용된다. 예제 cpu_set_t set;int ret, i;CPU_ZERO (&amp;amp;set);ret = sched_getaffinity (0, sizeof (cpu_set_t), &amp;amp;set);if (ret == −1) perror (&quot;sched_getaffinity&quot;);for (i = 0; i &amp;lt; CPU_SETSIZE; i++) { int cpu; cpu = CPU_ISSET (i, &amp;amp;set); printf (&quot;cpu=%i is %s\\n&quot;, i, cpu ? &quot;set&quot; : &quot;unset&quot;);} 현재 프로세스의 affinity 를 출력한다. cpu=0 is setcpu=1 is setcpu=2 is unsetcpu=3 is unset...cpu=1023 is unset 두개의 프로세스가 돌아가는 상태일 때의 출력값 만약 0,1 중에서 0만 쓰고 싶다면,,```ccpu_set_t set;int ret, i; CPU_ZERO (&amp;amp;set); /* clear all CPUs /CPU_SET (0, &amp;amp;set); / allow CPU #0 /CPU_CLR (1, &amp;amp;set); / disallow CPU #1 */ret = sched_setaffinity (0, sizeof (cpu_set_t), &amp;amp;set);if (ret == −1) perror (“sched_setaffinity”); for (i = 0; i &amp;lt; CPU_SETSIZE; i++) { int cpu; cpu = CPU_ISSET (i, &amp;amp;set); printf (&quot;cpu=%i is %s\\n&quot;, i, cpu ? &quot;set&quot; : &quot;unset&quot;); } ``` # 6.6 실시간 시스템 - 실시간 : Time Limit 이 존재함. - Low latency or 빠른 응답시간 : 빠르면 빠를수록 좋은 것 - 예제 - Task A는 특정 기능을 1초안에 수행하지 않으면 전체 시스템에 치명적인 위해를 가할 수 있음.- → Time limit : 1초.- → 1초라는 시간을 준수하면서 특정 기능을 수행하는 것을 `실시간`이라고 함 - Task B는 기능을 수행함에 있어 Time Limit 은 없다.- → 빠르면 좋긴하겠지만 느리다고 해서 치명적인 손상을 입히는 것은 아님. - 리눅스 기반의 Real-time OS - 리눅스 기반의 RTOS와 일반 리눅스 OS는 크게 차이가 없다.- 모든 OS는 preemptive 한 작업 수행을 보장함. - **일반 리눅스는 interrupt가 들어왔을 때 현재 수행 중인 시스템 콜을 끝낸 뒤 Context switching**이 일어나지만, **RT 커널 기반의 리눅스는 현재 작업 중인 프로세스의 시스템 콜 수행마저도 Interrupt를 걸어 작업 Switching에 대한 Latency를 최소화**한다 일반 프로세스 레벨에서는 nice/renice를, RT 프로세스에서는 chrt를 쓰면됨.6.6.1 Hard Real-Time VS Soft Real-Time System hard real-time system os의 데드라인을 무조건 따라야함. 데드라인을 넘어서는것은 제약적인 실패이고 큰 버그를 야기함. 예제 ABS 시스템, 총기 시스템, 의료 장비, 시그널 프로세싱.. soft real-time system 데드라인을 넘는 것을 크리티컬하다고 생각하지 않음. 예제 비디오 프로세싱 애플리케이션 데드라인이 넘어서 조금의 프레임이 깨져도 크게 영향이 없다. 6.6.2 Latency, Jitter and Deadlines Latency 잠재적 시간 지연 응답에 대한 실행이 나타나기 까지의 시간을 의미 지연이 os 데드라인보다 작거나 같다면 잘 동작하는 것이다. Jitter 응답간의 시차 지연 요소의 변동량. 즉, Latency 나 delay의 변화량 수준 연속적인 이벤트 간의 시간 편자는 레이턴시가 아니라 지터임. 6.6.3 Linux’s Real-Time Support 리눅스는 시스템 콜을 지원하여서 soft real-time 애플리케이션을 지원한다.6.6.4 Linux Scheduling Policies and Priorities 리눅스는 두가지 스케줄링 정책을 제공함. 로부터 매크로를 제공받는다. SCHED_FIFO, SCHED_RR, SCHED_OTHER 모든 프로세스들은 nice 값이랑 별개로 static priority 를 가지고 있음. 기본적으로 0이다. 실시간 프로세스에서는 1-99까지의 값을 가진다. 일반적으로 SCHED_OTHER 를 사용함 기본적으로 여러 프로세스 간 Time-sharing 방식(CFS Scheduler) 또는 우선순위 기반 스케줄링을 사용. FIFO, RR RT를 위한 FIFO 전략 FIFO는 timeslices가 없는 매우 심플한 실시간 전략이다. 더 높은 우선순위를 가진 프로세스가 없는 한 계속 진행한다. SCHED_FIFO 매크로를 사용함 정책에 타임슬라이스가 없기 때문에 비교적 간단하다. 프로세스가 가장 높은 우선순위에 있다면 항상 실행됨. 프로세스가 블록되거나 sched_yield() 가 실행되거나, 더 높은 우선순위의 프로세스가 들어오기 전까지 계속 실행됨. Q) 같은 우선순위의 프로세스들이 있다면?Rount-Robin 전략 FIFO 클래스와 동일하지만 같은 우선순위일 때 추가적인 전략들이 있다. SCHED_RR 매크로를 사용 스케줄러는 RR-classed 프로세스 각각에 타임 슬라이스를 배정한다. 프로세스가 자신의 타임슬라이스를 다 쓰면 스케줄러는 그 프로세스를 우선순위 리스트에 끝으로 보낸다. 주어진 우선순위에 프로세스가 하나밖에 없다면 RR은 FIFO 와 동일하다. RR 또한 같은 우선순위의 프로세스들 중에서 스케줄링을 계속 하기 때문에 FIFO와 거의 유사함.Normal 전략 SCHED_OTHER 은 기복적인 스케줄링 전략을 대표한다. (default non-real-time class) 모든 normal-classed 프로세스는 고정된 우선순위 값으로 0을 가지고 있다. 결과적으로 동작중인 FIFO or RR 프로세스들은 normal 프로세스를 점유할 것이다. 이 스케줄러는 nice 값을 사용함.Batch 스케줄링 전략 SCHED_BATCH 는 Batch or idle 스케줄링 전략이다. real-time과 어느정도는 반대된다. 다른 프로세스들이 타임슬라이스를 다 썼더라도, 시스템에 동작 가능한 프로세스가 있다면 실행하지 않는다.Linux 스케줄링 전략 설정하기#include &amp;lt;sched.h&amp;gt;struct sched_param { /* ... */ int sched_priority; /* ... */};int sched_getscheduler (pid_t pid);int sched_setscheduler (pid_t pid, int policy, const struct sched_param *sp); sched_getscheduler() 와 sched_setscheduler() 를 사용해서 리눅스 스케줄링 값을 조작할 수 있다. sched_getscheduler() 호출이 성공하면 pid 프로세스의 스케줄링 전략을 리턴한다. pid가 0이면 호출을 실행한 프로세스의 스케줄링 정책이 반환됨. 예시 int policy;/* get our scheduling policy */policy = sched_getscheduler (0);switch (policy) {case SCHED_OTHER: printf (&quot;Policy is normal\\n&quot;); break;case SCHED_RR: printf (&quot;Policy is round-robin\\n&quot;); break;case SCHED_FIFO: printf (&quot;Policy is first-in, first-out\\n&quot;); break;case -1: perror (&quot;sched_getscheduler&quot;); break;default: fprintf (stderr, &quot;Unknown policy!\\n&quot;);} sched_setscheduler() 를 호출하면 pid 에다가 policy를 넣어준다. sp 정책과 관련된 다른 파라미터들을 넣어주는 곳 shced_param 구조 안의 유효한 값들은 policy에 따라 다름. SCHED_RR과 SCHED_FIFO 는 sched_priority가 필요하고 SCHED_OTHER는 필요없음. 예시 이 예시는 RR 정책으로 static priority를 1로 수정한다.```cstruct sched_param sp = { .sched_priority = 1 };int ret; ret = sched_setscheduler (0, SCHED_RR, &amp;amp;sp);if (ret == −1) { perror (“sched_setscheduler”); return 1;}``` 스케줄링 정책을 지정할 때 SCHED_OTHER를 제외하고는 CAP_SYS_NICE 설정이 필요하다.6.6.5 스케줄링 파라미터 설정하기 POSIX는 스케줄링 정책과 관련된 파라미터들을 설정하고 가져올 수 있게 하기 위해서 sched_getparam() 과 sched_setparam() 을 지원한다.#include &amp;lt;sched.h&amp;gt;struct sched_param { /* ... */ int sched_priority; /* ... */};int sched_getparam (pid_t pid, struct sched_param *sp);int sched_setparam (pid_t pid, const struct sched_param *sp); sched_getscheduler() 호출은 오직 스케줄링 정책만 리턴하지만 sched_getparam() 은 sp에다가 pid 프로세스와 연관된 스케줄링 파라미터들을 전달한다. 예시 struct sched_param sp;int ret;ret = sched_getparam (0, &amp;amp;sp);if (ret == −1) { perror (&quot;sched_getparam&quot;); return 1;}printf (&quot;Our priority is %d\\n&quot;, sp.sched_priority); 만약 pid가 0이면 호출한 프로세스의 파라미터를 넘김. sched_setscheduler() 또한 스케줄링 파라미터를 저장하긴 하지만, sched_setparam() 은 나중에 파라미터를 변경할 때 유용하다. 예시 struct sched_param sp;int ret;sp.sched_priority = 1;ret = sched_setparam (0, &amp;amp;sp);if (ret == −1) { perror (&quot;sched_setparam&quot;); return 1;} 유효한 우선순위 값 범위 결정하기 리눅스에서는 1-99 값의 범위로 RT 스케줄링 정책을 지원한다. 리눅스는 현재 유효한 우선순위 값을 알기 위해서 시스템 콜을 지원한다.#include &amp;lt;sched.h&amp;gt;int sched_get_priority_min (int policy);int sched_get_priority_max (int policy); 각각 유효한 우선순위 값의 최소값과 최대값을 리턴함. policy 스케줄링 정책을 넣어줌 예시 int min, max;min = sched_get_priority_min (SCHED_RR);if (min == −1) { perror (&quot;sched_get_priority_min&quot;); return 1;}max = sched_get_priority_max (SCHED_RR);if (max == −1) { perror (&quot;sched_get_priority_max&quot;); return 1;}printf (&quot;SCHED_RR priority range is %d - %d\\n&quot;, min, max); 6.6.6 sched_rr_get_interval() SCHED_RR은 time slice를 사용한다는 것을 제외하고 SCHED_FIFO와 동일하게 작동한다. 만약 SCHED_RR 프로세스가 타임슬라이스를 다 썼다면, 스케줄러는 현재 우선순위의 동작 리스트 중 가장 마지막으로 간다. 이런 방법에서는 같은 우선순위의 모든 SCHED_RR 프로세스들은 Rount robin 으로 순회한다. 높은 우선순위의 프로세스들 (SCHED_FIFO of the same or higher priority) 은 SCHED_RR 프로세스의 타임슬라이스가 더 남았건 상관안하고 점유한다. POSIX 는 주어진 프로세스의 timeslice의 길이를 얻을 수 있는 인터페이스를 제공한다.#include &amp;lt;sched.h&amp;gt;struct timespec { time_t tv_sec; /* seconds */ long tv_nsec; /* nanoseconds */};int sched_rr_get_interval (pid_t pid, struct timespec *tp); 호출이 성공하면 timespec 구조의 tp에다가 pid에 할당된 타임슬라이스의 길이를 저장한다. POSIX에 따르면 이 함수는 SCHED_RR에만 사용할 수 있지만, 리눅스에서는 어떤 프로세스든 적용할 수 있다. 예시 struct timespec tp;int ret;/* get the current task&#39;s timeslice length */ret = sched_rr_get_interval (0, &amp;amp;tp);if (ret == −1) { perror (&quot;sched_rr_get_interval&quot;); return 1;}/* convert the seconds and nanoseconds to milliseconds */printf (&quot;Our time quantum is %.2lf milliseconds\\n&quot;, (tp.tv_sec * 1000.0f) + (tp.tv_nsec / 1000000.0f)); 만약 프로세스가 FIFO 로 돌고있다면 tv_sec과 tv_nsec는 둘 다 0이다. 6.6.7 Real-time 프로세스의 주의점6.6.8 Determinism (결정론) RT 프로세스는 결정론에 크게 좌지우지 한다. RT 컴퓨팅에서 행동은 결정론적인데, 만약 같은 인풋을 받았을 때 항상 같은 양의 시간에 같은 결과를 만들어야하기 때문. 최신 컴퓨터는 결정론적이지 않은데, 여러 계층에 걸친 캐시와 여러 개의 프로세스, 페이징, 스와핑, 그리고 멀티태스킹은 명령이 얼마나 걸릴지 예측할 수 없음 실시간 애플리케이션은 예측할 수 없는 부분과 최악의 지연을 제한하려고 시도한다.선행 폴트 데이터와 메모리 락 종종 선행 폴트를 일으켜 스왑된 데이터를 메모리에 올린 다음, 주소 공간 내 모든 페이지를 실제 물리 메모리에 ‘락을 걸거나’, ‘고정 배선’한다. 메모리 락을 걸고 나면 커널은 절대로 이 페이지를 디스크로 스왑하지 않는다.CPU 친화도와 실시간 프로세스 각 실시간 프로세스를 위해 프로세서 하나를 예약해두고 나머지 프로세스는 남은 프로세서상에서 시분할 방식으로 동작하게 한다. 구현 방법: init 프로그램을 수정 - CPU_CLR (1, &amp;amp;set): CPU #1을 금지한다. 실행 가능한 프로세서 집합은 부모 프로세스로부터 상속받게 된다. 실시간 프로세스가 CPU #1에서만 실행되도록 affinity를 준다. - CPU_SET (1, %set) " }, { "title": "[Linux System Programming] Ch05 프로세스 관리 ", "url": "/posts/linux_ch05/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-24 00:00:00 +0900", "snippet": "[Ch05 프로세스 관리] 유닉스는 새로운 바이너리 이미지를 메모리에 적재하는 과정에서 새로운 프로세스를 생성하는 부분을 분리했다.5.1 프로그램, 프로세스, 스레드 바이너리는 디스크 같은 저장 장치에 기록되어 있는 컴파일된 실행할 수 있는 코드를 말한다. 흔히 프로그램을 지칭하기도 함. 때로는 애플리케이션을 뜻하기도 함. /bin/ls, /usr/bin/X11 모두 바이너리 프로세스는 실행 중인 프로그램을 뜻함. 프로세스는 메모리에 적재된 바이너리 이미지와 가상화된 메모리의 인스턴스, 열린 파일 같은 커널 리소스, 관련된 사용자 정보와 같은 보안 정보와 하나 이상의 스레드를 포함하고 있다. 스레드는 프로세스 내 실행 단위. 각각의 스레드는 가상화된 프로세서를 가지고 있음 프로세서에는 스택, 레지스터, 명령어 포인터 같은 프로세서의 상태가 포함되어 있다. Q) 리눅스에서 스레드를 어떻게 구현했을까? (프로세스와 거의 유사하다고함…) 스레드든 프로세스든 다 Task로 구분하고 스케줄링함. 또? 구조는? 싱글 스레드 프로세스는 프로세스가 곧 스레드가 된다. 5.2 프로세스 ID 모든 프로세스는 프로세스 ID(pid)라고 하는 유일한 식별자로 구분됨. pid는 특정 시점에서 유일한 값임을 보장한다. 커널이 같은 프로세스 식별자를 다른 프로세스에 다시 할당하지 않으리라 가정한다. 동작 중인 다른 프로세스가 없을 때 커널이 실행하는 idle 프로세스는 Pid가 0이다. 부팅이 끝나면 커널이 실행하는 최초 프로세스인 init 의 pid는 1이다. 리눅스 커널이 적절한 init 프로세스를 찾으면서 실행할 프로세스를 결정하는 순서 /sbin/init /etc/init /bin/init /bin/sh 이 순서에서 가장 먼저 찾은 프로세스를 실행함. 네 가지 모두 실패하면 panic 발생 복구할 수 없는 치명적인 내부 에러를 감지 5.2.1 프로세스 ID 할당 보통 커널의 최대 pid 값은 32768이다. pid값으로 signed 16bit integer를 사용했던 오래된 유닉스 시스템과의 호환성을 위함. 커널은 pid를 순서대로 엄격하게 할당한다. /proc/sys/kernel/pid_max 값에 도달해서 처음부터 다시 할당하기 전까진 앞선 pid가 비어있더라도 재사용되지 않는다. 5.2.2 프로세스 계층 Spawn(새로운 프로세스를 생성하는) 프로세서를 부모 프로세스라고 한다. 새롭게 생성된 프로세스를 자식 프로세스라고 한다. init 프로세스를 제외한 모든 프로세스는 다른 프로세스로부터 생성된다. → 그래서 모든 자식 프로세스에는 부모 프로세스가 있다. ppid 로 확인가능함. 모든 프로세스는 사용자와 그룹이 소유하고 있다. 모든 자식 프로세스는 부모 프로세스의 사용자와 그룹 권한을 상속받는다. 접근 권한을 제어하기 위해 사용됨. 프로세스 그룹 프로세스와 다른 프로세시의 관계를 표현하고 있음. $ echo $$19610$ cat | grep | wc -l////////// other shell /////////$ ps -A -o pid,ppid,pgid,sid,command PID PPID PGID SID COMMAND19610 18942 19610 19610 /usr/bin/zsh29844 19610 29844 19610 cat29845 19610 29844 19610 grep --color=auto --exclude-dir=.bzr --exclude-dir=CVS --exclude-dir=.git --exclude-dir=.hg --exclude-dir=.svn --exclude-d29846 19610 29844 19610 wc -l5.2.3 pid_t pid 는 pid_t 자료형으로 표현됨 C의 int 자료형에 대한 typedef이다. typedef __kernel_pid_t __pid_t;#ifndef __kernel_pid_t typedef int __kernel_pid_t;#endif pid_t 자료형은 사실 int 형이다. 5.2.4 프로세스 ID와 부모 프로세스 ID 얻기 getpid() 시스템 콜은 호출한 프로세스의 pid를 반환함.#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;pid_t getpid (void); getppid() 는 호출한 프로세스의 부모 프로세스 pid를 반환함#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;pid_t getppid (void); 예제 printf (&quot;My pid=%jd\\n&quot;, (intmax_t) getpid ());printf (&quot;Parent&#39;s pid=%jd\\n&quot;, (intmax_t) getppid ()); 5.3 새로운 프로세스 실행하기 유닉스에서는 프로그램 이미지를 메모리에 적재하고 실행하는 과정 새로운 프로세스를 생성하는 과정 두 가지가 분리되어 있음. 5.3.1 exec 함수들 exec 류 시스템 콜은 한 가지로 제공되지 않고 여러 형태로 제공된다. 먼저 excel을 알아보자 #include &amp;lt;unistd.h&amp;gt;int execl (const char *path, const char *arg, ...); 호출하면 현재 프로세스를 path가 가리키는 프로그램으로 대체한다. arg path에 명시된 프로그램을 위한 첫 번째 인자다. ... 가변인자 뒤에 다른 인자가 여럿 올 수 있음 마지막은 반드시 NULL로 끝나야함. 예제 int ret;ret = execl (&quot;/bin/vi&quot;, &quot;vi&quot;, NULL);if (ret == −1) perror (&quot;execl&quot;); 실행 파일의 경로인 path의 마지막 요소 vi 를 첫 번째 인자로 두어, 프로세스의 fork()/exec 과정에서 argv[0]을 검사하여 바이너리 이미지의 이름을 찾을 수 있도록 한다. 일반적으로 반환값이 없다. 에러 발생시 -1 반환, errno 설정 성공 시 새로운 프로그램의 시작점으로 건너 뛰므로 이전에 실행했던 코드는 그 프로세스의 주소 공간에 더 이상 존재하지 않음. 대기 중인 시그널 사라짐 프로세스가 받은 시그널은 시그널 핸들러가 더 이상 프로세스의 주소 공간에 존재하지 않으므로 디폴트 방식으로 처리됨 메모리 락이 해제됨 스레드의 속성 대부분이 기본값으로 돌아감 프로세스의 통계 대부분이 재설정됨 맵핑된 파일을 포함하여 프로세스의 메모리 주소 공간과 관련된 모든 내용이 사라짐 사용자 영역에만 존재하는 모든 내용이 사라짐. Q) 마지막에 NULL을 넣는 이유는 뭘까? 즉, fork 로 child 프로세스를 만든 후 그 프로세스를 새로운 독립적인 프로세스로 만들어 주는 역할을 한다.다른 exec 함수들#include &amp;lt;unistd.h&amp;gt;int execlp (const char *file, const char *arg, ...);int execle (const char *path, const char *arg, ..., char * const envp[]);int execv (const char *path, char *const argv[]);int execvp (const char *file, char *const argv[]);int execve (const char *filename, char *const argv[], char *const envp[]); exec 라는 기본 이름 뒤에 함수의 특징을 나타내는 알파벳이 뒤따름. l 포함하는 함수 : 인자를 리스트로 전달함. v 포함하는 함수 : 인자를 배열로 전달함. e 포함하는 함수 : 새로 생성되는 함수의 환경변수를 지정함 p 포함하는 함수 : 실행파일을 사용자의 환경변수에서 찾도록함 exec 함수군 중에서 execve() 만 시스템 콜이고 다른 것들은 래퍼 함수임. 인자로 배열을 사용하면 인자를 실행 시간에 결정할 수 있다는 장점이 있음 Q) 바로 위의 문장 무슨 뜻? Q)tip 으로 나온 execlp()와 execvp() 함수의 보안위험 무슨말?? 205페이지5.3.2 fork() 시스템 콜 fork 시스템 콜을 사용해서 현재 실행 중인 프로세스와 동일한 프로세스를 새롭게 실행할 수 있다.#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;pid_t fork (void); fork() 호출이 성공하면 fork() 를 실행한 프로세스와 거의 모든 내용이 동일한 새로운 프로세스를 생성함. 두 프로세스는 계속 실행 상태 성공 시 부모 프로세스에서는 fork() 시스템 콜의 반환값은 자식 프로세스의 pid가 됨. 필수적인 항목을 제외하고는 거의 모든 측면에서 자식,부모가 동일 자식의 pid는 새롭게 할당됨 자식의 ppid는 부모의 pid 자식 프로세스에서 리소스 통계는 0으로 초기화됨 처리되지 않은 시그널은 모두 사라지고 자식 프로세스로 상속되지 않음. 부모가 가지고 있던 파일 락은 상속되지 않음 호출 실패 시 -1반환, errno 설정 예제 pid_t pid;pid = fork ();if (pid &amp;gt; 0) printf (&quot;I am the parent of pid=%d!\\n&quot;, pid);else if (!pid) printf (&quot;I am the child!\\n&quot;);else if (pid == −1) perror (&quot;fork&quot;) 가장 흔한 예제는 새로운 프로세스를 생성하고 그 후에 새 프로세스에 새로운 바이너리 이미지를 올리는 것임. 즉 ,fork() 하고 자식 프로세스는 exec 를 진행함 예제 pid_t pid;pid = fork ();if (pid == −1) perror (&quot;fork&quot;);/* the child ... */if (!pid) { const char *args[] = { &quot;windlass&quot;, NULL }; int ret; ret = execv (&quot;/bin/windlass&quot;, args); if (ret == −1) { perror (&quot;execv&quot;); exit (EXIT_FAILURE); }} 부모 프로세스는 자식 프로세스가 생겼다는 사실 외에는 아무런 변화 없이 진행됨 copy-on-write 기존에는 fork() 수행 시 페이지 단위로 복사했음 최신 유닉스 시스템은 주소 공간 모두 복사하는 것이 아니라 페이지에 대한 COW를 수행함. COW 복사에 의한 부하를 완화하기 위한 최적화기법. 자신이 가지고 있는 리소스의 읽기 요청이 들어오더라도 포인터만 넘겨받으면 된다는 전제에서 시작함. 쓰기 작업을 할 경우에만 복사가 일어남. vfork() 쓸모없는 주소 공간 복사 문제를 해결하기 위한 옛날 노력 fork()와 같은 동작을 하지만 자식 프로세스는 즉시 exec 계열의 함수를 성공적으로 호출하든가, _exit() 함수를 호출해서 프로세스를 끝내야함 vfork() 구현은 버그를 수반함 exec 호출이 실패할 경우는? 자식 프로세스가 어떻게 처리해야할지 파악하거나 종료하기 전까지는 계속 멈춰있음. Q) 요부분 다시 이해하기 5.4 프로세스 종료하기#include &amp;lt;stdlib.h&amp;gt;void exit (int status); exit()을 호출하면 몇 가지 기본적인 종료 단계를 거쳐 커널이 프로세스를 종료함. 반환값이 없기 때문에 exit 호출 이후 명령은 의미가 없음 종료 순서 atexit()이나 on_exit()에 등록된 함수가 있다면 등록 수선의 역순으로 호출 열려있는 모든 표준 입출력 스트림의 버퍼를 비운다. tmpfile() 함수를 통해 생성한 임시 파일을 삭제 프로세스가 사용자 영역에서 해야하는 모든 작업을 종료시킴. 마지막으로 exit()은 _exit() 시스템 콜을 실행해서 나머지 단계를 커널이 처리하게 한다. 프로세스가 종료되면 커널은 해당 프로세스가 생성한 더 사용되지 않는 모든 리소스를 정리한다. _exit()을 직접 사용하면 표준 출력 스트림을 비우는 등의 사후 처리를 직접 해야한다. vfork()를 사용하면 _exit() 을 사용해야함. 5.4.1 프로세스를 종료하는 다른 방법 고전적인 방법은 시스템 콜을 사용하지 않고, 단순히 프로그램을 끝까지 진행시키는 것임 main() 함수가 반환되는 경우 하지만 이런 경우도 컴파일러가 프로그램 종료 코드 이후에 exit() 시스템 콜을 묵시적으로 추가한다. SIGTERM 과 SIGKILL 을 보내서 종료도 가능 커널에 밉보이는 방법 잘못된 연산 수행 세그멘테이션 폴트 일으킴 메모리 고갈 리소스 과다 소모 → 프로세스를 강제로 죽임 5.4.2 atexit() atexit() 함수는 프로세스가 종료될 때 실행할 함수를 등록하기 위한 용도로 사용됨.#include &amp;lt;stdlib.h&amp;gt;int atexit (void (*function)(void)); 정상적으로 실행되면 프로세스가 정상적으로 종료될 때 호출할 함수를 등록함. 등록할 함수는 아무런 인자도 갖지 않고 어떠한 값도 반환하지 않는 함수여야함. 이 함수들을 스택에 저장되며 LIFO 방식으로 실행됨. 등록된 함수에서 exit()을 호출한다면 무한 루프에 빠진다. _exit() 을 사용하자 예제 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;void out (void){ printf (&quot;atexit() succeeded!\\n&quot;);}int main (void){ if (atexit (out)) fprintf(stderr, &quot;atexit() failed!\\n&quot;); return 0;} 5.4.3 on_exit() SunOS4 는 atexit()와 동일한 자신만의 함수를 정의하고 있으며, 리눅스의 glibc에서도 지원한다.#include &amp;lt;stdlib.h&amp;gt;int on_exit (void (*function)(int, void *), void *arg); atexit()와 동일하게 동작하지만 등록할 수 있는 함수의 프로토 타입이 다름.void my_function (int status, void *arg);5.4.4 SIGCHLD 프로세스가 종료될 때 커널은 SIGCHLD 시그널을 부모 프로세스로 보낸다. 기본적으로 부모 프로세스는 이 시그널을 무시하며 아무런 행동도 하지 않음. 하지만 프로세스는 signal() 이나 sigaction() 시스템 콜을 사용해서 처리한다. 부모 프로세스 관점에서는 자식 프로세스의 종료가 비동기로 일어남5.5. 자식 프로세스 종료 기다리기 시그널을 통해 알림을 받는 방법도 훌륭하지만, 많은 부모 프로세스는 자식 프로세스 중 하나가 종료될 때 좀 더 많은 정보를 얻고자 함. 자식 프로세스가 완전히 사라져버리면 정보를 얻을 수 없어서, 유닉스 초기 설계자들은 자식 프로세스가 부모 프로세스보다 먼저 죽으면 자식 프로세스를 좀비 프로세스로 바꾸기로 함. 아주 최소한의 기본적인 커널 자료구조만 가지고 있음. 좀비프로세스는 부모 프로세스가 자신의 상태를 조사하도록 기다림. 부모 프로세스가 종료된 자식 프로세스로부터 정보를 회수한 다음에야 공식적으로 종료됨. 리눅스 커널은 종료된 자식 프로세스에 대한 정보를 얻기 위해 몇 가지 인터페이스를 제공함 wait()#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/wait.h&amp;gt;pid_t wait (int *status); wait()을 호출하면 종료된 프로세스의 pid를 반환하며 에러가 발생한 경우 -1을 반환. 만약 자식 프로세스가 종료되지 않았다면 자식 프로세스가 종료될 때까지 블록됨. (대기 상태) 이미 종료된 상태라면 호출은 즉시 반환됨. 예제 #include &amp;lt;unistd.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/wait.h&amp;gt;int main (void){ int status; pid_t pid; if (!fork ()) return 1; pid = wait (&amp;amp;status); if (pid == −1) perror (&quot;wait&quot;); printf (&quot;pid=%d\\n&quot;, pid); if (WIFEXITED (status)) printf (&quot;Normal termination with exit status=%d\\n&quot;, WEXITSTATUS (status)); if (WIFSIGNALED (status)) printf (&quot;Killed by signal=%d%s\\n&quot;, WTERMSIG (status), WCOREDUMP (status) ? &quot; (dumped core)&quot; : &quot;&quot;); if (WIFSTOPPED (status)) printf (&quot;Stopped by signal=%d\\n&quot;, WSTOPSIG (status)); if (WIFCONTINUED (status)) printf (&quot;Continued\\n&quot;); return 0;} 이 프로그램은 즉시 종료되는 자식 프로세스를 생성한다. 부모 프로세스는 wait() 시스템 콜을 호출해서 자식 프로세스의 상태를 확인한다. 자식 프로세스의 pid와 어떻게 종료되었는지를 출력함. 5.5.1 특정 프로세스 기다리기 자식프로세스의 행동을 관찰하는 것은 중요한데, 모든 자식프로세스의 상황을 확인하는 것은 번거롭다. 기다리길 원하는 프로세스의 pid를 알고있다면 waitpid() 시스템 콜을 사용할 수 있다.#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/wait.h&amp;gt;pid_t waitpid (pid_t pid, int *status, int options); watipid() 는 wait() 보다 훨씬 강력한 버전이다. pid &amp;lt; -1 프로세스 gid가 이 값의 절댓값과 동일한 모든 자식 프로세스를 기다림. -1 모든 자식 프로세스를 기다린다. 이렇게 하면 wait()와 동일하게 동작함 0 호출한 프로세스와 동일한 프로세스 그룹에 속한 모든 자식 프로세스를 기다림 0 인자로 받은 pid와 일치하는 자식 프로세스를 기다린다. status wait() 의 status와 동일하게 동작 options WNOHANG 이미 종료된 자식 프로세스가 없다면 블록되지 않고 바로 반환 WUNTRACED 호출하는 프로세스가 자식 프로세스를 추적하지 않더라도 반환되는 status 인자에 WIFSTOPPED비트가 설정됨 WCONTINUED waitpid() 는 상태가 바뀐 프로세스의 pid를 반환함. WNOHANG 이 설정되고 지정한 자식 프로세스의 상태가 아직 바뀌지 않았다면 0을 반환 에러 발생시 -1 반환, errno 설정 예제 pid가 1742인 특정 자식 프로세스의 반환값을 알려고 하며 자식 프로세스가 아직 종료되지 않았다면 즉시 반환 하는 예제 int status;pid_t pid;pid = waitpid (1742, &amp;amp;status, WNOHANG);if (pid == −1) perror (&quot;waitpid&quot;);else { printf (&quot;pid=%d\\n&quot;, pid); if (WIFEXITED (status)) printf (&quot;Normal termination with exit status=%d\\n&quot;, WEXITSTATUS (status)); if (WIFSIGNALED (status)) printf (&quot;Killed by signal=%d%s\\n&quot;, WTERMSIG (status), WCOREDUMP (status) ? &quot; (dumped core)&quot; : &quot;&quot;);} 사용법 wait (&amp;amp;status);둘은 동일waitpid (−1, &amp;amp;status, 0); 5.5.2 좀 더 다양한 방법으로 기다리기 waitid()#include &amp;lt;sys/wait.h&amp;gt;int waitid (idtype_t idtype, id_t id, siginfo_t *infop, int options); wait() 이나 waitpid()와 마찬가지로 waitid()는 자식 프로세스를 기다리고 상태변화 (종료, 멈춤, 다시 실행) 을 얻기 위해 사용함. 더 많은 옵션을 제공하는 대신 훨씬 복잡함. pid인자 하나로 기다리는 것이 아니라, idtype 과 id 인자로 기다릴 자식 프로세스를 지정함. idtype P_PID pid가 id와 일치하는 자식 프로세스를 기다림 P_GID gid가 id와 일치하는 자식 프로세스를 기다림 P_ALL 모든 자식 프로세스를 기다림. id는 무시됨 id_t 타입은 거의 보기 힘든 타임. 일반적인 식별 번호를 나타내는 타입이다. 나중에 새로운 idtype 값이 추가되었을 경우를 대비하여 미리 정의된 타입이 나중에 새롭게 생성된 식별자를 저장할 수 있도록 충분한 여유를 제공하기 위해서임. options WEXITED WSTOPPED WCONTINUED WNOHANG WNOWAIT 성공적으로 반환하면 유효한 siginfo_t 타입을 가리키는 infop 인자에 값을 채운다. siginfo_t 타입 si_pid si_uid si_code 자식프로세스의 종료상태를 나타냄 si_signo si_status 성공하면 0반환, 에러 발생 시 -1 반환하고 errno 설정 waitid() 는 siginfo_t 구조체를 통해 다양한 정보를 얻을 수 있다. 근데 만약 이런 정보가 필요하지 않다면 단순 함수를 사용하는게 시스템 이식성을 위해서 더 바람직하다.5.5.3 BSD 방식으로 기다리기 : wait3()과 wait4() BSD는 자식 프로세스의 상태 변화를 기다리기 위한 두 가지 독자적인 함수를 제공함. Berkeley Software Distribution #include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/time.h&amp;gt;#include &amp;lt;sys/resource.h&amp;gt;#include &amp;lt;sys/wait.h&amp;gt;pid_t wait3 (int *status, int options, struct rusage *rusage);pid_t wait4 (pid_t pid, int *status, int options, struct rusage *rusage);pid_t wait3(int* status, int options, struct rusage* rusage) { return wait4(-1, status, options, rusage);} 함수 뒤에 붙은 숫자는 인자의 개수를 뜻한다. rusage 인자만 예외로 하면 두 함수는 waitpid와 흡사하다. 예제 wait3() pid = wait3 (status, options, NULL);동일pid = waitpid (−1, status, options); wait4() pid = wait4 (pid, status, options, NULL);동일pid = waitpid (pid, status, options); wait3()는 모든 자식 프로세스의 상태 변화를 기다리며, wait4()는 pid인자로 지정한 특정 자식 프로세스의 상태 변화만 기다린다. options 인자는 waitpid와 동일 rusage (Resource Usage) waitpid와 가장 큰 차이점인데, rusage 포인터가 NULL이 아니면 자식 프로세스에 관한 정보를 채워넣는다. #include &amp;lt;sys/resource.h&amp;gt;struct rusage { struct timeval ru_utime; /* user time consumed */ struct timeval ru_stime; /* system time consumed */ long ru_maxrss; /* maximum resident set size */ long ru_ixrss; /* shared memory size */ long ru_idrss; /* unshared data size */ long ru_isrss; /* unshared stack size */ long ru_minflt; /* page reclaims */ long ru_majflt; /* page faults */ long ru_nswap; /* swap operations */ long ru_inblock; /* block input operations */ long ru_oublock; /* block output operations */ long ru_msgsnd; /* messages sent */ long ru_msgrcv; /* messages received */ long ru_nsignals; /* signals received */ long ru_nvcsw; /* voluntary context switches */ long ru_nivcsw; /* involuntary context switches */}; 성공할 경우 상태가 변경된 프로세스의 pid를 반환함. 실패 시 -1반환하고 errno 설정 wait3 와 wait4 는 POSIX가 정의한 함수는 아니므로 리소스 사용정보가 매우 중요한 경우에만 사용하자.5.5.4 새로운 프로세스를 띄운 다음에 기다리기 ANSI C와 POSIX는 새로운 프로세스를 생성하고 종료를 기다리는 동작을 하나로 묶은, 말하자면 동기식 프로세스 생성 인터페이스를 정의하고 있음.#define _XOPEN_SOURCE /* if we want WEXITSTATUS, etc. */#include &amp;lt;stdlib.h&amp;gt;int system (const char *command); 함수이름이 system인 이유는 동기식 프로세스 생성이 시스템 외부로 셸 띄우기라고 불리기 때문임. 흔히 간단한 유틸리티나 셸 스크립트를 실행할 목적으로 system() 을 사용하는데 종종 실행 결과의 반환값을 얻기 위한 명시적인 목적으로 사용하기도 한다. command 주어진 명령을 실행한다. /bin/sh -c 뒤에 command가 따라 붙음 즉, 셸에 그대로 전달되는 것 NULL 이면 /bin/sh가 유효하면 0이 아닌 값, 그렇지 않다면 0 반환 호출이 성공하면 wait()와 마찬가지로 그 명령의 상태를 반환함. 실행한 명령의 종료 코드는 WEXITSTATUS 로 얻을 수 있다. 명령어 수행에 실패했다면 exit(127)과 동일 command 를 실행하는 동안 SIGCHLD 는 블록되고 SIGINT와 SIGQUIT는 무시된다. 몇 가지 주의점 system()이 반복문안에서 실행될 때 문제가 발생함. 프로그램이 자식 프로세스의 종료 상태를 적절하게 검사할 수 있도록 해야한다. 예제 do { int ret; ret = system (&quot;pidof rudderd&quot;); if (WIFSIGNALED (ret) &amp;amp;&amp;amp; (WTERMSIG (ret) == SIGINT || WTERMSIG (ret) == SIGQUIT)) break; /* or otherwise handle */} while (1); fork(), exec 함수군, waitpid()를 사용해서 system() 구현하기 /* * my_system - synchronously spawns and waits for the command * &quot;/bin/sh -c &amp;lt;cmd&amp;gt;&quot;. * * Returns −1 on error of any sort, or the exit code from the * launched process. Does not block or ignore any signals. */int my_system (const char *cmd){ int status; pid_t pid; pid = fork (); if (pid == −1) return −1; else if (pid == 0) { const char *argv[4]; argv[0] = &quot;sh&quot;; argv[1] = &quot;-c&quot;; argv[2] = cmd; argv[3] = NULL; execv (&quot;/bin/sh&quot;, argv); exit (−1); } if (waitpid (pid, &amp;amp;status, 0) == −1) return −1; else if (WIFEXITED (status)) return WEXITSTATUS (status); return −1;} System 의 문제 child의 stdout 등 의 값을 볼 수가 없음. popen을 쓴다? 5.5.5 좀비 프로세스 실행을 마쳤지만 부모 프로세스에서 종료 코드를 읽어가지 않은, 즉 부모 프로세스에서 wait() 시스템 콜을 호출하지 않은 프로세스를 의미 최소한의 기본 뼈대만 유지할만큼 적은 리소스를 차지하지만 어쨌든 시스템 리소스를 계속 소비하고 있음. 부모 프로세스가 종료될 때마다 리눅스 커널은 그 프로세스의 자식 프로세스 목록을 뒤져서 모두 init 프로세스 (pid = 1) 의 자식으로 입양시킨다. 고아 프로세스가 생기지 않도록 보장함. 5.6 사용자와 그룹5.6.1 실제, 유효, 저장된 사용자 ID와 그룹 ID 그룹아이디도 아래와 동일하게 적용됨 실제 사용자 ID (Rreal User ID) 프로세스를 최초로 실행한 사용자의 ID 유효 사용자 ID (Effective User ID) 프로세스가 현재 영향을 미치고 있는 사용자 ID 저장된 사용자 ID (Saved User ID) 프로세스의 최초 유효 사용자 ID → 유효한 사용자 ID가 가장 중요한 값. 이는 프로세스의 자격을 확인하는 과정에서 점검하는 사용자 ID이다5.6.2 실제, 저장된 사용자, 그룹 ID 변경하기#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int setuid (uid_t uid);int setgid (gid_t gid); setuid() 호출하면 현재 프로세스의 유효 사용자 ID를 설정한다. 현재 유효 사용자 ID가 0(root) 이면 실제 사용자와 저장된 사용자 ID 역시 설정됨 성공하면 0반환, 실패 시 -1 반환 errno 설정 앞의 내용은 그룹 ID에도 동일하다.5.6.3 유효 사용자 ID나 유효 그룹 ID 변경하기#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int seteuid (uid_t euid);int setegid (gid_t egid); seteuid() 를 호출하면 유효 사용자 ID 를 euid로 설정한다. root 사용자는 euid 값으로 어떤 값이든 사용할 수 있다. 비 root 사용자는 seteuid() 와 setuid() 가 동일하게 동작함. 앞의 내용은 그룹 ID에도 동일하다.5.6.4 BSD 방식으로 사용자, 그룹 ID 변경하기#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;int setreuid (uid_t ruid, uid_t euid);int setregid (gid_t rgid, gid_t egid); setreuid() 를 호출하면 프로세스의 실제 사용자 ID와 유효 사용자 ID를 각각 ruid와 euid로 설정한다.5.6.5 HP-UX 방식으로 사용자, 그룹 ID 변경하기#define _GNU_SOURCE#include &amp;lt;unistd.h&amp;gt;int setresuid (uid_t ruid, uid_t euid, uid_t suid);int setresgid (gid_t rgid, gid_t egid, gid_t sgid); setresuid() 를 호출하면 실제, 유효, 그리고 저장된 사용자 ID를 각각 ruid, euid, suid로 설정한다. -1 을 지정하면 ID는 변경하지 않은채로 놔둔다. 5.6.6 바람직한 사용자/그룹 ID 조작법 비 root 프로세스는 seteuid() 를 사용해서 유효 사용자 ID를 바꿔야한다.5.6.7 저장된 사용자 ID 지원5.6.8 사용자 ID와 그룹 ID 얻어오기#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;uid_t getuid (void);gid_t getgid (void);#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;uid_t geteuid (void);gid_t getegid (void); 이 함수들은 실패하지 않으며 유효 사용자 ID와 유효 그룹 ID를 반환함.5.7 세션과 프로세스 그룹 각 프로세스는 작업 제어 목적으로 관련된 하나 이상의 프로세스를 모아놓은 집합인 프로세스 그룹의 일원이다. 프로세스 그룹의 주된 속성은 그룹 내 모든 프로세스에게 시그널을 보낼 수 있다는 점이다. 각 프로세스 그룹은 pgid 로 구분하며 프로세스 그룹마다 그룹 리더가 있다. 구성원이 하나라도 남아있는 동안에는 그룹이 사라지지 않음 그룹 리더가 종료되더라도 프로세스 그룹은 남는다. 새로운 사용자가 처음으로 시스템에 로그인하면 로그인 프로세스는 사용자 로그인 셸 프로세스 하나로 이루어진 새로운 세션을 생성한다. 세션은 하나 이상의 프로세스 그룹이 들어 있는 집합이다. 로그인한 사용자 활동을 처리하며 사용자의 터미널 입출력을 다루는 tty 장치로 명시되어 제어 터미널과 사용자 사이를 연결한다. 대부분 셸과 관련을 맺고 있다. 프로세스 그룹이 작업 제어와 다른 셸 기능을 쉽게 하도록 모든 구성원에게 시그널을 보내는 메커니즘을 제공한다면 세션은 제어 터미널을 둘러싼 로그인을 통합하는 기능을 제공한다. 세션에 속한 프로세스 그룹은 하나의 Foreground 프로세스 그룹과 0개 이상의 Background 프로세스 그룹으로 나뉜다. 사용자가 터미널을 종료하면 Foreground 프로세스 그룹 내 모든 프로세스에 SIGQUIT 시그널이 전달됨 터미널에서 네트워크 단절이 포착되면 Foreground 프로세스 그룹 내 모든 프로세스에 SIGHUP 시그널이 전달됨. 사용자가 Ctrl + C 같은 취소 명령을 입력했을 경우 Foreground 프로세스 그룹 내 모든 프로세스에 SIGINT 시그널이 전달됨. 예제 사용자가 시스템에 로그인했고, bash의 pid가 1700일 때 bash 인스턴스는 gid가 1700인 새로운 프로세스 그룹의 유일한 멤버이자 리더가 됨. 그 셸에서 실행하는 명령어는 세션 1700에 속하는 새로운 프로세스 그룹에서 동작. 사용자에게 직접 연결되어 있고 터미널 제어가 가능한 프로세스 그룹 중 하나가 포어그라운드 프로세스 그룹 다른 프로세스 그룹은 백그라운드 프로세스 그룹 다른 예제 $ cat ship-inventory.txt | grep booty | sort 세 개의 프로세스를 가지는 하나의 프로세스 그룹은 생성함. → 셸에서 한번에 세 프로세스 모두에 시그널을 보낼 수 있다는 뜻. 사용자가 명령어 뒤에 &amp;amp;를 붙인다면 백그라운드로 돈다. 5.7.1 세션 시스템 콜 시스템에 로그인을 하는 시점에 셸은 새로운 세션을 생성한다. 이 작업은 새로운 세션을 쉽게 만들 수 있는 특수한 시스템 콜을 통해서 이루어진다.#include &amp;lt;unistd.h&amp;gt;pid_t setsid (void); setsid() 를 호출하면 그 프로세스가 프로세스 그룹의 리더가 아니라고 가정하고 새로운 세션을 생성한다. 호출한 프로세스는 새롭게 만들어진 세션의 유일한 멤버이자 리더가 되며 제어 tty를 가지지 않는다. → setsid() 는 새로운 세션 내부에 새로운 프로세스 그룹을 생성하며 호출한 프로세스를 그 세션과 프로세스 그룹 모두의 리더로 정한다. 호출 성공 시 새롭게 생성한 세션의 ID 반환, 실패 시 -1 반환하고 errno 설정.(EPERM 뿐) 예제 어떤 프로세스가 프로세스 그룹 리더가 되지 않게 하는 가장 손쉬운 방법은 프로세스를 포크하고 부모 프로세스를 종료한 다음 자식 프로세스에서 setsid() 를 호출하는 것 pid_t pid;pid = fork ();if (pid == −1) { perror (&quot;fork&quot;); return −1;} else if (pid != 0) exit (EXIT_SUCCESS);if (setsid () == −1) { perror (&quot;setsid&quot;); return −1;} 세션 ID를 얻는 법 #define _XOPEN_SOURCE 500#include &amp;lt;unistd.h&amp;gt;pid_t getsid (pid_t pid); getsid() 호출이 성공하면 pid가 가리키는 프로세스 세션 ID를 반환한다. Q) 왜 유용하지 않음? getsid() 는 드물지만 주로 진단 목적으로 사용함 pid값이 0 이면 getsid() 를 호출한 프로세스의 세션 ID를 반환한다. pid_t sid;sid = getsid (0);if (sid == −1) perror (&quot;getsid&quot;); /* should not be possible */else printf (&quot;My session id=%d\\n&quot;, sid); 5.7.2 프로세스 그룹 시스템 콜 setpgid() 는 pid 인자로 지정한 프로세스의 프로세스 그룹 ID를 pgid로 설정한다.#define _XOPEN_SOURCE 500#include &amp;lt;unistd.h&amp;gt;int setpgid (pid_t pid, pid_t pgid); pid인자가 0인 경우 현재 프로세스의 프로세스 그룹 ID를 변경하며 pgid 인자가 0인 경우 pid 인자로 지정한 프로세스의 ID를 프로세스 그룹 ID로 설정한다. 호출 성공 여부 pid로 지정한 프로세스가 해당 시스템 콜을 호출하는 프로세스이거나 호출하는 프로세스의 자식 프로세스이며 아직 exec 호출을 하지 않았고 부모 프로세스와 동일한 세션에 속해 있어야 한다. pid로 지정한 프로세스가 세션의 리더가 아니어야 한다. pgid가 이미 있으면 호출하는 프로세스와 동일한 세션에 속해 있어야 한다. pgid 값이 양수여야 한다. 세션과 마찬가지로 프로세스의 프로세스 그룹 ID를 얻는 것도 가능하지만 유용하지 않음. #define _XOPEN_SOURCE 500#include &amp;lt;unistd.h&amp;gt;pid_t getpgid (pid_t pid); Q) 왜 유용하지 않음?5.7.3 사용되지 않는 프로세스 그룹 관련 함수들 리눅스는 프로세스 그룹 ID를 가져오거나 설정하는 두 가지 오래된 BSD 인터페이스를 지원한다. 프로세스 그룹 ID를 설정할 때 사용 #include &amp;lt;unistd.h&amp;gt;int setpgrp (void); if (setpgrp () == −1) perror (&quot;setpgrp&quot;); 다음 setpgid() 코드와 동일함 if (setpgid (0,0) == −1) perror (&quot;setpgid&quot;); 둘 다 현재 프로세스를 현재 프로세스의 pid와 같은 프로세스 그룹으로 설정함. getpgrp() 함수는 프로세스의 그룹 ID를 알아내기 위해 사용함 #include &amp;lt;unistd.h&amp;gt;pid_t getpgrp (void); pid_t pgid = getpgrp (); getpgid() 코드와 동일 pid_t pgid = getpgid (0); 5.8 데몬 Daemon 은 백그라운드에서 수행되며 제어 터미널이 없는 프로세스이다. 일반적으로 부팅 시에 시작되며 root 혹은 다른 특수한 사용자 계정 (apache, postfix … ) 권한으로 실행되어 시스템 수준의 작업을 처리한다. 데몬의 두 가지 일반적인 요구사항 반드시 init 프로세스의 자식 프로세스여야 함. 터미널과 연결되어 있으면 안됨. 다음 과정을 통해 데몬이 될 수 있다. fork() 를 호출해서 데몬이 될 새로운 프로세스를 생성 부모 프로세스에서 exit() 을 호출해서 데몬 프로세스의 부모 프로세스를 종료한다. setsid()를 호출해서 데몬이 새로운 프로세스 그룹과 세션의 리더가 되도록 한다. chdir() 를 사용하여 작업 디렉터리를 루트 디렉터리로 변경한다. 모든 fd 를 닫는다. 0, 1, 2번 fd(각각 표준 입력,출력,에러) 를 열고 /dev/null 로 리다이렉트 한다. 예제 규칙에 따라 스스로 데몬이 되는 예제 코드 #include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;linux/fs.h&amp;gt;int main (void){ pid_t pid; int i; /* create new process */ pid = fork (); if (pid == −1) return −1; else if (pid != 0) exit (EXIT_SUCCESS); /* create new session and process group */ if (setsid () == −1) return −1; /* set the working directory to the root directory */ if (chdir (&quot;/&quot;) == −1) return −1; /* close all open files--NR_OPEN is overkill, but works */ for (i = 0; i &amp;lt; NR_OPEN; i++) close (i); /* redirect fd&#39;s 0,1,2 to /dev/null */ open (&quot;/dev/null&quot;, O_RDWR); /* stdin */ dup (0); /* stdout */ dup (0); /* stderror */ /* do its daemon thing... */ return 0;} 대부분의 유닉스 시스템은 C 라이브러리에서 daemon() 함수를 제공하여 이 과정을 자동화하여 간단하게 쓸 수 있다. #include &amp;lt;unistd.h&amp;gt;int daemon (int nochdir, int noclose); nochdir 인자가 0이 아니면 현재 작업 디렉터리를 루트 디렉터리로 변경하지 않는다. noclose 인자가 0이 아니면 열려있는 모든 fd를 닫지 않는다. 일반적으로 두 인자를 0으로 넘긴다. " }, { "title": "[Linux System Programming] Ch04 고급 버퍼 입출력 ", "url": "/posts/linux_ch04/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-21 00:00:00 +0900", "snippet": "[Ch04 고급 버퍼 입출력]2장에서는 파일입출력의 근본일 뿐만 아니라, 리눅스에서 일어나는 모든 통신의 토대인 기본 입출력 시스템 콜을 배웠다.3장에서는 기본 입출력 시스템 콜에 사용자 영역 버퍼링이 필요한 때를 알아보고 해법으로 C언어의 표준 입출력 라이브러리에 대해 공부했다.4장에서는 리눅스의 고급 입출력 시스템 콜에 대해 알아본다.벡터 입출력 한번의 호출로 여러 버퍼에서 데이터를 읽거나 쓸 수 있도록 해줌. 다양한 자료구조를 단일 입출력 트랜젝션으로 다룰 때 유용하다.epoll poll()과 select() 시스템 콜을 개선한 시스템 콜이다. 싱글 스레드에서 수백 개의 FD를 poll해야 하는 경우에 유용하다.메모리맵 입출력 파일을 메모리에 맵핑해서 간단한 메모리 조작을 통해 파일 입출력을 수행함. 특정 패턴의 입출력에 유용하다.파일 활용법 조언 프로세스에서 파일을 사용하려**의도**를 커널에게 제공할수 있도록 하여, 입출력 성능을 향상시킴.비동기식 입출력 작업 완료를 기다리지 않는 입출력을 요청한다. 스레드를 사용하지 않고 동시에 입출력 부하가 많은 작업을 처리할 경우 유용함4.1 벡터 입출력 한번의 시스템 콜을 사용해서 여러개의 버퍼 벡터에 쓰거나, 여러 개의 버퍼 벡터로 읽어 들일 때 사용하는 입출력 메서드 2장의 표준 읽기와 쓰기는 선형 입출력이라고 함. 벡터 입출력의 장점 좀 더 자연스러운 코딩 패턴 - 미리 정의된 구조체의 여러 필드에 걸쳐서 데이터가 분리되어 있는 경우, 벡터 입출력을 사용하면 직관적인 방법으로 조작할 수 있음 효율 - 한번의 사용으로 여러번의 선형 입출력 연산을 대체할 수 있음 성능 - 시스템 콜 호출 횟수 ⬇️, 내부적으로 최적화된 구현을 제공 원자성 - 벡터 입출력 연산 중에 다른 프로세스가 끼어들 수 없음 4.1.1 readv() 와 writev() readv() 함수는 fd에서 데이터를 읽어서 count 개수만큼 iov 버퍼에 저장한다.ssize_t readv (int fd, const struct iovec *iov, int count); writev() 함수는 count 개수만큼 iov 버퍼에 있는 데이터를 fd에 기록함ssize_t writev(int fd, const struct iovec *iov, int count); readv()와 writev() 함수는 여러 개의 버퍼를 사용한다는 점에서 read(), write()와 구분됨 iovec 구조체는 세그먼트라고 하는 독립적으로 분리된 버퍼를 나타낸다.struct iovec{ void *iov_base; // 버퍼의 시작 포인터 size_t iov_len; // 버퍼 크기 (바이트)} 이런 세그먼트의 집합을 벡터라고 한다. 벡터의 각 세그먼트에는 데이터를 기록하거나 읽어올 메모리 공간의 주소와 크기가 저장되어 있다. 두 함수는 각 버퍼에 iov_len 바이트만큼 데이터를 채우거나 쓴 다음, 다음 버퍼로 넘어간다. 두 함수 모드 iov[0] 부터 시작해서 iov[1], 그리고 iov[count-1]까지 세그먼트 순서대로 동작한다.반환값 두 함수는 호출이 성공했을 때 읽거나 쓴 바이트 개수를 반환함 반환값은 반드시 count * iov_len 값과 같아야함. 에러 발생 시 -1을 반환, errno 를 설정 각각 read(), write() 시스템 콜에서 발생 가능한 모든 종류의 에러가 발생할 수 있음 추가로 두 가지 에러 상황을 정의하고 있음 반환값의 자료형이 ssize_t 이기 때문에, 만약 count * iov_len 값이 SSIZE_MAX보다 큰 경우에는 데이터가 전송되지 않고 -1을 반환하며 errno 는 EINVAL로 설정됨 POSIX에서는 count가 0보다 크고 IOV_MAX(리눅스에서는 현재 1024로 정의하고 있음) 와 같거나 작아야 한다고 명시하고 있는데, 만약 count가 0이라면 readv()와 writev()는 0을 반환한다. 만약 count 값이 IOV_MAX보다 크다면 데이터는 전송되지 않고 -1을 반환하며 errno는 EINVAL로 설정됨 최적 count 찾기 벡터 입출력 작업을 할 때 리눅스 커널에서는 각 세그먼트를 위해 내부 데이터 구조체를 반드시 할당하게 됨! 근데 이 할당은 count의 크기에 따라 동적으로 일어난다. 만약 count값이 크지 않다면 스택에 미리 만들어둔 작은 세그먼트 배열을 사용해서, 동적 할당이 일어나지 않도록한다. → 성능 개선! (아주 효율적으로 동작함) 그러니깐 벡터 입출력 연산을 사용할 때 세그먼트의 개수의 감이 오지 않는다면 8 이하로 시도~~예제 writev() 예제 3개의 벡터 세그먼트에 데이터를 쓰는 예제 각각 크기가 다른 문자열을 담고 있음 int main(){ struct iovec iov[3]; ssize_t nr; int fd, i; char *buf[] = { &quot;aaa&quot;, &quot;bbbb&quot;, &quot;cccccc&quot; }; fd = open(&quot;buccaneer.txt&quot;, O_WRONLY | OCREAT | O_TRUNC); if (fd == -1) // error return 1; // 세 iovec 구조체 값을 채운다 for (i = 0; i&amp;lt;3;i++){ iov[i].iov_base = buf[i]; iov[i].iov_len = strlen(buf[i]) + 1; } // 단 한번의 호출로 세 iovec 내용을 모두 쓴다 nr = writev (fd, iov, 3); if (nr == -1 ) //error return 1; printf(&quot;wrote %d bytes\\n&quot;, nr); if (close(fd)) //error return 1; return 0} readv()예제#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;sys/uio.h&amp;gt;int main (){ char foo[48], bar[51], baz[49]; struct iovec iov[3]; ssize_t nr; int fd, i; fd = open (&quot;buccaneer.txt&quot;, O_RDONLY); if (fd == −1) { perror (&quot;open&quot;); return 1; } /* set up our iovec structures */ iov[0].iov_base = foo; iov[0].iov_len = sizeof (foo); iov[1].iov_base = bar; iov[1].iov_len = sizeof (bar); iov[2].iov_base = baz; iov[2].iov_len = sizeof (baz); /* read into the structures with a single call */ nr = readv (fd, iov, 3); if (nr == −1) { perror (&quot;readv&quot;); return 1; } for (i = 0; i &amp;lt; 3; i++) printf (&quot;%d: %s&quot;, i, (char *) iov[i].iov_base); if (close (fd)) { perror (&quot;close&quot;); return 1; } return 0;} 구현#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;sys/uio.h&amp;gt;ssize_t naive_writev (int fd, const struct iovec *iov, int count){ ssize_t ret = 0; int i; for (i = 0; i &amp;lt; count; i++) { ssize_t nr; errno = 0; nr = write (fd, iov[i].iov_base, iov[i].iov_len); if (nr == −1) { if (errno == EINTR) continue; ret = −1; break; } ret += nr; } return ret;} readv()와 writev()는 사용자 영역에서 단순 루프를 사용해서 구현할 수 있음! 사실 리눅스 커널 내부의 모든 입출력은 벡터 입출력이다. read, write 구현 역시 하나짜리 세그먼트를 가지는 벡터 입출력으로 구현되어 있음. 4.2 epoll poll과 select 의 한계에 대해서 인지하면서 커널 2.6버전에서는 epoll(event poll) 이라는 기능이 추가되었음 poll과 select 실행할 때마다 전체 fd를 요구함 → 커널은 검사해야 할 모든 파일 리스트를 다 살펴봐야함. → fd 리스트의 크기가 수백 ~ 수천까지 커지면 병목현상이 발생 epoll은 실제로 검사하는 부분과 검사할 fd를 등록하는 부분을 분리해서 위의 문제를 해결함 epoll은 세 가지 System call로 동작함 epoll 컨텍스트를 초기화 검사해야 할 fd를 epoll 컨텍스트에 등록하거나 삭제함 실제 이벤트를 기다리도록 동작 4.2.1 새로운 epoll 인스턴스 생성하기 epoll 컨텍스트는 epoll_create1()을 통해서 생성됨#include &amp;lt;sys/epoll.h&amp;gt;int epoll_create1 (int flags);/* deprecated. use epoll_create1() in new code. */int epoll_create (int size); 호출이 성공하면 새로운 epoll 인스턴스를 생성하고 그 인스턴스와 연관된 fd (epoll fd) 를 반환한다. 요 fd는 실제 파일과는 아무런 관계가 없고 epoll 기능을 사용하는 다음 호출에 사용되는 핸들일 뿐임. flag 인자는 epoll 동작을 조정하기 위한 것 0을 쓰면 size 인자가 없어졌다는 점을 빼면 epoll_create()과 동일함! epoll_fd 의 크기정보를 전달했었음 현재는 EPOLL_CLOSEXEC 만 유효함 새 프로세스가 실행될 때 이 파일을 자동적으로 닫아준다. 에러가 발생하면 -1을 반환, errno를 설정 EINVAL 잘못된 flags 인자 EMFILE 사용자의 최대 파일 초과 ENFILE 시스템의 최대 파일 초과 ENOMEM 메모리 부족 사용예제 int epfd;epfd = epoll_create1 (0);if (epfd &amp;lt; 0) perror (&quot;epoll_create1&quot;); epoll_create1()에서 반환하는 fd는 폴링이 끝난 뒤에 반드시 close()로 닫아줘야한다. 4.2.2 epoll 제어 epoll_ctl() 시스템 콜은 주어진 epoll 컨텍스트에 fd를 추가하거나 삭제할 때 사용한다. #include &amp;lt;sys/epoll.h&amp;gt;int epoll_ctl (int epfd, int op, int fd, struct epoll_event *event); struct epoll_event { __u32 events; /* events */ union { void *ptr; int fd; __u32 u32; __u64 u64; } data;}; epoll_ctl() 호출이 성공하면 해당 epoll 인스턴스는 epfd 파일 디스크립터와 연결된다. epfd 이전에 epoll_create1() 로 생성한 epoll fd fd 등록할 fd op 인자는 fd가 가리키는 파일에 대한 작업을 명시한다. 어떤 변경을 할지 결정하는 값 EPOLL_CTL_ADD epfd와 연관된 epoll 인스턴스가 fd와 연관된 파일을 감시하도록 추가하며, 각 이벤트는 event 인자로 정의한다. EPOLL_CTL_DEL epfd와 연관된 epoll 인스턴스에 fd를 감시하지 않도록 삭제한다. EPOLL_CTL_MOD 기존에 감시하고 있는 fd에 대한 이벤트를 event에 명시된 내용으로 갱신한다. event 인자는 그 작업의 동작에 대한 설명을 담고 있다. 이벤트 유형 epoll_event 구조체의 events 필드는 주어진 fd에서 감시할 이벤트의 목록을 담고 있음 여러가지 이벤트를 OR로 묶을 수 있다. enum Events{ EPOLLIN, //수신할 데이터가 있다. EPOLLOUT, //송신 가능하다. EPOLLPRI, //중요한 데이터(OOB)가 발생. EPOLLRDHUD,//연결 종료 or Half-close 발생 EPOLLERR, //에러 발생 EPOLLET, //엣지 트리거 방식으로 설정 EPOLLONESHOT, //한번만 이벤트 받음} epoll_event 구조체의 data 필드는 사용자 데이터를 위한 필드이다. 이 필드에 담긴 내용은 요청한 이벤트가 발생해서 사용자에게 반환될 때 함께 반환됨. 일반적인 사용 예 event.data.fd를 fd로 채워서 이벤트가 발생했을 때 어떤 fd를 들여다 봐야 하는지 확인하는 용도 성공 시 0을 반환하고 실패 시 -1을 반환, errno 설정 예제 코드 struct epoll_event event;int ret;event.data.fd = fd; /* return the fd to us later (from epoll_wait) */event.events = EPOLLIN | EPOLLOUT;ret = epoll_ctl (epfd, EPOLL_CTL_ADD, fd, &amp;amp;event);if (ret) perror (&quot;epoll_ctl&quot;); epfd와 연관된 fd에 설정된 기존 구독 이벤트를 변경하려면 아래와 같이 작성하면 됨 struct epoll_event event;int ret;event.data.fd = fd; /* return the fd to us later */event.events = EPOLLIN;ret = epoll_ctl (epfd, EPOLL_CTL_MOD, fd, &amp;amp;event);if (ret) perror (&quot;epoll_ctl&quot;); 반대로 epoll 인스턴스 epfd에 등록된 fd에 연관된 기존 이벤트를 삭제하려면 아래와 같이! struct epoll_event event;int ret;ret = epoll_ctl (epfd, EPOLL_CTL_DEL, fd, &amp;amp;event);if (ret) perror (&quot;epoll_ctl&quot;); op 값이 EPOLL_CTL_DEL인 경우 이벤트 마스크가 없기 때문에 event 값이 NULL이 될 수도 있음. 하지만, 호환성 문제 떄문에 유효한 포인터를 넘겨야함. 4.2.3 epoll로 이벤트 기다리기 epoll_wait() 시스템 콜은 epoll 인스턴스와 연관된 fd에 대한 이벤트를 기다린다.#include &amp;lt;sys/epoll.h&amp;gt;int epoll_wait( int efpd, //epoll_fd struct epoll_event* event, //event 버퍼의 주소 int maxevents, //버퍼에 들어갈 수 있는 구조체 최대 개수 int timeout //select의 timeout과 동일 단위는 1/1000 ); epoll_wait 를 호출하면 timeout 밀리 초 동안 epoll 인스턴스인 epfd와 연관된 파일의 이벤트를 기다린다. 성공할 경우 events에는 발생한 해당 이벤트 (파일이 읽어나 쓰기가 가능한 상태인지를 나타내는 epoll_event 구조체에 대한 포인터) 가 기록된다. 발생한 이벤트 개수를 반환 events의 data 필드에는 사용자가 epoll_ctl() 을 호출하기 전에 설정한 값이 담겨 있다. 따라서 모든 fd에 대해 순회하면서 체크할 필요가 없음! 이벤트가 있는 fd들이 배열에 담겨오고 그 개수를 알 수 있으니 꼭 필요한 event 만 순회하면서 처리할 수 있다는 장점! 에러가 발생할 경우 -1을 반환하고 errno 를 설정 timeout 0이면 epoll_wait()는 이벤트가 발생하지 않아도 즉시 0을 반환함. -1이면 이벤트가 발생할 때까지 해당 호출은 반환되지 않음 예제#define MAX_EVENTS 64struct epoll_event *events;int nr_events, i, epfd;events = malloc (sizeof (struct epoll_event) * MAX_EVENTS);if (!events) { perror (&quot;malloc&quot;); return 1;}nr_events = epoll_wait (epfd, events, MAX_EVENTS, −1);if (nr_events &amp;lt; 0) { perror (&quot;epoll_wait&quot;); free (events); return 1;}for (i = 0; i &amp;lt; nr_events; i++) { printf (&quot;event=%ld on fd=%d\\n&quot;, events[i].events, events[i].data.fd); /* * We now can, per events[i].events, operate on * events[i].data.fd without blocking. */}free (events);4.2.4 에지 트리거와 레벨 트리거 epoll_ctl()로 전달하는 event 인자의 events 필드를 EPOLLET로 설정하면 fd에 대한 이벤트 모니터가 레벨 트리거가 아닌 에지 트리거로 동작한다. 유닉스 파이프 통신 입출력 예시 출력하는 쪽에서 파이프에 1KB만큼의 데이터를 씀 입력을 받는 쪽에서는 파이프에 대해서 epoll_wait()를 수행하고 파이프에 데이터가 들어와서 읽을 수 있는 상태가 되기를 기다림 레벨 트리거일 경우 2단계의 epoll_wait() 호출은 즉시 반환하며 파이프가 읽을 준비가 되었음을 알려줌 에지 트리거일 경우 1단계가 완료될 때까지 호출이 반환되지 않음. 즉, epoll_wait()를 호출하는 시점에 파이프를 읽을 수 있는 상황이더라도 파이프에 데이터가 들어오기 전까지는 결과 반환 안함. 기본 동작 방식은 레벨 트리거 poll()과 select()의 동작방식도 동일 4.3 메모리에 파일 맵핑하기 리눅스 커널은 표준 파일 입출력의 대안으로 애플리케이션이 파일을 메모리에 맵핑할 수 있는 인터페이스를 제공한다. 메모리 주소와 파일의 단어가 일대일 대응이 된다는 것을 의미 → 개발자가 메모리를 통해 파일에 직접 접근이 가능함. → 메모리 주소에 직접 쓰는 것만으로 디스크에 있는 파일에 기록할 수 있음 4.3.1 mmap() mmap()을 호출하면 fd가 가리키는 파일의 offset 위치에서 len 바이트만큼 메모리에 맵핑하도록 커널에 요청한다.#include &amp;lt;sys/mman.h&amp;gt;void * mmap (void *addr, size_t len, int prot, int flags, int fd, off_t offset); addr addr가 포함되면 메모리에서 해당 주소를 선호한다고 커널에 알려줌 그저 힌트일 뿐이며 대부분 0을 넘겨줌 len fd 가 가리키는 파일의 offset 위치에서 len 바이트만큼 메모리에 맵핑하도록 커널에 요청함. prot 접근권한을 지정 맵핑에 원하는 메모리 보호 정책을 명시PROT_NONE: 접근 불가PROT_READ: 읽기 가능PROT_WRITE: 쓰기 가능PROT_EXEC: 실행 가능 flag 맵핑의 유형과 그 동작에 관한 몇 가지 요소를 명시 MAP_FIXED : mmap()의 addr 인자를 힌트가 아니라 요구사항으로 취급하도록 함 MAP_PRIVATE : 맵핑이 공유되지 않음을 명시. 파일은 copy-on-write 로 맵핑됨. MAP_SHARED : 같은 파일을 맵핑한 모든 프로세스와 맵핑을 공유 MAP_SHARED와 MAP_PRIVATE를 함께 지정하면 안됨. 반환 메모리 맵핑의 실제 시작 주소를 반환한다. fd를 맵핑하면 해당 파일의 참조 카운터가 증가한다. → 따라서 파일을 맵핑한 후에 fd를 닫더라도 프로세스는 여전히 맵핑된 주소에 접근할 수 있다. 예시 fd가 가리키는 파일의 첫 바이트부터 len 바이트까지를 읽기 전용으로 맵핑한다. void *p;p = mmap (0, len, PROT_READ, MAP_SHARED, fd, 0);if (p == MAP_FAILED) perror (&quot;mmap&quot;); mmap() 에 전달하는 인자가 맵핑하는 과정 페이지 크기 페이지는 메모리 관리 유닛 (MMU)에서 사용하는 최소 단위이다. 별도의 접근 권한과 동작 방식을 따르는 가장 작은 메모리 단위라고 할 수 있음. 메모리 맵핑을 구성하는 블록이자 프로세스 주소 공간을 구성하는 블록 mmap() 시스템 콜은 페이지를 다루기 때문에 addr과 offset 인자는 페이지 크기 단위(페이지 크기의 정수배)로 정렬되어야 한다. 만약 len인자가 페이지 크기 단위로 정렬되지 않았다면 다음 크기의 페이지 정수배로 확장됨 마지막 유효 바이트와 맵핑의 끝 사이에 추가된 메모리는 0으로 채워짐 페이지 크기를 얻을 수 있는 표준 메서드는 sysconf()이다. #include &amp;lt;unistd.h&amp;gt;long sysconf (int name); POSIX는 페이지 크기를 바이트 단위로 _SC_PAGESIZE (or _SC_PAGE_SIZE) 로 정의함. 런타임의 페이지 크기를 구하는 방법은 아래와 같다. long page_size = sysconf (_SC_PAGESIZE); 리눅스는 바이트 단위의 페이지 크기를 반환하는 getpagesize()를 제공함 #include &amp;lt;unistd.h&amp;gt;int getpagesize (void); int page_size = getpagesize (); PAGE_SIZE 매크로를 통해서도 페이지 크기를 구할 수 있는데, 런타임이 아닌 컴파일 시점에 시스템의 페이지 크기를 가져온다. int page_size = PAGE_SIZE; 반환값과 에러 mmap() 호출이 성공하면 맵핑된 주소를 반환한다. 실패하면 MAP_FAILED(-1) 를 반환하고 errno를 설정한다. 절대 0을 반환하지 않음.관련 시그널 SIGBUS 프로세스가 더 이상 유효하지 않은 맵핑 영역에 접근하려고 할 때 발생함. 맵핑된 후에 파일이 잘렸을 경우에 이 시그널이 발생함. SIGSEGV 프로세스가 읽기 전용으로 맵핑된 영역에 쓰려고 할 때 발생 4.3.2 munmap() mmap()으로 생성한 맵핑을 해제하기 위한 munmap() 시스템 콜을 제공함.#include &amp;lt;sys/mman.h&amp;gt;int munmap (void *addr, size_t len); 페이지 크기로 정렬된 addr에서 시작해서 len 바이트만큼 이어지는 프로세스 주소 공간에 존재하는 페이지를 포함하는 맵핑을 해제함. 맵핑 해제하고 다시 접근하면 SIGSEGV 시그널이 발생함. 성공 시 0을 반환, 실패 시 -1을 반환하고 errno 설정 예제 [addr, addr+len] 사이에 포함된 페이지를 담고 있는 메모리 영역에 대한 맵핑을 해제함. if (munmap (addr, len) == −1) perror (&quot;munmap&quot;); 4.3.3 맵핑 예제#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;sys/mman.h&amp;gt;// 인자로 파일 이름을 받음int main (int argc, char *argv[]){ struct stat sb; off_t len; char *p; int fd; if (argc &amp;lt; 2) { fprintf (stderr, &quot;usage: %s &amp;lt;file&amp;gt;\\n&quot;, argv[0]); return 1; } fd = open (argv[1], O_RDONLY); // 인자로 넘겨받은 파일을 연다 if (fd == −1) { perror (&quot;open&quot;); return 1; } if (fstat (fd, &amp;amp;sb) == −1) { // fstat : 주어진 파일에 대한 정보 반환 perror (&quot;fstat&quot;); return 1; } if (!S_ISREG (sb.st_mode)) { // 주어진 파일이 디바이스 파일이나 디렉터리가 아닌 일반 파일인지 점검 fprintf (stderr, &quot;%s is not a file\\n&quot;, argv[1]); return 1; } p = mmap (0, sb.st_size, PROT_READ, MAP_SHARED, fd, 0); // 맵핑 수행 if (p == MAP_FAILED) { perror (&quot;mmap&quot;); return 1; } if (close (fd) == −1) { perror (&quot;close&quot;); return 1; } for (len = 0; len &amp;lt; sb.st_size; len++) putchar (p[len]); if (munmap (p, sb.st_size) == −1) { perror (&quot;munmap&quot;); return 1; } return 0;}4.3.4 mmap()의 장점 read()와 write() 시스템 콜을 사용하는 것보다 mmap()을 이용해서 파일을 조작하는 것이 좀 더 유용하다. read, write 시스템 콜 사용할 때 발생하는 불필요한 복사를 방지할 수 있음. 사용자 영역의 버퍼로 데이터를 읽고 써야 하기 때문에 추가적인 복사가 발생함. (잠재적인 페이지 폴트 가능성을 제외하면) 시스템 콜 호출이나 컨텍스트 스위칭 오버헤드가 발생하지 않음 여러 개의 프로세스가 같은 객체를 메모리에 맵핑한다면 데이터는 모든 프로세스 사이에서 공유된다. lseek() 같은 시스템 콜을 사용하지 않고도 맵핑영역 탐색 가능4.3.5 mmap()의 단점 메모리 맵핑은 항상 페이지 크기의 정수배만 가능하다. 메모리 맵핑은 반드시 프로세스의 주소 공간에 딱 맞아야한다. 다양한 사이즈의 맵핑이 있다면 파편화가 일어남 메모리 맵핑과 관련 자료구조를 커널 내부에서 생성, 유지하는데 오버헤드가 발생한다. 이중 복사 제거 방법으로 방지할 수 있음 읽기 요청마다 표준 입출력 버퍼를 가리키는 포인터를 반환하는 대체 구현을 통해 데이터를 표준 입출력 버퍼에서 직접 읽을 수 있음 → 불필요한 복사 피함 4.3.6 맵핑 크기 변경하기 리눅스는 주어진 메모리 맵핑 영역의 크기를 확장하거나 축소하기 위한 mremap() 시스템 콜을 제공함.#define _GNU_SOURCE#include &amp;lt;sys/mman.h&amp;gt;void * mremap (void *addr, size_t old_size, size_t new_size, unsigned long flags); mrepap()은 [addr, addr + old_size) 에 맵핑된 영역을 new_size 만큼의 크기로 변경한다. flag 0 MREMAP_MAYMOVE : 크기 변경 요청을 수행하는데 필요하다면 맵핑의 위치를 이동해도 괜찮다고 커널에 알려준다. 맵핑 위치를 이동시킬 수 있다면 큰 크기 변경 요청이 성공할 가능성이 높아짐 성공 시 조정된 메모리 맵핑의 시작 주소를 반환함. 실패할 경우 MAP_FAILED 를 반환하며 errno 를 설정 예제 glibc 같은 라이브러리는 malloc()으로 할당한 메모리의 크기를 변경하기 위한 realloc()을 효율적으로 구현하기 위해 mremap()을 자주 사용함 void * realloc (void *addr, size_t len){ size_t old_size = look_up_mapping_size (addr); void *p; p = mremap (addr, old_size, len, MREMAP_MAYMOVE); if (p == MAP_FAILED) return NULL; return p;} 4.3.7 맵핑의 보호 모드 변경하기 POSIX는 기존 메모리 영역에 대한 접근 권한을 변경할 수 있는 mprotect() 인터페이스를 저으히함#include &amp;lt;sys/mman.h&amp;gt;int mprotect (const void *addr, size_t len, int prot); [addr, addr+len) 영역내에 포함된 메모리 페이지의 보호 모드를 변경한다. prot mmap()에 사용한 prot 와 같은 값을 사용할 수 있다. 즉, 메모리 영역이 읽기가 가능한 상태에서 prot로 PROT_WRITE를 설정한다면 쓰기만 가능해짐! 어떤 시스템에서는 mmap()으로 생성한 메모리 맵핑에 대해서만 mprotect()를 쓸 수 있지만, 리눅스에서는 어떤 메모리 영역에도 사용할 수 있다. 성공 시 0반환, 실패 시 -1 반환하고 errno 설정4.3.8 파일과 맵핑의 동기화 POSIX 는 2장에서 살펴본 fsync() 시스템 콜의 메모리 맵핑 버전인 msync()를 제공한다.#include &amp;lt;sys/mman.h&amp;gt;int msync (void *addr, size_t len, int flags); msync()는 mmap()으로 맵핑된 파일에 대한 변경 내용을 디스크에 기록하여 파일과 맵핑을 동기화한다. 구체적으로 살펴보면 메모리 주소 addr에서부터 len 바이트 만큼 맵핑된 파일이나 파일 일부를 디스크로 동기화함. 이때 addr 값은 반드시 페이지 크기로 정렬되어야 한다. 보통은 mmap()에서 반환한 값을 사용함 msync()를 호출하지 않으면 맵핑이 해제되기 전까지는 맵핑된 메모리에 쓰여진 내용이 디스크로 반영된다는 보장을 할 수가 없다. 쓰기 과정 중에 갱신된 버퍼를 디스크에 쓰도록 큐에 밀어넣는 write()와는 동작방식이 다름 flag MS_SYNC : 디스크에 모든 페이지를 기록하기 전까지 msync()는 반환하지 않는다. MS_ASYNC : 비동기 방식으로 동기화한다. MS_INVALIDATE : 맵핑의 캐시 복사본을 모두 무효화한다. OR로 명시할 수 있지만, MS_SYNC와 MS_ASYNC 중 하나는 반드시 해야함. (둘을 함께하는 것은 안됨) 예제 [addr, addr+len) 영역에 맵핑된 파일을 디스크로 동기화한다. fsync()에 비해서 10배 빠름 (메모리라서) if (msync (addr, len, MS_ASYNC) == −1) perror (&quot;msync&quot;); 성공하면 0 반환, 실패하면 -1반환하고 errno 설정4.3.9 맵핑의 사용처 알려주기 리눅스는 프로세스가 맵핑을 어떻게 사용할 것인지 커널에 알려주는 madvise() 시스템 콜을 제공한다. 커널이 이를 통해 얻는 힌트를 사용해서 최적화가 가능함. 부하가 걸리는 상황에서 필요한 캐시와 미리 읽기 방식을 확실히 보장할 수 있게 된다.#include &amp;lt;sys/mman.h&amp;gt;int madvise (void *addr, size_t len, int advice); addr 로 시작해서 len 바이트의 크기를 가지는 메모리 맵핑 내의 페이지와 관련된 동작 방식에 대한 힌트를 커널에 제공함. len 0 이라면 커널은 addr에서 시작하는 전체 맵핑에 힌트를 적용한다. advice MADV_NORMAL : 이 메모리 영역에 대한 특별한 힌트를 제공하지 않는다. MADV_RANDOM : 이 영역의 페이지는 랜덤하게 접근한다. MADV_SEQUENTIAL : 이 영역의 페이지는 낮은 주소에서 높은 주소로 순차적으로 접근한다. MADV_WILLNEED : 이 영역의 페이지는 곧 접근한다. MADV_DONTNEED : 이 영역의 페이지는 당분간 접근하지 않는다. POSIX는 힌트에 대한 의미만 정의하고 있다. 리눅스 커널 2.6 버전 부터는 각 힌트에 대해 조금 다르게 대응한다. madvise 예시 [addr, addr + len) 메모리 영역을 순차적으로 접근할 것이라고 커널에 알려줌 int ret;ret = madvise (addr, len, MADV_SEQUENTIAL);if (ret &amp;lt; 0) perror (&quot;madvise&quot;); 성공하면 0을 반환, 실패 시 -1을 반환하고 errno 설정4.4 일반 파일 입출력에 대한 힌트 위에서는 메모리 맵핑을 사용하는데 힌트를 제공하는 방법에 대해서 알아봤음. 4.4 에서는 커널에 일반적인 파일 입출력에 대한 힌트를 제공하는 방법에 대해서 알아본다.4.4.1 posix_fadvise() 시스템 콜#include &amp;lt;fcntl.h&amp;gt;int posix_fadvise (int fd, off_t offset, off_t len, int advice); fd의 [offset, offset + len) 범위에 대한 힌트를 커널에 제공한다. len 0이면 파일 전체인 [offset, 파일 길이] 에 적용된다. len과 offset을 0으로 넘기면 전체 파일에 대한 힌트제공 advise madvise와 유사함. 한 가지 설정만 가능하다. POSIX_FADV_NORMAL : 힌트 제공 안함 POSIX_FADV_RANDOM : 데이터에 랜덤하게 접근 POSIX_FADV_SEQUENTIAL : 낮은 주소에서 높은 주소로 순차적 POSIX_FADV_WILLNEED : 곧 접근 POSIX_FADV_NOREUSE : 한번만 접근 POSIX_FADV_DONTNEED : 당분간 접근안함 madvise와 동일하게 커널이 이런 힌트에 대응하는 방법은 구현에 따라 다른다. (심지어는 커널 버전에 따라 다르게 동작함.) 예제 커널에게 fd가 가리키는 전체 파일에 랜덤하게 접근하겠다고 알려줌 int ret;ret = posix_fadvise (fd, 0, 0, POSIX_FADV_RANDOM);if (ret == −1) perror (&quot;posix_fadvise&quot;); 성공하면 0을 반환, 실패하면 -1 반환하고 errno 설정4.4.2 readahead() 시스템 콜 POSIX_FADV_WILLNEED 힌트와 동일한 동작 방식을 제공하기 위해 사용 리눅스 전용 인터페이스이다.#define _GNU_SOURCE#include &amp;lt;fcntl.h&amp;gt;ssize_t readahead (int fd, off64_t offset, size_t count); fd가 가리키는 파일의 [offset, offset+count) 영역의 페이지 캐시를 생성한다. 성공하면 0 반환, 실패 시 -1반환하고 errno 설정4.4.3 부담 없이 힌트를 사용하자 !! 일반적으로 애플리케이션에서 발생하는 일부 부하는 커널에 힌트를 제공함으로써 쉽게 개선할 수 있음! 힌트는 입출력의 부하를 완화시킨다. 파일 조각을 읽기 전에 POSIX_FADV_WILLNEED(곧 접근) 힌트를 제공하여 커널이 읽으려는 파일을 페이지 캐시에 밀어 넣을 수 있음 입출력은 백그라운드에서 비동기식으로 일어남. 애플리케이션이 최종적으로 파일에 접근하면 입출력을 블록킹하지 않고 원하는 작업을 완료할 수 있다. 많은 데이터를 연속적으로 디스크에 기록하는 경우 POSIX_FADV_DONTNEED(당분간 접근 X) 힌트를 제공하면 파일 조각을 페이지 캐시에서 제거할 수도 있다. 다시 접근하지 않으면, 불필요한 데이터로 가득 차있을 수 있기 때문에 주기적으로 캐시에서 스트림 데이터를 제거하는것이 합리적 파일 전체를 읽을 때는 POSIX_FADV_SEQUENTIAL(순차적) 힌트를 사용해서 커널에 미리읽기를 공격적으로 수행하도록 할 수 있다. 파일을 랜덤하게 접근하거나 파일의 이곳 저곳을 읽어야 한다면 POSIX_FADV_RANDOM(랜덤하게 접근) 힌트를 사용해서 불필요한 미리읽기를 방지할 수 있음4.5 동기화, 동기식, 비동기식 연산 동기식(Synchronous)과 동기화(Synchroinized)는 크게 다르지 않음 동기식 쓰기 연산 동기식 쓰기 연산은 최소한 쓰고자 하는 데이터가 커널의 버퍼 캐시에 기록되기 전까지는 반환되지 않는다. 비동기식 쓰기 연산은 데이터가 사용자 영역에 머무르고 있을지라도 즉시 반환될 수 있다. 읽기 연산 동기식 읽기 연산은 읽고자 하는 데이터가 애플리케이션에서 제공하는 사용자 영역의 버퍼에 저장되기 전까지는 반환되지 않는다. 비동기식 읽기 연산은 읽으려는 데이터가 미처 준비되기도 전에 반환될 수 있다. 비동기식 연산은 나중을 위해 요청을 큐에 넣을 뿐 실제로 요청된 작업을 수행하지 않음! 동기화 연산은 단순 동기식 연산보다 좀 더 제약적이지만 더 안전하다. 동기화 쓰기 연산은 데이터를 디스크에 기록해서 커널 버퍼에 있던 데이터와 디스크에 기록된 데이터가 동기화되도록 보장한다. 동기화 읽기 연산은 항상 데이터의 최신 복사본을 반환하며 이 복사본은 디스크에서 읽어낼 가능성이 높다. → 동기식과 비동기식이라는 용어는 입출력 연산이 반환하기 전에 데이터 저장과 같은 이벤트를 기다리는지의 여부를 나타냄 → 동기화와 비동기화는 데이터를 디스크에 기록하는 것과 같은 정확한 이벤트가 발생해야 함을 나타냄 보통 유닉스의 쓰기 연산은 동기식이자 비동기화 연산임 특징들의 모든 가능한 조합으로 동작이 가능함| | 동기화 | 비동기화 || ——– | ————————————————————————————— | ——————————————————————————————————– || 동기식 | 데이터를 디스크에 다 비우기 전에는 반환되지 않음. O_SYNC 플래그 명시했을 때 이렇게 동작 | 데이터가 커널 버퍼에 저장되기 전까지 반환되지 않음. 일반적인 동작 || 비동기식 | 요청이 큐에 들어가자마자 반환됨. 최종적으로 쓰기 연산이 실행되어야 디스크에 기록된다. | 요청이 큐에 들어가자마자 반환됨. 최종적으로 쓰기 연산이 실행되어야 적어도 데이터가 커널 버퍼에 저장된다. | 읽기 연산은 동기식이면서 동기화 연산이다. 오랜 데이터를 읽는 것이 의미가 없으므로 항상 동기화 방식으로 동작함| | 동기화 || ——– | ——————————————————————————————– || 동기식 | 최신 데이터가 제공된 버퍼로 읽어오기 전에는 반환하지 않는다. 일반적 동작 || 비동기식 | 요청이 큐에 들어가자마자 반환된다. 하지만 최종적으로 연산이 실행되어야 최신 데이터를 반환함. | 4.5.1 비동기식 입출력 비동기식 입출력을 수행하려면 커널의 최하위 레벨에서부터 지원이 필요하다. aio 인터페이스가 정의되어 있으며 리눅스에서 구현하고 있다. 이는 비동기식 입출력을 요청하고 작업이 완료되면 알림을 받는 함수를 제공함.#include &amp;lt;aio.h&amp;gt;/* asynchronous I/O control block */struct aiocb { int aio_fildes; /* file descriptor */ int aio_lio_opcode; /* operation to perform */ int aio_reqprio; /* request priority offset */ volatile void *aio_buf; /* pointer to buffer */ size_t aio_nbytes; /* length of operation */ struct sigevent aio_sigevent; /* signal number and value */ /* internal, private members follow... */};int aio_read (struct aiocb *aiocbp);int aio_write (struct aiocb *aiocbp);int aio_error (const struct aiocb *aiocbp);int aio_return (struct aiocb *aiocbp);int aio_cancel (int fd, struct aiocb *aiocbp);int aio_fsync (int op, struct aiocb *aiocbp);int aio_suspend (const struct aiocb * const cblist[], int n, const struct timespec *timeout);4.6 입출력 스케줄러와 성능 디스크 성능을 가장 떨어뜨리는 부분은 seek 이라고 하는 하드 디스크에서 데이터를 읽고 쓰는 헤드를 이동시키는 과정이다. 프로세스의 사이클 하나보다 25,000,000배나 더 오래 걸리는 시간. 입출력 요청을 순서대로 디스크로 보내는 방식은 비효율적임 → 입출력 스케줄러를 통해서 디스크 탐색 횟수를 최소화 함.4.6.1 디스크 주소 지정 방식 하드 디스크는 실린더, 헤드, 섹터 또는 CHS 주소 지정방식을 사용함 하드 디스크는 플래터 여러 장으로 구성되어 있으며, 각 플래터는 하나의 디스크, 스핀들, 그리고 read/write 헤더로 구성되어 있다. 플래터를 CD로 생각할 수 있다 각각의 플래터는 CD 처럼 원형의 트랙으로 나뉘어져 있다. 그 트랙들은 정수 개의 섹터로 나뉘어져 있음 특정 데이터가 저장되어 있는 디스크의 위치를 찾을 때 하드 디스크는 실린더, 헤드, 섹터 값을 필요로 함. 어떤 플래터의 어느 트랙, 어느 섹터에 데이터가 있는지 알아야 함 실린더 값 : 데이터가 위치한 트랙을 나타냄 헤드 값 : 요청한 읽기/쓰기 헤드(정확한 플래터)의 정확한 값을 구분함 섹터 값 : 트랙에 위치한 정확한 섹터 요즘 HD는 유일한 블록 번호를 맵핑해서 하나의 블록이 특정 섹터에 대응되도록 한다. 반면 파일 시스템은 소프트웨어로만 존재함. 논리 블록이라는 독자적인 단위를 사용해서 동작함. 파일 시스템의 논리 블록은 디스크의 하나 이상의 물리 블록에 맵핑되어 있다. 4.6.2 입출력 스케줄러의 동작 방식 입출력 스케줄러는 병합과 정렬이라는 두 가지 기본 동작을 수행한다. 병합 둘 이상의 인접한 입출력요청을 단일 요청으로 합치는 과정 예시 : 하나는 5번을 읽으려 하고, 하나는 6,7번까지 읽으려고 할 때 합쳐서 수행함. 연산 횟수는 절반으로 줄어듬 정렬 대기 중인 입출력 요청을 블록 순서의 오름차순으로 정렬하는 것이다. 예시 : 52, 109, 7에 대한 입출력 연산이 들어오면 입출력 스케줄러는 이 요청을 7, 52, 109 순서대로 정렬함. 만약 81번 블록에 대한 새로운 요청이 들어오면 52번과 109번 연산 요청 사이에 끼워넣음 선형적인 방법으로 부드럽게 이동시킬 수 있게 하여서 디스크의 헤드 움직임을 최소화 한다. 4.6.3 읽기 개선 읽기 요청은 반드시 최신 데이터를 반환해야 함 따라서 요청한 데이터가 페이지 캐시에 존재하지 않으면 디스크에서 데이터를 읽어올 때까지 블록되어야 하며 시간이 오래 걸릴 수 있음 → 읽기 Latency 라고 한다. 읽기의 경우 나중에 들어온 요청은 앞선 요청의 완료에 의존적이다. 이에 반해 쓰기 요청은 디스크 성능에 방해가 되지 않는 스트림을 사용하는데, 이는 커널과 디스크의 주의를 독차지 할 수 있다. → 이렇게 되면 읽기 문제가 복잡해지는데 이를 “Writes-starving-reads problem”이라고 한다 만약 입출력 스케줄러가 항상 요청이 들어온 순서에 따라 새로운 요청을 끼워 넣는다면 멀리 떨어진 블록에 대한 요청을 무기한으로 굶겨 죽일 수 있음. 리누스 엘리베이터 같은 단순한 접근 방식은 큐에 충분히 오래된 요청이 있다면 삽입-정렬 기능을 멈춘다. 전체 성능을 희생하여 요청에 대한 공정석을 유지하고 읽기 요청인 경우 레이턴시를 개선한다. 문제는 이 휴리스틱이 너무 단순하다는 것 데드라인 입출력 스케줄러 전통적인 엘리베이터 알고리즘의 일반적인 문제를 해결하기 위해 도입되었다. 리누스 엘리베이터는 대기 중인 입출력 요청을 정렬된 목록(큐)으로 유지한다. 데드라인 입출력 스케줄러는 이 큐를 유지하고 읽기 FIFO 큐와 쓰기 FIFO 큐라는 두 가지 추가 큐를 도입해서 문제를 해결한다. 각 큐에 들어있는 각 요청은 만료기간이 할당되어 있음. 읽기 500밀리초, 쓰기 5초 새로운 입출력 요청이 들어오면 표준 큐에 삽입-정렬되고, 읽기 or 쓰기 FIFO 큐의 끝 부분에 위치한다. 일반적으로 표준 큐가 블록 번호로 정렬되어 있으므로 탐색을 최소화하여 전체 처리량을 최대로 높임 만약 읽기 쓰기 FIFO 큐 앞부분에 있는 아이템이 해당 표준 큐의 만료기간보다 오래되면 입출력 스케줄러는 포준 큐에서 입출력 요청을 처리하지 않고 해당 FIFO 큐에서 요청을 처리하기 시작함. 입출력 요청에 대해서 말랑한 데드라인을 강제한다. 비록 만료전에 처리된다고 보장할 수는 없지만, 이반적으로 거의 요청 만료시간 안에 처리함. 읽기 요청의 만료시간이 좀 더 짧기 떄문에 쓰기가 읽기를 굶겨 죽이는 문제도 최소화 한다.예측 입출력 스케줄러 데드라인 입출력 스케줄러의 문제점 연속된 읽기 요청이 계속 들어올 경우, 정렬된 큐의 요청을 처리하기 위해서 앞뒤로 계속 왔다 갔다함. 새로운 읽기 요청은 앞선 요청이 반환되어야만 처리되는데, 그렇게 되면 데이터를 읽어서 서비스 하는 데 한번, 다시 되돌리는데 한번해서 총 두번의 탐색을 낭비함. 위의 문제점들을 해결하기 위해서 예측 입출력 스케줄러는 데드라인 입출력 스케줄러에다가 예측 매커니즘을 추가하였다. 예측 입출력 스케줄러는 읽기 요청이 들어오면 평소처럼 만료시간 내에 처리한다. 하지만 요청을 처리하고 아무것도 하지 않고 6밀리 초까지 기다림. → 6밀리 초는 애플리케이션이 파일 시스템의 동일한 부분에 대한 새로운 읽기를 요청할 충분한 시간이다. 6밀리 초 까지 요청이 없다면 예측이 잘못되었음을 인정하고 이전 작업 내용을 반환한다. 대부분의 읽기는 의존적이므로 에측을 통해 시간을 많이 아낄 수 있음CFQ 입출력 스케줄러 Complete Fair Queuing 프로세스마다 독자적인 큐를 할당하고, 각 큐는 시간을 할당받는다. Round Robin 방식으로 각 큐를 순회하면서 큐의 허락된 시간이 다 지나거나, 요청이 남아 있지 않을 때 큐에 있는 요청을 처리함. 시간이 남았지만, 더이상 요청이 큐에 없다면 CFQ 스케줄러는 짧은 시간 동안 (default = 10밀리초) 그 큐의 새로운 요청을 기다림. 예측이 맞으면 탐색을 피하고, 틀리면 다음 프로세스의 큐로 간다. 프로세스의 개별 큐 안에서 동기화된 요청(읽기 요청…) 은 동기화되지 않은 요청보다 더 높은 우선순위를 가짐. → 읽기 요청을 배려해서 쓰기 요청이 읽기를 굶겨 죽이는 문제를 회피한다. 대부분의 업무 부하에 적합하며 가장 먼저 고려해볼 만하다.Noop 입출력 스케줄러 가장 기본적인 스케줄러 정렬을 수행하지 않고 병합만 수행함. → 정렬할 필요가 없거나, 정렬을 하지 않는 장치에 특화된 스케줄러 4.6.4 입출력 스케줄러 선택과 설정 기본 입출력 스케줄러는 부팅 시 커널 명령행 인자인 iosched 를 통해서 선택할 수 있다. 유효한 값으로는 cfq, deadline, noop 이 있다. 실행 중에도 각 장치에 대해 /sys/block/[device]/queue/scheduler 값을 변경해서 선택할 수 있음 device : 블록 디바이스를 의미 입출력 스케줄러 설정 예시 # echo cfq &amp;gt; /sys/block/hda/queue/scheduler 4.6.5 입출력 성능 최적화 디스크 입출력은 많이 느리기 떄문에 성능 극대화는 매우 중요함 여러가지 기법들 자잘한 연산을 묶어 몇 개로 합쳐서 연산 최소화 하기 입출력을 블록 크기에 정렬되도록 수행하기 사용자 버퍼링을 사용하기 벡터 입출력 위치를 지정한 입출력 비동기식 입출력 사용자 영역에서 입출력 스케줄링하기 엄청난 입출력을 처리해야 하는 애플리케이션은 입출력 요청을 정렬하고 병합해서 조금이라도 더 성능을 높여야함. (입출력이 많지 않은 애플리케이션에서 정렬하는 것은 어리석은 짓) 만약 입출력 요청이 계속 들어오고 있는 상황에서 중간에 정렬을 하는 것은 비효율적이다. 따라서 요청을 제출하기 전에 정렬을 해주면 원하는 순서대로 수행이 가능함.경로로 정렬하기 파일 경로로 정렬하는 방법은 가장 쉽지만, 효과는 적은 방법이다. (블록 단위 정렬을 흉내내는 방식) 대부분의 파일시스템의 배치 알고리즘에 의해 디렉터리 내의 파일 혹은 부모 디렉터리를 공유하는 디렉터리들은 디스크에서 인접하는 경향이 있음. → 파일의 물리적인 위치를 얼추 비슷하게 맞출 수 있다. 장점 적어도 모든 파일 시스템에 적용 가능한 방법 일시적인 지역성 덕분에 중간 정도의 정확도를 기대할 수 있음. 구현하기 쉬움 단점 파편화를 고려하지 않았음 inode로 정렬하기 inode 는 개별 파일과 관련된 메타데이터를 담고 있는 유닉스의 구성 요소이다. 파일의 데이터가 물리 디스크 블록을 여러개 점유하고 있다고 해도, 하나의 inode만을 가짐. inode 는 유일한 번호가 할당됨.파일 i의 inode 번호 &amp;lt; 파일 j의 inode 번호==파일 i의 물리블록 &amp;lt; 파일 j의 물리블록 inode의 번호는 stat() 시스템 콜을 통해서 얻을 수 있음 주어진 파일의 inode 번호 출력 프로그램 예시 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;/* * get_inode - returns the inode of the file associated * with the given file descriptor, or −1 on failure */int get_inode (int fd){ struct stat buf; int ret; ret = fstat (fd, &amp;amp;buf); if (ret &amp;lt; 0) { perror (&quot;fstat&quot;); return −1; } return buf.st_ino;}int main (int argc, char *argv[]){ int fd, inode; if (argc &amp;lt; 2) { fprintf (stderr, &quot;usage: %s &amp;lt;file&amp;gt;\\n&quot;, argv[0]); return 1; } fd = open (argv[1], O_RDONLY); if (fd &amp;lt; 0) { perror (&quot;open&quot;); return 1; } inode = get_inode (fd); printf (&quot;%d\\n&quot;, inode); return 0;} inode 정렬 장점 inode번호는 쉽게 얻을 수 있고 정렬도 쉬움 물리적인 파일 배치를 추측할 수 있는 좋은 지표 단점 파편화에 따라 추측이 틀릴 수 있음 유닉스 파일 시스템이 아닌 경우 정확도가 떨어짐 사용자 영역에서 입출력 요청을 스케줄링하기 위해서 가장 흔히 사용되는 방법물리 블록으로 정렬하기 최적의 방법은 물리적인 디스크 블록으로 정렬하는 것임 각 파일은 파일 시스템에서 가장 작은 할당 단위인 논리 블록 단위로 쪼개짐. (논리 블록 크기는 파일 시스템 마다 다르다.) 각각의 논리 블록은 하나의 물리 블록에 맵핑되어 있다. 커널은 파일의 논리 블록에서 물리 디스크 블록을 알아내는 메서드를 제공한다. ret = ioctl (fd, FIBMAP, &amp;amp;block);if (ret &amp;lt; 0) perror (&quot;ioctl&quot;); block 찾고 싶은 물리 블록에 대한 논리 블록 block은 0부터 시작하는 파일에 상대적인 값. 성공하면 block 은 물리 블록 번호로 바뀐다. 논리 블록과 물리 블록의 맵핑을 찾으려면 2단계가 필요함. 주어진 파일의 블록 개수를 구함 stat() 시스템 콜로 구할 수 있다. 각 논리 블록을 가지고 ioctl()을 통해 이에 상응하는 물리 블록을 구한다. 예제 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;stdlib.h&amp;gt;#include &amp;lt;fcntl.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;sys/stat.h&amp;gt;#include &amp;lt;sys/ioctl.h&amp;gt;#include &amp;lt;linux/fs.h&amp;gt;/* * get_block - for the file associated with the given fd, returns * the physical block mapping to logical_block */int get_block (int fd, int logical_block){ int ret; ret = ioctl (fd, FIBMAP, &amp;amp;logical_block); if (ret &amp;lt; 0) { perror (&quot;ioctl&quot;); return −1; } return logical_block;}/* * get_nr_blocks - returns the number of logical blocks * consumed by the file associated with fd */int get_nr_blocks (int fd){ struct stat buf; int ret; ret = fstat (fd, &amp;amp;buf); if (ret &amp;lt; 0) { perror (&quot;fstat&quot;); return −1; } return buf.st_blocks;}/* * print_blocks - for each logical block consumed by the file * associated with fd, prints to standard out the tuple * &quot;(logical block, physical block)&quot; */void print_blocks (int fd){ int nr_blocks, i; nr_blocks = get_nr_blocks (fd); if (nr_blocks &amp;lt; 0) { fprintf (stderr, &quot;get_nr_blocks failed!\\n&quot;); return; } if (nr_blocks == 0) { printf (&quot;no allocated blocks\\n&quot;); return; } else if (nr_blocks == 1) printf (&quot;1 block\\n\\n&quot;); else printf (&quot;%d blocks\\n\\n&quot;, nr_blocks); for (i = 0; i &amp;lt; nr_blocks; i++) { int phys_block; phys_block = get_block (fd, i); if (phys_block &amp;lt; 0) { fprintf (stderr, &quot;get_block failed!\\n&quot;); return; } if (!phys_block) continue; printf (&quot;(%u, %u) &quot;, i, phys_block); } putchar (&#39;\\N&#39;);}int main (int argc, char *argv[]){ int fd; if (argc &amp;lt; 2) { fprintf (stderr, &quot;usage: %s &amp;lt;file&amp;gt;\\n&quot;, argv[0]); return 1; } fd = open (argv[1], O_RDONLY); if (fd &amp;lt; 0) { perror (&quot;open&quot;); return 1; } print_blocks (fd); return 0;} 장점 정확히 정렬하고 싶은 대상인 파일이 실제 존재하는 물리 디스크 블록을 반환한다 단점 root 권한이 필요함. ioctl의 FIBMAP이 root 권한이 필요한 CAP_SYS_RAWIO 기능을 요구함 " }, { "title": "[Linux System Programming] Ch03 버퍼 입출력 ", "url": "/posts/linux_ch03/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-13 00:00:00 +0900", "snippet": "[Ch03 버퍼 입출력] 블록 : 파일 시스템의 최소 저장 단위를 나타내는 추상 개념.파일 시스템 연산은 블록 단위로 일어난다. (데이터에 필요한 블록이 4.5개라 하더라도 5개를 써야함.)→ 블록의 일부분만 다루는 연산이 비효율적임.3.1 사용자 버퍼 입출력 일반 파일에 대해 잦은 입출력을 처리해야만 하는 프로그램은 종종 사용자 버퍼 입출력을 수행한다. 이는 커널이 아니라 사용자 영역에서 버퍼링을 처리한다는 의미 커널은 내부적으로 지연된 쓰기연산, 미리읽기, 연속된 입출력 요청 을 모아서 처리하는 방식으로 버퍼링을 구현하고 있음. 일반 파일에 대해 잦은 입출력을 처리해야 하는 프로그램은 종종 사용자 버퍼 입출력을 수행한다.dd bs=1 count=2097152 if=/dev/zero of=pirate2MB 데이터를 1B씩 약 2백만 번에 걸쳐 읽어들임dd bs=1024 count=2048 if=/dev/zero of=pirate2MB 데이터를 1KB씩 약 2천 번에 걸쳐 읽어들임 블록 크기 (Byte) 실제 시간 (초) 사용자 시간 (초) 시스템 시간 (초) 1 18.707 1.118 17.549 1024 0.025 0.002 0.023 1130 0.035 0.002 0.027 1KB 단위로 읽어들이면 시스템콜의 횟수를 1024배 줄임으로써 성능을 비약적으로 개선할 수 있다.다만 블록 크기를 1130 Byte로 키우면 시스템콜의 횟수는 줄지만 실제 물리 블록의 크기의 약수나 배수가 아니므로 성능 저하가 발생한다. 실제로 /dev/zero의 경우 블록 크기는 4096 Byte다.3.1.1 블록크기 실제로 블록 크기는 보통 512, 1024, 2048, 4096 혹은 8192로 정해진다. → 커널과 하드웨어는 블록 크기를 기준으로 대화하기 때문에 블록 크기의 정수배나 약수 단위로 연산을 수행하기만 해도 상당한 성능 개선이 따라옴 그렇다면 모든 데이터를 4KB or 8KB단위로 취급하는게 좋은가? No. 실제로 데이터를 블록 단위로 취급하는 프로그램이 드물기에 현실성이 없음 프로그램은 블록 같은 추상 개념이 아니라 필드, 행, 단일 문자를 다룬다. 그래서 사용자 버퍼 입출력이 필요함.데이터가 쓰여지면 프로그램 **주소 공간 내 버퍼**에 저장이 됨.버퍼가 특정 크기에 도달하면 전체 버퍼는 **한 번의 쓰기 연산을 통해 실제로 기록이 됨.**읽기 또한 버퍼 크기에 맞춰 **블록에 정렬된 데이터를 읽는다.**→ 데이터가 많더라도 모두 블록 크기에 맞춰 적은 횟수의 시스템 콜만 사용하게 됨. 성능 향상! 사용자 애플리케이션 코드 레벨에서 인위적으로 버퍼링을 구현해서 사용해야함.But, 표준 입출력 라이브러라 (stdio)와 표준 C++ iostream이라는 견고하고 뛰어난 사용자 버퍼링 구현체를 가져다 사용하면 된다!3.2 표준 입출력 표준 C 라이브러리는 표준 입출력 라이브러리 (stdio)를 제공함3.2.1 파일 포인터 표준 입출력 루틴은 File Descriptor를 직접 다루지 않고, File pointer라는 독자적인 식별자를 사용한다. 표준 입출력 용어로 열린 파일은 Stream 이라고 부르기도 함. Stream 은 읽기(입력 스트림), 쓰기 (출력 스트림), 또는 읽기/쓰기 (입출력 스트림) 모드로 열 수 있음 3.3 파일 열기 파일을 읽거나 쓰기 위해서 fopen()을 사용한다. (FILE은 stdio.h 에 정의된 FILE typedef)FILE * fopen (const char *path, const char *mode)// EXFILE *stream;stream = fopen (&quot;/etc/manifest&quot;, &quot;r&quot;);if (!stream) ERROR 파일 path를 mode에 따라 원하는 용도로 새로운 스트림을 만든다. 성공 시 유효한 FILE 포인터를 반환. 실패 시 NULL 반환, errno 설정3.3.1 모드 r : 읽기 목적으로 파일을 엶. r+: 읽기/쓰기 목적. 스트림은 파일 시작 지점. w : 쓰기 목적으로 파일을 엶. 파일이 이미 존재하면 길이를 0으로 잘라버림. 파일이 존재하지 않으면 새로 만듬. w+: 읽기/쓰기 목적. 파일이 이미 존재하면 길이를 0으로 자름. 파일이 존재하지 않으면 새로 만듦. 스트림은 파일 시작 지점. a : 덧붙이기 상태에서 쓰기 목적으로 파일을 엶. a+: 덧붙이기 상태에서 읽기/쓰기 목적으로 파일을 엶. 파일이 존재하지 않으면 새로 만듦. 스트림은 파일 끝 지점.3.4 파일 디스크립터로 스트림 열기 fdopen() 함수는 이미 열린 파일 디스크립터를 통해 스트림을 만든다.FILE * fdopen (int fd, const char *mode); 사용가능한 mode는 fopen()과 동일하며, 원래 fd를 열 때 사용했던 모드와 호환성을 유지해야 한다. fopen()에서는 w모드로 스트림을 열었을 때 이미 존재한다면 파일을 0으로 잘라버렸음. 하지만 fdopen()은 그렇지 않은데 그 이유는 이미 파일이 fd에 대해서 열려있기 때문. 따라서 open() 함수에 의해 반환 받은 fd를 fdopen() 함수에서 받았을 경우 open() 함수에 O_TRUNC 플래그가 있어야만 파일을 자를 수 있음! fd가 스트림으로 변환되면 그 fd를 통해 직접 입출력을 수행이 가능하긴 하지만 그렇게 하면 안됨!3.5 스트림 닫기int fclose (FILE *stream) 버퍼에 쌓여있지만 아직 스트림에 쓰지 않은 데이터를 먼저 처리함. fclose 하면 fd까지 닫히나? 성공하면 0 반환, 실패하면 EOF 반환하고 errno 적절한 값으로 설정3.5.1 모든 스트림 닫기int fcloseall (void) fcloseall() 함수는 stdin, sdtout, stderr 를 포함해서 현재 프로세스와 관련된 모든 스트림을 닫는다. 닫기 전에 버퍼에 남아 있는 데이터는 모두 스트림에 쓰여지며 언제나 0을 반환3.6 스트림에서 읽기 스트림에서 데이터를 읽으려면 w나 a를 제외한 나머지 모드(읽기 가능 모드)로 스트림을 열어야 함3.6.1 한 번에 한 문자씩 읽기int fgetc(FILE *stream) stream 에서 다음 문자를 읽고 unsigned char 타입을 int 타입으로 변환해서 반환한다. 타입 변환 이유 : 파일 끝이나 에러를 알려줄 수 있도록 하기 위함. 이런 에러일 때는 EOF반환 반드시 반환 값이 int 타입이어야 한다. char타입으로 저장하게 되면 에러 확인이 불가능함! int c;c = fgetc (stream);if (c == EOF) // errorelse printf()읽은 문자 되돌리기 스트림을 찔러보고 원하는 문자가 아닌 경우 되돌려버린다. 즉, 스트림에 문자를 다시 집어넣는 것임.int ungetc (int c, FILE *stream)// 여러번 호출 시 역순으로 출력. LIFO (Last In First Out)// 파일에 직접 쓰여지는 것이 아니라 버퍼에 쓰여지게 됨// 리눅스에서는 메모리가 허용하는 범위 내에서 무제한 되돌리기 허용ungetc(&#39;a&#39;, stream);ungetc(&#39;b&#39;, stream);ch = getc(fp); // ch 에는 b 가 들어간다.ch = getc(fp); // ch 에는 a 가 들어간다.// 중간에 파일 위치 표시자의 값이 0이 된다면 그 이후에 호출된 unget함수들은 모두 무시됨.fp = fopen(&quot;test.txt&quot;, &quot;r&quot;);getc(fp); // 이 함수 호출 이후 위치 표시자의 값은 1ungetc(&#39;a&#39;, fp); // 이 함수 호출 이후 값은 0ungetc(&#39;b&#39;, fp); // 따라서 버퍼에 b 가 들어갈 수 없다.ch = getc(fp); // ch 에는 a 가 들어간다.printf(&quot;%c&quot;, ch);ch = getc(fp); // ch 에는 test.txt 의 두 번째 문자가 들어간다. ungetc()를 호출하고 중간에 탐색함수를 호출했고, 읽기 요청은 아직 하지 않았을 경우 되돌린 문자를 다 버린다. 스레드는 버퍼를 공유하므로 단일 프로세스에서 여러 스레드가 동작하는 경우에도 동일한 현상 발생 3.6.2 한 줄씩 읽기 fgets() 함수는 stream에서 문자열을 읽는다.char *fgets (char *str, int size, FILE *stream) stream에서 size보다 하나 적은 내용을 읽어서 결과를 str에 저장한다. 마지막 바이트를 읽고 난 다음, 버퍼 마지막에 null 문자 (\\0)을 저장한다. EOF나 개행문자를 만나면 읽기 중단. 개행문자를 읽으면 str에 \\n을 저장 무조건 \\0은 마지막에 넣음. 문자열은 마지막에 NUll로 끝남 성공하면 str을 반환, 에러일 경우 NULL 반환원하는 만큼 문자열 읽기 행 단위로 읽는 방법은 유용하지만 다른 구분자를 사용하고 싶을 때도 있음 fgetc 로 fgets와 동일한 로직을 구현할 수 있다char *s;int c;s = str// n-1 바이트를 읽어서 str에 저장while (--n &amp;gt; 0 &amp;amp;&amp;amp; (c = fgets (stream)) != EOF) *s++ = c;// \\0 을 추가*s = &#39;\\0&#39;;// d를 \\n으로 하면 fgets와 동일while (--n &amp;gt; 0 &amp;amp;&amp;amp; (c = fgec (stream) != EOF &amp;amp;&amp;amp; (*s++ = c) != d) ;if (c==d) *--s = &#39;\\0&#39;;else *s = &#39;\\0&#39;;3.6.3 바이너리 데이터 읽기 개별 문자나 행을 읽는 기능만으로 부족할때 (C 구조체 같은 복잡한 바이너리 데이터를 읽고 써야하는 경우) fread()함수 사용size_t fread( void *buf, size_t size, size_t nr, FILE *stream) stream에서 각각 크기가 size 바이트인 엘리먼트를 nr개 읽어서 buf가 가리키는 버퍼에 저장한다. 읽어들인 엘리먼트 개수가 반환됨. nr보다 적은 값을 반환하여 실패나 EOF를 반환 ferror() or feof()를 사용하지 않고서는 실패 or EOF를 알 수가 없음 변수의 크기, 정렬, 채워넣기, 바이트 순서가 다르기 때문에 어떤 애플리케이션에서 기록한 바이너리 데이터를 다른 앱에서는 못 읽을 수도 있다.정렬문제 모든 아키텍처는 데이터 정렬 요구사항을 가지고 있음. 프로세스는 바이트 크기 단위로 메모리를 읽고 쓰지 않고, 2,4,8,,, 바이트처럼 정해진 기본 단위로 메모리에 접근함. → 기본 단위의 정수배로 시작하는 주소에 접근해야함 따라서 C언어에서 변수는 반드시 정렬된 주소에 저장하고 접근해야함. 예를 들어 32비트 정수는 4바이트 경계에 맞춰 정렬됨. → int는 4로 나누어 떨어지는 메모리 주소 공간에 저장된다. 정렬되지 않은 데이터 접근에 대해서는 다양한 패널티가 존재한다. 접근 가능 but 성능 저하 접근 허용 X, 하드웨어 예외로 처리 강제 정렬을 위해 하위 비트를 제거해버림 3.7 스트림에 쓰기3.7.1 한 번에 문자 하나만 기록하기 fgetc()에 대응하는 쓰기 함수는 fputc()이다.int fputc(int c, FILE *stream); c로 지정한 바이트를 (unsigned char로 변환한 후에) stream이 가리키는 스트림에 쓴다. 문자 혹은 숫자가 아스키 코드표에 맞게 int값으로 들어감. 성공 시 c 반환, 실패 시 EOF 반환하고 errno 설정if (fputc (&#39;p&#39;, stream) == EOF)3.7.2 문자열 기록하기int fputs (const char *str, FILE *stream) str이 가리키는 NULL로 끝나는 문자열 전무를 stream이 가리키는 스트림에 기록한다. 성공하면 음수가 아닌 값 반환, 실패 시 EOF 반환3.7.3 바이너리 데이터 기록하기 C 변수처럼 바이너리 데이터를 직접 저장하려면 표준 입출력에서 제공하는 fwrite()를 사용size_t fwrite (void *buf, size_t size, size_t nr, FILE *stream); buf가 가리키는 데이터에서 size크기의 엘리먼트 nr개를 stream에 쓴다.3.8 사용자 버퍼 입출력 예제 프로그램int main(void){ FILE *in, *out; struct pirate { char name[100]; unsigned long booty; unsigned int beard_len; } p, blackbeard = {&quot;Edward Teach&quot;, 950, 48}; out = fopen (&quot;data&quot;, &quot;w&quot;); if (!out) { perror(&quot;fopen&quot;); return 1; } if (!fwrite(&amp;amp;blackbeard, sizeof(struct pirate), 1, out)){ perror (&quot;fwrite&quot;); return 1; } if (fclose(out)){ perror(&quot;fclose&quot;); return 1; } in = fopen(&quot;data&quot;, &quot;r&quot;); if (!fread(&amp;amp;p, sizeof (struct pirate), 1, in)){ perror(&quot;fread&quot;) return 1; }} 변수 크기, 정렬 등에서 차이가 있기 때문에 특정 애플리케이션에서 쓴 바이너리 데이터를 다른 애플리케이션에서 읽지 못할 수도 있다.만약 unsigned long 타입의 크기가 바뀌거나 채워 넣는 값의 양이 달라진다면 정확한 데이터를 못쓸것.아키텍처와 ABI가 동일한 경우에만 바이너리 데이터를 일관적으로 읽고 쓸 수 있음. ABI : Application Binary Interface3.9 스트림 탐색하기int fseek (FILE *stream, long offset, int whence) offset과 whence에 따라 stream에서 파일 위치를 조작한다. whence SEEK_SET - 파일 위치를 offset값으로 설정 SEEK_CUR - 현재위치에서 offset만큼 더한 값으로 설정 SEEK_END - 파일 위치를 파일 끝에서 offset만큼 더한 값으로 설정 성공하면 0 반환하고 EOF 지시자를 초기화하고 이전에 실행했던 ungetc()를 취소한다. 에러 발생하면 -1 반환하고 errno를 설정 EBADF - 유효하지 않은 스트림 EINVAL - whence인자 잘못됨 fsetpos는 stream의 위치를 pos로 설정한다.int fsetpos (FILE *stream, fpos_t *pos) 이는 whence가 SEEK_SET인 fseek()와 동일하게 동작함. C의 long 타입만으로는 스트림의 위치를 지정하기에 충분하지 않으므로 어떤 플랫폼에서는 이 함수가 스트림 위치를 특정한 값으로 설정할 수 있는 유일한 방법임.void rewind (FILE *stream)// ==fseek(stream, 0, SEEK_SET); 스트림을 시작 위치로 되돌리며 fseek을 위와 같이 사용하는 것과 동일함. 하지만 fseek()와는 달리 rewind()는 오류 지시자를 초기화 한다. rewind는 반환값이 없어서 에러 조건을 직접적으로 파악할 수가 없음.// 이런식으로 직접 확인을 해야함.errno = 0;rewind(stream);if (errno) //error3.9.1 현재 스트림 위치 알아내기 lseek()와는 다르게 fseek()는 갱신된 위치를 반환하지 않음. 따라서 위치를 파악하기 위한 용도로 분리된 인터페이스를 제공함. ftell 은 현재 스트림 위치를 반환한다.long ftell(FILE *stream); 표준 입출력에서는 fgetpos도 제공을 한다.int fgetpos (FILE *stream, fpos_t *pos) 성공하면 0을 반환하고 현재 스트림 위치를 pos에 기록함. 실패하면 -1을 반환하고 errno를 설정 fsetpos()와 마찬가지로 fgetpos()는 복잡한 파일 위치 타입을 사용하는 비-유닉스 플랫폼을 위해 제공한다.3.10 스트림 비우기 표준 입출력 라이브러리는 사용자 버퍼를 커널로 비워서 스트림에 쓴 모든 데이터가 write()을 통해 실제로 디스크에 기록되도록 만드는 인터페이스를 제공함.int fflush (FILE *stream); stream에 있는 쓰지 않은 데이터를 커널로 비운다. stream이 NULL이면 프로세스의 열려있는 모든 입력 스트림이 비워짐. 성공하면 0 반환, 실패하면 EOF반환하고 errno를 설정 fflush()와 버퍼 여기서 설명하는 모든 함수 호출은 커널이 유지하는 버퍼가 아니라 C 라이브러리가 관리하는 버퍼 를 의미한다. 이는 커널 영역이 아니라 사용자 영역에 위치함. → 시스템 콜을 사용하지 않고 사용자 코드를 실행함으로써 성능개선 fflush()는 단지 사용자 버퍼에 있는 데이터를 커널 버퍼로 쓰기만 함. → 이는 사용자 버퍼를 사용하지 않고 write()을 직접 사용하는 효과와 동일 즉, 데이터를 매체에 물리적으로 기록한다는 보장이 없다. 데이터가 매체에 즉각 기록되어야 하는 경우에는 fflush()를 호출한 다음 바로 fsync()를 호출한다. → 사용자 버퍼를 커널에 쓰고 fsync()를 통해 커널 버퍼를 디스크에 기록하도록 보장한다. 3.11 에러와 EOF fread()와 같은 몇몇 표준 입출력 인터페이스는 에러와 EOF를 구분하는 방법을 제공하지 않는 등 이슈가 있다. ferror()는 스트림에 에러 지시자가 설정되었는지 검사한다. int ferror(FILE *stream) 에러 지시자는 에러 조건에 따라 표준 입출력 인터페이스에서 설정한다. 해당 스트림에 에러 지시자가 설정되어 있을 경우 0이 아닌 값을 반환, 그렇지 않은 경우 0 반환 feof()는 해당 스트림에 EOF 지시자가 설정되어 있는지 검사한다.int feof (FILE *stream) EOF 지시자는 파일 끝에 도달하면 표준 입출력 인터페이스에서 설정한다. clearerr() 함수는 스트림에서 에러 지시자와 EOF 지시자를 초기화한다. void clearerr (FILE *stream); 반환값이 없고 항상 성공하기 때문에 stream 인자값이 정상인지 확인할 수 있는 방법이 없다. 이를 호출하고 나면 다시 복구할 방법이 없으므로 에러 지시자와 EOF 지시자를 먼저 검사한 후에 호출해야함3.12 파일 디스크립터 얻어오기 스트림에서 파일 디스크립터를 구해야 하는 경우가 있다. 대응하는 표준 입출력 함수가 없을 때 int fileno (FILE *stream) fileno를 통해서 fd를 구할 수 있다. 성공하면 stream과 관련된 fd를 반환하고, 실패하면 -1을 반환. 주어진 스트림이 유효하지 않은 경우 errno를 EBADF로 설정 표준 입출력 함수와 시스템 콜 사이에서 사용자 버퍼링과 관련된 충돌이 발생하지 않도록 주의해야 함fd를 사용하기전에 스트림을 비우는 것은 좋은 습관.어쨌든 두가지를 섞어 쓰는 것은 좋지 않다.3.13 버퍼링 제어하기 표준 입출력은 세 가지 유형의 사용자 버퍼링을 구현하고, 버퍼의 유형과 크기를 다룰 수 있는 인터페이스를 제공한다. 각각의 사용자 버퍼링 타입은 저마다의 목적이 있으며 상황에 맞게 사용할 때 가장 이상적임버퍼 미사용 사용자 버퍼를 사용하지 않는다. 즉, 커널로 바로 데이터를 보낸다. 표준 에러를 제외하고는 거의 사용되지 않음행 버퍼 행 단위로 버퍼링을 수행한다. 즉, 개행문자가 나타나면 버퍼의 내용을 커널로 보난다. 화면 출력 메시지는 개행문자로 구분되기 때문에 행 버퍼는 화면 출력을 위한 스트림일 경우 유용함. 표준 출력처럼 터미널에 연결된 스트림에서 기본적으로 사용블록 버퍼 고정된 바이트 개수로 표현되는 블록 단위로 버퍼링을 수행한다. 기본적으로 파일과 관련된 모든 스트림은 블록 버퍼를 사용한다. 표준 입출력에서는 블록 버퍼링을 Full 버퍼링이라고 한다. 표준 입출력은 버퍼링 방식을 제어할 수 있는 인터페이스를 제공한다.int setvbuf (FILE *stream, char *buf, int mode, size_t size); mode _IONBF - 버퍼 미사용 _IOLBF - 행 버퍼 _IOFBF - 블록 버퍼 buf 와 size를 무시하는 _IONBF를 제외하고 나머지는 size 바이트 크기의 버퍼를 가리키는 buf를 주어진 stream을 위한 버퍼로 사용한다. buf가 NULL이라면 glibc 가 자동적으로 지정된 크기만큼 메모리를 할당한다. 스트림을 연 다음 다른 연산을 수행하기 전에 호출해야함. 제공된 버퍼는 스트림이 닫힐 때까지 반드시 존재해야 한다. 흔히 스트림을 닫기 전에 끝나는 스코프 내부의 자동 변수로 버퍼를 선언하는 실수를 함. 특히 main()에서 지역변수로 버퍼를 만든 다음에 스트림을 명시적으로 닫지 않는 경우를 주의해야함. int main(void){ char buf[BUFSIZ]; // stdout을 bufsiz 크기에 맞춰 블록 버퍼로 설정한다. setvbuf(stdout, buf, _IOFBF, BUFSIZ); return 0; // buf는 스코프를 벗어나고 해제된다. 하지만 stdout을 닫지 않았음} 스코프를 벗어나기 전에 스트림을 명시적으로 닫아주거나, buf를 전역 변수로 설정함으로써 방지할 수 있음. 표준 에러를 제외하고 터미널은 행 버퍼링으로 동작, 파일은 블록 버퍼링을 사용하는 것이 맞다.블록 버퍼링에서 버퍼의 기본 크기는 BUFSIZ이며 일반적인 블록 크기의 정수배인 최적의 값이다.따라서 개발자들은 일반적으로 스트림을 다룰 때 버퍼링에 대해 고민할 필요가 없다.3.14 스레드 세이프 스레드 : 개별 프로세스 내에 존재하는 여러 개의 실행 단위.멀티 스레드 프로세스 : 주소 공간을 공유하는 여러 개의 프로세스 스레드 세이프(Thread-safe)멀티 스레드 프로그래밍에서 일반적으로 어떤 함수나 변수, 혹은 객체가 여러 스레드로부터 동시에 접근이 이루어져도 프로그램의 실행에 문제가 없음을 뜻함→ 멀티스레드 환경에서 동작해도 원래 의도한 대로 동작하는 것을 스레드 세이프 하다고 할 수 있음 Thread Safe 하지 않은 코드 예시 int num;boolean is_even;int inc(int n){ num += n; if ((num%2) == 0) is_even = true; else is_evne = false; return num;} num이라는 변수에 숫자를 더해서 짝수이면 is_even = true, 홀수이면 false로 설정 싱글 스레드 환경에서는 문제없는 코드 a = int(1); 위의 라인을 멀티스레드에서 수행했을 경우 의도와 맞지 않는 결과가 발생할 수 있음 멀티코어 시스템에서는 둘 이상의 스레드가 같은 프로세스에서 동시에 실행될 수가 있다. 스레드에서 데이터에 접근할 때 동기화에 주의하지 않거나, 스레드 로컬(스레드 감금) 로 만들지 않으면 스레드가 공유 데이터를 덮어써버릴 수 있다. 스레드를 지원하는 OS는 락 메커니즘을 지원한다. 표준 입출력은 이런 메커니즘을 활용해서 단일 프로세스 내의 여러 스레드가 동시에 표준 입출력을 호출할 수 있도록 한다. 심지어는 같은 스트림에 대해서도 가능하다. 하지만 이것만으로는 부족함 여러 함수 호출을 그룹으로 묶어 통째로 락을 걸면 크리티컬 섹션이 하나의 입출력 연산에서 여러 입출력 연산으로 확장됨 크리티컬 섹션 : 임계 구역. 다른 스레드의 간섭 없이 실행할 수 있는 코드 효율성을 높이기 위해 락을 완전히 없애고 싶은 경우 락을 없애면 온갖 문제가 난무하지만 어떤 프로그램은 모든 입출력을 싱글 스레드에 위임하여 스레드를 가두어서 스레드 세이프를 구현하기도 함. → 락에 의한 오버헤드가 없다. 스레드는 입출력 요청에 앞서 락을 획득하고 고유 스레드가 되어야 함 표준 입출력 함수는 본질적으로 스레드 세이프를 보장한다는 것 단일 함수 호출 관점에서 보면 표준 입출력 연산은 Atomic 하다. 3.14.1 수동으로 파일 락 걸기 stream의 락이 해제될 때까지 기다린 후에 락 카운터를 올리고 락을 얻은 다음, 스레드가 stream을 소유하도록 만든 후에 반환한다.void flockfile (FILE *stream); funlockfile 함수는 stream과 연관된 락 카운터를 하나 줄인다.void funlockfile(FILE *stream); 락 카운터가 0이 되면 현재 스레드는 stream의 소유권을 포기해서 다른 스레드가 락을 얻을 수 있도록 한다. 여러번 중첩 호출이 가능함. ftrylockfile() 함수는 flockfile()의 논블록 버전이다.int ftrylockfile (FILE *stream) stream이 락이 걸려있다면 ftrylockfile()은 아무것도 하지 않고 즉시 0이 아닌 값을 반환함. 논블록이 아니라면, 락이 걸린 상태에서는 블록되어서 계속 기다려야함. 만약 stream이 락이 걸린 상태가 아니라면 락을 걸고 락 카운터를 하나 올린 다음 그 stream을 소유하도록 만들고 0을 반환한다.flockfile (stream);fputs(&quot;a&quot;, stream);fputs(&quot;b&quot;, stream);fputs(&quot;c&quot;, stream);funlockfile(stream); 기록 중 다른 스레드가 중간에 끼어들지 못하게 하려면 락을 이용해야함. 설계 자체에서 동일 스트림을 대상으로 입출력하지 않도록 해야한다. 만약 그렇게 할 수 없다면 flockfile() 같은 함수를 이용해서 크리티컬 섹션을 확장해야 함.3.14.2 락을 사용하지 않는 스트림 연산 상세하고 정밀한 락 제어를 통해 가능한 한 락 오버헤드를 최소화해서 성능을 향상시킬 수 있기때문에 스트림에 대해 수동으로 락을 설정함. 앞서 다루었던 표준 입출력 함수들은 내부적으로 락을 사용한다. 수동으로 락을 걸면 각 표준 입출력 함수들의 내부적인 락을 사용하지 않아도 된다. _unlocked postfix가 붙은 함수들을 이용하면 락 오버헤드를 최소화할 수 있다.3.15 표준 입출력 비평 몇몇 전문가는 표준 입출력의 결함을 지적함 fgets()는 충분한 기능을 제공하지 못한다 wild character 같은 경우 eof 로 읽을 수도 있음 gets()는 표준에서 제거되기도 함.. 경고: the ‘gets‘ function is dangerous and should not be used. 이 문제는 gets함수가 strName[20]의 크기를 모르면서 개행을 찾거나 EOF를 만날 때까지 계속 읽기 때문에 주어진 버퍼의 크기를 넘을 수 있습니다.다르게 설명하자면 C언어는 경계 검사를 수행하지 않아 gets함수가 접근 권한이 없는 주소에 도달 할 때까지 읽기를 계속합니다. 접근 권한이 없는 주소에 도달하는 이런 행위가 아마도 Linux에서는 시스템을 이상 종료 시킬 수 있는 오류 일 것입니다. 그래서 많은 바이러스가 이러한 문제점을 이용합니다. 가장 큰 불만은 이중 복사로 인한 성능 문제이다. 데이터를 읽을 때 표준 입출력의 read() 시스템 콜을 사용하면 데이터는 커널에서 표준 입출력의 버퍼로 복사된다. fgetc()같은 표준 입출력을 통해서 읽기를 요청하면 그 데이터는 표준 입출력 버퍼에서 인자로 제공된 버퍼로 또 복사된다. 쓰기 요청도 마찬가지 읽기 요청은 표준 입출력 버퍼를 가리키는 포인터를 반환하는 대체 구현으로 이중 복사 문제를 피할 수 있음 쓰기 요청도 포인터 기록을 통해서 피할 수 있음3.16 맺음말 표준 입출력은 표준 C 라이브러리의 일부로 제공되는 사용자 버퍼링 라이브러리이다. 아래의 가정을 만족할 때 표준 입출력과 사용자 버퍼링은 의미가 있다. 많은 시스템 콜이 의심되는 경우 수 많은 호출을 합쳐서 줄이는 방법으로 오버헤드를 줄이고 싶다. 성능이 중요하며 모든 입출력은 정렬된 블록 경계에 맞춰 블록 크기 단위로 일어나도록 확실하게 보장해야 한다. 접근 패턴이 문자나 행 기반이며 낯선 시스템 콜에 의지하지 않고 손쉽게 데이터에 접근할 수 있는 인터페이스가 필요하다. 저수준 리눅스 시스템 콜보다는 고수준의 인터페이스를 선호한다. " }, { "title": "[Linux System Programming] Ch02 파일 입출력", "url": "/posts/linux_ch02/", "categories": "Linux", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-09 00:00:00 +0900", "snippet": "Ch02 파일 입출력2. 파일 입출력리눅스는 많은 인터페이스를 파일로 구현했음. (유닉스 시스템에서는 거의 모든 것을 파일로 표현)파일 입출력은 단순한 파일 처리를 넘어서 다양한 작업에 밀접하게 관련되어 있음.파일은 읽거나 쓰기 전에 반드시 열어야 한다. File Table 커널은 파일 테이블이라고 하는 프로세스 별로 열린 파일 목록을 관리 함. File Descripter(fd) 음이 아닌 정수 값으로 인덱싱 되어있음 각 항목은 열린 파일에 대한 정보를 담고 있음 inode Pointer Metadata fd 0 : stdin 1 : stdout 2 : stderr 읽고 쓸 수 있는 모든 것은 파일 디스크립터를 통해 접근할 수 있음 2.1 파일 열기 파일 접근하는 기본적인 방법 read(), write() 하지만 접근하기 전에 open() 이나 creat() 로 열고, 다 쓴 후에는 close()로 닫아야함. open() 시스템 콜int open (const char *name, int flags);int open (const char *name, int flags, mode_t mode); 경로 이름이 name인 파일을 fd에 맵핑 성공하면 fd return offset 0으로 설정 실패 시 -1 리턴 flags O_RDONLY, O_WRONLY, O_RDWR 이외에도 여러가지 플래그들이 있는데, flags 파라미터에 OR연산을 통해 작동 가능 O_WRONLY O_TRUNC 새로운 파일의 권한파일 생성하는게 아니라면 open()의 mode 인자는 무시됨.하지만 O_CREAT과 같이 생성 시에는 mode가 꼭 필요하다.creat() 함수 O_WRONLY O_CREAT O_TRUNC → 읽기 전용, 해당 파일이 없으면 새로 생성, 파일이 존재하고 일반파일이며 flags인자에 쓰기가 가능하다면 파일 길이를 0으로 잘라버림→ 너무나도 일반적인 flags라서 이를 지원하는 시스템 콜이 바로 create()fd = creat(filename, 0644);==fd = open(filename, O_WRONLY | O_CREAT | O_TRUNC, 0644);2.2 read()로 읽기ssize_t read (int fd, void *buf, size_t len);호출할 때마다 fd가 참조하는 파일의 현재 오프셋에서 len 바이트만큼 buf로 읽어 들인다. return 성공 시 buf에 쓴 바이트 숫자 실패 시 -1 read()의 여러가지 반환 케이스nr = read (fd, &amp;amp;word, len) nr == len 같은 값 : 정상 0 &amp;lt; nr &amp;lt; len : 읽은 바이트는 word에 저장. 중간에 시그널이 중단 or 읽는 도중 에러 발생 or len 만큼 읽기전에 EOF 발생 → 원인 파악 가능 (buf ,len 을 고친다음 호출 수행) nr == 0 : EOF 블록 : 현재 사용가능한 데이터가 없음 nr == -1 &amp;amp;&amp;amp; EINTR : 바이트 읽기 전에 시그널 도착 nr == -1 &amp;amp;&amp;amp; EAGAIN : 읽을 데이터가 없어서 블록. 논믈록 모드 일 때만 일어나는 상황 nr == -1 &amp;amp;&amp;amp; OTHER : 심각한 에러→ 일반적인 read()는 에러 처리하면서 실제로 모든 len 바이트( 적어도 EOF까지)를 읽는 경우 적합하지 않음while (len != 0 &amp;amp;&amp;amp; (ret = read(fd, buf, len)) != 0) { if (ret == -1){ if (errno == EINTR) continue; perror (&quot;read&quot;); break; } len -= ret; buf += ret;}위의 코드는 5가저 조건을 모두 처리한다.논블록 읽기 때떄로 프로그래머 입장에서 읽을 데이터가 없을 때 read()호출이 블록되지 않기를 바라는 경우가 있음 블록되는 대신 읽을 데이터가 없다는 사실을 알려주기 위해 호출이 즉시 반환되는 편을 선호한다. open() 할 때 플래그를 O_NONBLOCK을 넘겨주었다면 파일 디스크립터를 논블록으로 열게되는데, 이 때 읽을 데이터가 없다면 read() 는 호출이 블록되는 대신 -1을 반환하면서 errno를 EAGAIN으로 설정한다.Blocking I/O Model I/O 작업이 진행되는 동안 유저 프로세스는 자신의 작업을 중단한채 대기해야함. → 리소스 낭비가 심하다. 이를 해결하기 위해 클라이언트 별로 쓰레드를 만들어 연결시켜준다면 클라이언트 수가 늘어날 수록 쓰레드가 너무 많아진다. 이렇게 되면 context switching 횟수가 증가하게 됨. 비효율적Non Blocking I/O Model I/O 작업을 진행하는 동안 유저 프로세스의 작업을 중단시키지 않는다. 함수를 호출하면 진행상황과 상관없이 바로 결과를 반환read() 크기 size_t 바이트 단위로 크기를 측정하기 위해 사용되는 값 저장 SIZE_MAX ssize_t 부호가 있는 size_t 이다. (signed) 음수 에러를 포함하기 위해 사용 SSIZE_MAX len이 이보다 큰 경우의 read() 호출결과는 정의되어 있지 않음 32bit 기계에서는 0x7ffffffff 2.3 Write()로 쓰기파일에 데이터를 기록하기 위해 사용하는 가장 기본적이며 일반적인 시스템 콜ssize_t write (int fd, const void *buf, size_t count);count 바이트만큼 fd가 참조하는 파일의 현재 위치에 시작지점이 buf인 내용을 기록한다. return 성공하면 쓰기에 성공한 바이트 수를 반환 에러가 발생하면 -1을 반환하며 errno를 적절한 값으로 설정 /*buf에 들어있는 문자열을 fd가 가리키는 파일에 입력한다 */const char *buf = &quot;My ship is solid!&quot;;ssize_t nr;nr = write(fd, buf, strlen (buf));if (nr == -1) /* 에러 */덧붙이기 모드 O_APPEND 옵션으로 open()을 하게되면 현재 파일 오프셋이 아니라 파일 끝에서부터 쓰기 연산이 일어난다. 다중 프로세스가 같은 파일 수정을 진행했을 때 race condition이 발생한다. 이는 명시적인 동기화 과정 없이 동일 파일에 덧붙이는 작업이 불가능함을 의미함. 덧붙이기 모드를 이용하면 프로세스가 여럿 존재할지라도 항상 덧붙이기 작업이 수행됨write() 동작방식리눅스 커널은 디스크의 데이터를 캐싱하는데, 이를 페이지 캐시(page cache)라고 하고, 캐시되어 있던 페이지가 다시 디스크로 적용되는 것(동기화 되는 것)을 page writeback이라고 한다. 페이지 캐시의 최대 목적은 디스크 입출력을 최소화 시키는 데 있다. 디스크 접근은 메모리 접근에 비해 상대적으로 많이 느리다. milliseconds vs nanoseconds L1 &amp;gt; L2 &amp;gt; L3 &amp;gt; Memory &amp;gt; Disk Data Locality: 최근에 사용된 데이터는 다시 사용될 가능성이 높다.Write caching3가지 동작 예상 write()에 대해서 이미 캐싱해놓은 데이터와는 상관없이 바로 디스크에 데이터를 내려버리는 경우. 즉, 메모리에 있는 캐시 데이터를 지나치고 바로 디스크로 데이터를 갱신한다. 이 경우에는 기존에 캐싱되어 있는 페이지 캐시는 invalidate 된다. 만약 read()가 해당 데이터에 대해서 들어오면 디스크로부터 읽어온다. 메모리에 있는 캐시와 디스크 모두 갱신해준다. 가장 간단한 방법으로 이러한 방식을 write-through cache라고 한다. 캐시부터 디스크까지 모두 write() 연산이 수행된다. 이 경우 캐시와 디스크 모두를 항상 최신 상태로 만들어주기 때문에 캐시를 일관성있게 유지해준다. (cache coherent) (현재 Linux에서 사용하고 있는 방식) write back 방식은 write() 요청이 들어왔을 때 페이지 캐시에만 우선 갱신하고 backing store에는 바로 갱신하지 않는 방식이다. 이 방식을 채택하면 cache와 원본 데이터가 서로 다르게 되며, 캐시에 있는 데이터가 최신 데이터가 된다. 최신 데이터는 캐싱이 된 이후로 업데이트가 되었다는 의미로 dirty 상태(unsynchronized)가 되며 dirty list에 추가되어 커널에 의해 관리된다. 커널은 주기적으로 dirty list에 등록되어 있는 페이지 캐시를 backing store에 동기화해주는데 이러한 작업을 writeback이라고 한다. writeback 방식은 write-through 방식보다 나은 방법인데, 왜냐하면 최대한 디스크에 쓰는 것을 미루어둠으로써 나중에 대량으로 병합해서 디스크에 쓸 수 있기 때문이다. 단점은 조금 더 복잡하다는 것이다.출처(https://scslab-intern.gitbooks.io/linux-kernel-hacking/content/chapter16.html)2.4 동기식 입출력 필요한 경우 디스크에 순서대로 기록해야만 하는 경우→ 커널의 대기열에서 성능 개선에 적합한 방식으로 쓰기 요청 순서를 변경하기 때문에 문제가 발생할 수 있음 시스템이 비정상 종료될 경우→ 버퍼에 있는 내용을 디스크에 쓰기 전에 시스템이 종료될 수 있음 분명히 입출력을 동기화 하는 것은 중요한 주제임. 하지만 Write 작업이 지연되는 문제를 너무 확대 해석하면 안됨. 최신 OS라면 버퍼를 통해서 지연된 쓰기 작업 을 구현하고 있다. 그럼에도 시점을 제어하고 싶을 때가 있기 때문에 리눅스 커널에서는 “성능”을 희생하는 대신, 입출력을 동기화하는 몇 가지 옵션을 제공한다.fsync()와 fdatasync() fsync()를 호출하면 fd에 맵핑된 파일의 모든 변경점을 디스크에 기록한다. 반드시 fd는 쓰기 모드로 열려야함. fdatasync()는 fsync()와 동일한 기능을 하지만, 메타데이터까지 저장하는 fsync()와는 다르게 데이터만 기록한다. 그렇기 때문에 더 빠름.int ret;ret = fsync(fd);if (ret == -1); /* ERROR */ 몇몇 리눅스 배포판에서 fdatasync()는 구현되어있지만, fsync()는 구현되어있지 않을 때도 있다. 이때는 EINVAL를 반환sync() 모든 버퍼 내용(데이터와 메타데이터 모두) 을 디스크에 강제로 기록해서 동기화함. 최적화는 조금 부족void sync (void); 인자도 없고, 반환 값도 없음 → 항상 호출 성공O_SYNC 플래그 open() 호출 시 O_SYNC 플래그를 사용하면 모든 파일 입출력은 동기화 됨. (읽기는 언제나 동기화 됨. 하지만 write은 보통 동기화 되지 않음) write()가 작업 후 반환하기 직전에 fsync()를 매번 호출하는 방식이라고 이해해도 좋음. (실제로 리눅스 커널에서는 좀 더 효율적인 방식으로 구현하고 있지만 의미는 동일) Latency 가 조금씩 늘어남 → 입출력 동기화에 들어가는 비용이 매우 크기 때문에, 다른 대안을 모두 적용한 다음 최후의 선택으로 사용해야함 일반적으로 쓰기 작업이 디스크에 바로 기록되어야 하는 애플리케이션에서는 fsync()나 fdatasync()를 사용함. 이들은 호출 횟수가 적어서 O_SYNC보다 비용이 적게 듬.O_DSYNC와 O_RSYNC O_DSYNC는 메타데이터를 제외한 일반 데이터만 동기화 (fdatasync()와 동일) O_RSYNC는 쓰기뿐만 아니라 읽기까지도 동기화되도록 한다. read() 호출은 특별한 옵션 없이도 항상 동기화 되기 때문에 O_RSYNC 플래그가 특별히 필요하지 않다. 다만 최종적으로 사용자에게 넘겨줄 데이터가 생길 때 까지 반환되지 않음. 리눅스는 O_RSYNC를 O_SYNC와 동일하게 정의한다. 리눅스 구현상 이런 동작을 구현하기가 쉽지 않다고 한다. 2.5 직접 입출력 리눅스 커널은 디바이스와 애플리케이션 사이에 캐시, 버퍼링, 입출력 관리 같은 복잡한 계층을 구현하고 있음 성능이 중요한 애플리케이션에서는 우회해서 직접 입출력을 하고 싶을수도 있다. 일반적으로는 노력에 비해 효과가 낮다. 하지만 DB시스템은 독자적인 캐시를 선호하며 OS의 개입을 최소한으로 줄이기를 원함 O_DIRECT open()호출에서 O_DIRECT를 넘기면 커널이 입출력 관리를 최소화하도록 한다. 페이지 캐시를 우회해서 사용자 영역 버퍼에서 직접 디바이스로 입출력 작업을 시작한다. 모든 입출력은 동기식. 입출력 작업이 완료된 후에 호출이 반환 됨 2.6 파일 닫기int close (int fd); fd로 읽고 쓰는 작업을 마치면 close로 파일 맵핑을 끊어야한다. close()를 호출하면 fd에 연관된 파일과의 맵핑을 해제하며 프로세스에서 파일을 떼어낸다. 파일을 닫더라도 파일을 디스크에 강제로 쓰지 않는다는 점을 기억해야한다. 확실히 기록하려면 동기식 입출력 방법 중 하나를 써야함 에러 값 지연된 연산에 의한 에러는 한참 후에도 나타나지 않기 때문에 close()의 반환값을 검사해주는 것이 중요함. EBADF (파일 디스크립터가 유효하지 않음) EIO (저수준의 입출력에러)2.7 lseek()로 탐색하기 가끔 파일의 특정 위치로 직접 이동해야 할 필요가 있을 떄가 있다. lseek()을 사용하면 fd에 연결된 파일의 오프셋을 특정 값으로 지정할 수 있다. 파일 오프셋 갱신 외에 다른 동작은 하지 않고 어떤 입출력도 발생하지 않음.off_t lseek (int fd, off_t pos, int origin); origin SEEK_CUR fd의 파일 오프셋을 현재 오프셋에서 pos값을 더한 값으로 설정. pos값은 음수, 0, 양수 모두 가능 pos가 0이면 현재 파일 오프셋을 반환 SEEK_END fd의 파일 오프셋을 현재 오프셋에서 pos값을 더한 값으로 설정. pos값은 음수, 0, 양수 모두 가능 pos가 0이면 파일 오프셋을 현재 파일의 끝으로 설정 SEEK_SET fd의 파일 오프셋을 pos값으로 설정 pos가 0이면 파일 오프셋을 파일의 처음으로 설정 현재 파일 오프셋 찾기 lseek()은 갱신된 파일 오프셋을 반환하므로 lseek()에 SEEK_CUR와 0을 pos값으로 넘기면 현재 파일 오프셋을 찾을 수있다. pos = lseek (fd, 0, SEEK_CUR) 파일의 시작 혹은 끝 지점으로 오프셋을 이동하거나, 현재 오프셋을 알아내는데 많이 사용 됨!파일 끝을 넘어서 탐색하기 파일 끝을 넘어서도록 위치를 지정하는 것은 아무런 일도 발생하지 않음. 이때 read()를 하면 EOF반환 이때 write()를 하면 마지막 오프셋과 새로운 오프셋 사이에 새로운 공간이 만들어지며 0으로 채워짐 0으로 채운 공간을 구멍 (spare file)이라고 하는데, 이 구멍들은 물리적인 디스크 공간을 차지하지 않음→ 파일시스템에서 모든 파일을 합친 크기가 물리적인 디스크 크기보다 더 클 수 있음→ 이런 파일이 공간을 상당히 절약하며 효율을 크게 높일 수 있다.제약사항 파일 오프셋의 최댓값은 off_t의 크기에 제한됨. 커널은 내부적으로 오프셋 값을 C의 long long타입으로 저장 64비트 머신에서는 문제가 되지 않지만, 32비트 머신에서는 EOVERFLOW 에러 발생 가능 2.8 지정한 위치 읽고 쓰기ssize_t pread (int fd, void *buf, size_t count, off_t pos); pread()를 사용하면 fd에서 pos오프셋에 있는 데이터를 buf에 count 바이트만큼 읽는다.ssize_t pwrite (int fd, const void *buf, size_t count, off_t pos); pwrite()를 사용하면 buf에 담긴 데이터를 fd의 pos 오프셋에 count 파이트만큼 쓴다. 둘 모두 작업 후 파일 오프셋을 갱신하지 않는다. 둘 모두 현재 파일의 오프셋을 무시하며 pos로 지정한 오프셋을 사용한다는 점을 제외하고는 read()와 write() 시스템 콜과 거의 유사하게 동작한다. read() 나 write() 호출 전에 lseek()을 호출하는 방식과 유사하지만 3가지 차이점이 존재 작업 후 파일 오프셋을 원위치로 되돌리거나 임의의 오프셋에 접근해야 하는 경우 쉽게 사용가능 호출이 완료된 후 파일 포인터를 갱신하지 않음 lseek()를 사용할 때 발생할 수 있는 경쟁 상태를 피할 수 있다. lseek()은 본질적으로 여러 스레드에서 같은 fd를 처리할 경우 안전하지가 않음. race condition 발생 가능. 에러 값 두 함수는 호출이 성공하면 읽거나 쓴 바이트 개수를 반환함. pread() 0반환 : EOF pwrite() 0 반환 : 아무런 데이터도 쓰지 못했음 pread()는 read()와 lseek()에서 허용하는 errno 값을 설정 pwrite()는 write()와 lseek()에서 허용하는 errno 값을 설정2.9 파일 잘라내기파일을 특정 길이만큼 잘라내기 위한 시스템 콜int ftruncate (int fd, off_t len)int truncate (const char *path, off_t len); 두 시스템 콜은 모두 파일을 len 크기만큼 잘라낸다. ftruncate()는 쓰기 모드로 열린 fd에 대해 동작. truncate()는 쓰기 권한이 있는 파일 경로에 대해서 동작 성공 둘 다 0을 반환 에러 -1 반환, errno를 적절한 값으로 설정함. 호출이 성공하면 파일의 길이는 len이 된다. len과 자르기 전의 파일 크기 사이에 존재하던 데이터는 없어지고, read()를 통해 이 영역에 접근할 수 없게 됨.2.10 다중 입출력 논블록 입출력이 효과적이지 않은 두가지 이유 프로세스는 계속 열린 fd 중 하나가 입출력을 준비할 때까지 기다리면서 어떤 임의의 순서대로 입출력을 요청해야 한다. 프로세스를 재워 다른 작업을 처리하게 하고 fd가 입출력을 수행할 준비가 되면 깨우는 편이 더 효과적일 수 있음. 논블록 입출력으로 이것을 해결할 수 있지만 프로세스가 계속 깨워져있어야 한다는 단점이 존재한다. 다중 입출력은 애플리케이션이 여러개의 fd를 동시에 블록하고 그중 하나라도 블록되지 않고 읽고 쓸 준비가 되면 알려주는 기능을 제공 다중 입출력: fd 중 하나가 입출력이 가능할 때 알려준다. 준비가 됐나? 준비된 fd가 없다면 하나 이상의 fd가 준비될 때까지 잠든다. 깨어나기. 어떤 fd가 준비됐나? 블록하지 않고 모든 fd가 입출력을 준비하도록 관리한다. 1로 돌아가서 다시 시작한다. select()int select (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct *timeval *timeout);struct timeval { long tv_sec; long tv_usec;} select() 호출은 fd가 입출력을 수행할 준비가 되거나 옵션으로 정해진 시간이 경과할 때까지만 블록된다. 파라미터 n fd 집합에서 가장 큰 fd 숫자에 1을 더한 값 즉, fd에서 가장 큰 값이 무엇인지 알아내서 1 더해야 함. readfds 블록되지 않고 read()작업이 가능한지를 파악하기 위해 감시 writefds 블록되지 않고 write()작업이 가능한지를 파악하기 위해 감시 exceptfds 예외가 발생했거나 대역을 넘어서는 데이터 (이는 소켓에만 적용) 가 존재하는지 감시 → 어떤 집합이 NULL이면 해당 이벤트 감시하지 않음 timeout NULL이 아니면 입출력이 준비된 fd가 없을 경우에도 tv_sec, tv_usec 이후에 반환됨. 두 값이 모두 0이면 호출은 즉시 반환됨. 호출이 성공하면 각 집합은 요청받은 입출력 유형을 대상으로 입출력이 준비된 fd만 포함하도록 변경된다. ex) 7과 9인 두개의 fd가 readfds에 들어있다고 한다면 호출이 반환될 때 7이 집합에 남아있고 9가 남아있지 않다면, 7은 블록없이 읽기 가능! 9는 아마도 읽기 요청이 블록될 것임 select()에서 사용하는 fd집합은 직접 조작하지 않고 매크로를 사용해서 관리함FD_CLR(int fd, fd_set *set);FD_ISSET(int fd, fd_set *set);FD_SET(int fd, fd_set *set);FD_ZERO(fd_set *set); FD_ZERO 는 지정된 집합내의 모든 fd를 제거함. 항상 select() 호출 전에 사용해야함 FD_SET은 주어진 집합에 fd를 추가함 FD_CLR은 주어진 집합에서 fd를 하나 제거함 제대로 설계된 코드라면 FD_CLR을 사용할 일이 절대 없음! FD_ISSET은 fd가 주어진 집합에 존재하는지 검사 집합에 들어있다면 0이 아닌 정수 반환. 들어있지 않다면 0반환 반환값과 에러코드 호출 성공 전체 세 가지 집합 중에서 입출력이 준비된 fd개수를 반환함. timeout을 초과하면 반환값이 0이 될 수 있다. 에러 발생 -1 반환, errno 설정 select() 예제#include &amp;lt;stdio.h&amp;gt;#include &amp;lt;sys/time.h&amp;gt;#include &amp;lt;sys/types.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#define TIMEOUT 5#define BUF_LEN 1024int main() { struct timeval tv; fd_set readfds; int ret; // 표준 입력에서 입력을 기다리기 위한 준비를 합니다. FD_ZERO(&amp;amp;readfds); FD_SET(STDIN_FILENO, &amp;amp;readfds); // select가 5초 동안 기다리도록 timeval 구조체를 설정합니다. tv.tv_sec = TIMEOUT; tv.tv_usec = 0; // select() 시스템콜을 이용해 입력을 기다립니다. ret = select(STDIN_FILENO + 1, &amp;amp;readfds, NULL, NULL, &amp;amp;tv); if (ret == -1) { perror(&quot;select&quot;); return 1; } else if (!ret){ printf(&quot;%d seconds elapsed.\\n&quot;, TIMEOUT); return 0; } // select() 시스템콜이 양수를 반환했다면 &#39;블록(block)&#39;없이 즉시 읽기가 가능합니다. if (FD_ISSET(STDIN_FILENO, &amp;amp;readfds)) { char buf[BUF_LEN + 1]; int len; // &#39;블록(block)&#39;없이 읽기가 가능합니다. len = read(STDIN_FILENO, buf, BUF_LEN); if (len == -1) return 1; if (len) { buf[len] = &#39;\\0&#39;; printf(&quot;read: %s\\n&quot;, buf); } return 0; }}select()로 구현하는 이식 가능한 sleeptv.tv_sec = 0;tv.tv_usec = 500;select (0, NULL, NULL, NULL, &amp;amp;tv); 역사적으로 select()는 1초 미만의 짧은 시간 동안 프로세스를 재울 수 있는 더 나은 방법을 제공해왔음 최신 리눅스는 아주 짧은 시간 잠들기 인터페이스를 지원하고 있음pselect()int pselect (int n, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct *timespec *timeout, const sigset_t *sigmask);struct timespec { long tv_sec; long tv_nsec;} select()와의 차이점 pselect()는 timeout 인자로 timeval 구조체 대신 timespec 구조체를 사용. 이는 초, 나노 초 조합을 사용하므로 이론적으로 더 짧은 시간 동안 잠들 수 있다. 하지만 실제로는 둘 다 마이크로 초도 확실히 지원하지 못함.. pselect()는 timeout 인자를 변경하지 않기 때문에 잇달은 호출 과정에서 timeout 인자를 계속 초기화해야 할 필요가 없다. select() 시스템 콜은 sigmask 인자를 받지 않는다. 이 인자는 NULL로 설정하면 pselect()는 select()와 동일하게 동작한다. pselect()가 추가된 이유 fd와 시그널을 기다리는 사이에 발생할 수 있는 race condition 을 해결하기 위한 sigmask 인자를 추가하기 위함이다. 블록할 시그널 목록을 인자로 받아서 select() 도중에 시그널이 도착하는 경우에도 이를 처리함. sigmask가 가리키는 신호마스크가 자동으로 설정되어 차단되고, pselect() 호출이 반환될 때는 신호마스크가 복원되어 실행하게 된다. poll() select()의 몇 가지 결점을 보완함. select()는 ‘읽기, 쓰기 예외’ 3가지를 독립적으로 설정하고 매개변수로 넘겨줘야했다. 하지만 poll은 구조체 배열을 사용함으로써 설계상으로도 훨씬 더 좋아졌음. 그럼에도 불구하고 여전히 습관이나 이식성의 이유료 select()를 더 많이 사용함int poll (struct pollfd *fds, nfds_t nfds, int timeout);struct pollfd { int fd; short events; short revents;}; fds가 가리키는 단일 pollfd 구조체 배열을 nfds 개수만큼 사용함. events 필드는 fd에서 감시할 이벤트의 비트마스크 를 의미 POLLIN - 읽을 데이터가 존재한다. 즉, 읽기가 블록(blokc)되지 않는다. POLLRDNORM - 일반 데이터를 읽을 수 있다. POLLRDBAND - 우선권이 있는 데이터를 읽을 수 있다. POLLPRI - 시급히 읽을 데이터가 존재한다. POLLOUT - 쓰기가 블록(block)되지 않는다. POLLWRNORM - 일반 데이터 쓰기가 블록(block)되지 않는다. POLLWRBAND - 우선권이 있는 데이터 쓰기가 블록(block)되지 않는다. POLLMSG - SIGPOLL 메시지가 사용 가능하다. events를 설정하면 등록한 이벤트 중 발생한 이벤트가 revents필드에 설정된다. revents 필드는 등록한 이벤트 중 발생된 이벤트 정보를 커널이 설정해줌. revents 필드에는 다음 이벤트가 설정될 수 있다. POLLER - 주어진 파일 디스크립터에 에러가 있다. POLLHUP - 주어진 파일 디스크립터에서 이벤트가 지체되고 있다. POLLNVAL - 주어진 파일 디스크립터가 유효하지 않다. 예제1 fd의 읽기와 쓰기를 감시하려면 events 를 POLLIN POLLOUT으로 설정 호출이 반환되면 pollfd 구조체 배열에서 원하는 fd가 들어있는 항목을 찾아 revents에 해당 플래그가 켜져있는지 확인한다. POLLIN 이 설정되어 있다면 읽기는 블록되지 않음. POLLOUT이 설정되어 있다면 쓰기는 블록되지 않는다. 예제2 #include &amp;lt;stdio.h&amp;gt;#include &amp;lt;unistd.h&amp;gt;#include &amp;lt;poll.h&amp;gt;#define TIMEOUT 5int main() { struct pollfd fds[2]; int ret; // 표준 입력에 대한 이벤트를 감시하기 위한 준비를 한다 fds[0].fd = STDIN_FILENO; fds[0].events = POLLIN; // 표준 출력에 쓰기가 가능한지 감시하기 위한 준비를 한다. fds[1].fd = STDOUT_FILENO; fds[1].events = POLLOUT; // 위에서 pollfd 구조체 설정을 모두 마쳤으니 poll() 시스템콜을 작동시킨다. ret = poll(fds, 2, TIMEOUT * 1000); if (ret == -1) { perror(&quot;poll&quot;); return 1; } if (!ret) {//타임아웃 printf(&quot;%d seconds elapsed.\\n&quot;, TIMEOUT); return 0; } if (fds[0].revents &amp;amp; POLLIN) printf(&quot;stdin is readable\\n&quot;); if (fds[1].revents &amp;amp; POLLOUT) printf(&quot;stdout is writeable\\n&quot;); return 0;} ppoll() ppoll()은 리눅스에서만 사용가능한 인터페이스 pselect() 처럼 timeout 인자는 나노 초 단위로 지정 가능하며 블록할 시그널 집합은 sigmask 인자로 제공poll()과 select() 비교 비슷한 작업을 하지만 poll은 select 보다 훨씬 유용함! poll은 가장 높은 파일 fd값에다가 1을 더해서 인자로 전달할 필요 없음 select에서 값이 900인 fd를 감시하게되면 매번 fd 집합에서 900번째 비트까지 일일히 검사해야함 select 의 fd 집합은 크기가 정해져있어서 트레이드 오프가 발생함. poll은 딱 맞는 크기의 fd 집합을 사용함 select 는 fd 집합을 반환하는 시점에서 재구성되므로 매번 fd 집합을 초기화해야함. poll은 event(입력), revent(출력)이 분리되어있다. select 의 timeout 인자는 반환하게 되면 미정의 상태가 됨. 2.11 커널 들여다보기가상 파일 시스템(VFS) 사용 중인 파일시스템이 무엇인지 몰라도 파일시스템 데이터를 처리하고 파일 시스템 함수를 호출할 수 있도록 하는 추상화 메커니즘 추상화를 위해서 리눅스에서 모든 파일시스템의 기초가 되는 공통 파일 모델을 제공함. 일반적인 시스템 콜(read, write) 등은 커널이 지원하는 어떠한 파일시스템이나 매체에서도 파일을 다룰 수 있다페이지 캐시 디스크 파일 시스템에서 최근에 접근한 데이터를 저장하는 메모리 저장소 메모리에 쓰기를 요청한 데이터를 저장하면 동일한 데이터에 대한 요청이 연이어 발생할 경우 커널은 반복적인 Disk 접근을 피해서 메모리에서 바로 처리할 수 있다. Temporal Locality 라는 개념을 활용함 특정 시점에서 리소스에 접근하면 오래 지나지 않은 장래에 다시 또 접근할 가능성이 높다는 이론 Sequential Locality 데이터가 순차적으로 참조됨을 뜻함 이를 활용하기 위해 페이지 캐시 미리 읽기를 구현하고 있음 커널이 파일시스템 데이터를 탐색하는 첫번째 장소가 페이지 캐시 이다. 동적으로 페이지 캐시 크기 변경 가능 메모리가 가득차게되면 페이지 캐시 중에서 가장 적게 사용한 페이지를 삭제해서 메모리를 확보한다. 이런 작업은 자동적으로 매끄럽게 일어남. 디스크 스왑과 캐시 삭제 간의 균형을 맞추는 데는 휴리스틱 기법을 사용함 페이지 쓰기 저장 커널은 버퍼를 통해 쓰기 작업을 지연시킨다. 쓰기 요청을 하면 버퍼로 데이터를 복사한 다음 버퍼에 변경 표시를 하여 디스크에 있는 복사본보다 메모리에 있는 복사본이 새롭다고 알려준다. 그러면 쓰기 요청은 바로 반환된다. 최종적으로 버퍼에 있는 내용이 디스크로 반영되어 디스크와 메모리에 있는 데이터가 동기화가 되어야하는데 이를 쓰기 저장이라고 한다. 두 가지 상황에서 발생함 여유 메모리가 설정된 경계 값 이하로 줄어들면 변경된 버퍼를 디스크에 기록한 다음, 버퍼를 삭제해서 메모리 공간을 확보한다. 설정된 값보다 오랫동안 유지된 버퍼는 디스크에 기록된다. 이는 변경된 버퍼가 무한정 메모리에만 남아있는 상황을 방지한다. 쓰기 저장은 Flusher 스레드 라고 하는 커널 스레드 무리에서 수행함2.12 마무리 가능한 모든 것을 파일로 표현하는 리눅스 같은 시스템에서는 어떻게 파일을 열고, 읽고, 쓰고, 닫는지 이해하는 것이 매우 중요하다. 이 모든 연산은 유닉스의 고전이며 여러 표준에 기술되어 있다." }, { "title": "[BlockChain] Blockchain Transaction flow", "url": "/posts/blockchain/", "categories": "Trouble Shooting, Blockchain", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-03-05 00:00:00 +0900", "snippet": "Blockchain Transaction flow1. 작동 시작만약 민지가 철수한테 비트코인을 보내고 싶어한다면, 민지는 비트코인이 저장되어있는 휴대폰이나 컴퓨터의 비트코인 지갑 어플을 열어볼것이다.(지갑 어플은 보통 공짜로 받을 수 있음)(비트코인이나 이더리움은 단순히 블록체인의 특정 기능을 이용한 암호화폐인데, 비트코인은 비트코인만 거래하고, 이더리움은 이더만 거래할 수 있음! 그러니깐 민지가 사용하는 지갑 어플은 비트코인 지갑일 것이다.)2. Smart Contract가 발동됨민지가 네트워크에 거래를 보내게 되면, 네트워크의 노드들이 smart contract를 발동한다. 근데 이제 그 smart contract가 어떤일을 하냐면, 민지가 사용할 수 있는 비트코인이 있는지 아니면 이미 다 써버렸는지를 확인한다.확인이 되면 그 거래는 제안된 블록(proposed block)에다가 추가된다.3. Operators가 Transaction을 뿌림제안된 블록이 이제 peer-to-peer 프로토콜을 통해서 네트워크에 뿌려진다4. 합의하기이제 블록체인 시스템에서 가장 중요한 부분이라고 볼 수 있는 지점이다.비트코인 네트워크에서 그 블록 (이전에 제안된 블록)을 검증하기 위해서, 노드들이나 채굴꾼들이 수학문제 (아주 어려운) 를 계산하면서 유효성 검사를 하게된다. 비트코인은 이러한 검증을 Proof of Work 라는 개념을 통해서 진행한다.(요 부분은 추후에 다시 다룰 예정. 비잔티움 장애 허용이라는 문제를 해결하기 위해서 비트코인이 제안한 방법)어쨌든 그 Proof of Work 의 수식을 가장먼저 해결한 노드는 새롭게 채굴된 비트코인을 보상받는다.일단 그 문제(수식)의 솔루션이 나오면 다른 노드들은 쉽게 이 제안된 노드가 정확한지 알 수 있게되고 새로운 블럭은 블록체인 네트워크에 추가된다5. 새로운 블록 뿌리기이 블록은 처음에 제안된 거래를 위해서 우리가 사용하는 peer-to-peer 커뮤니케이션을 통해 네트워크에다가 뿌려지게 된다.Block operator가 새로운 블록의 사본을 받게되면 분산 장부에다가 그 사본을 추가한다. 이건 현재 네트워크에 참여하고 있는 멤버들이 모두 현 상태에 대해서 동의한다는 것을 보장한다!6. 거래 완료사용자의 지갑은 사용자와 연관된 거래가 포함된 새로운 블록이 생성되는지를 계속 바라보고 있다. 사용자의 작동에 대한 코드가 포함된 블록이 있다는 것을 확인하게 되면, 그 사용자가 요청했던 동작이 수행되었다고 지갑이 알람을 보낸다.민지가 철수에게 비트코인을 전송한다는 내용을 담은 블록이 블록체인 시스템에 추가되면, 그 거래가 영향을 끼친 지갑에 알람을 보내줄 것이다.출처 : https://learning.edx.org/course/course-v1:LinuxFoundationX+LFS170x+2T2021/" }, { "title": "[Shell] Shell script에서의 특수문자 사용", "url": "/posts/tb-shell/", "categories": "Trouble Shooting, Shell", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-02-21 00:00:00 +0900", "snippet": "Shell script에서의 특수문자 사용Shell 스크립트를 이용해서 Mysql 데이터를 백업하는 mysqldump 작업 로직을 구성하고 싶었다.가장 간단하게 짜본 스크립트는 아래와 같다.#!/bin/bashHOST_BETA=&quot;HOST_BETA&quot;HOST_STAGE=&quot;HOST_STAGE&quot;HOST_PROD=&quot;HOST_PROD&quot;PORT=&quot;PORT&quot;USER=&quot;USER&quot;PASSWORD=&quot;PASSWORD&quot;DB=&quot;DB&quot;PHASE=$1if [ $1 = &quot;beta&quot; ]; then echo &quot;mysqldump -h$HOST_BETA -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql&quot; `mysqldump -h$HOST_BETA -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql`elif [ $1 = &quot;stage&quot; ]; then echo &quot;mysqldump -h$HOST_STAGE -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql&quot; `mysqldump -h$HOST_STAGE -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql`elif [ $1 = &quot;prod&quot; ]; then echo &quot;mysqldump -h$HOST_STAGE -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql&quot; `mysqldump -h$HOST_PROD -P$PORT -u$USER -p$PASSWORD --single-transaction --default-character-set=utf8 --extended-insert=FALSE $DB &amp;gt; ./log/$(date +\\%Y\\%m\\%d)_${DB}_$PHASE.sql`else echo &quot;Usage:&quot; echo &quot;(phase:beta or stage or prod)&quot; exit 0fi그런데 자꾸mysqldump: Got error: 1045: Access denied for user &#39;USER&#39;@&#39;host&#39; (using password: YES) when trying to connec접속 에러가 발생하는 것었다.분명 echo로 나오는 커맨드를 터미널에서 직접 입력했을때는 잘 작동이 되었는데, shell script를 실행했을 때는 에러가 발생했다.Trouble Shooting 1. shell script ip 문제?처음에는 에러 메시지에서 host로 나오는 ip가 나의 localhost ip가 아니고ifconfigutun2: flags=8051&amp;lt;UP,POINTOPOINT,RUNNING,MULTICAST&amp;gt; mtu 1400 inet HOST --&amp;gt; HOST netmask 0xffffffffifconfig 명령어를 쳤을 때 마지막에 나오는 utun2 가상 인터페이스 호스트가 에러 메시지에 나오길래 shell script에서 가져오는 현재 아이피가 다른 것이라서 MySQL 연결이 안되는건가? 라고 생각을 했다.하지만 MySQL의 mysql 디비에서 user 접근 권한을 확인해봤더니mysql&amp;gt; use mysql;mysql&amp;gt; select host,user from user;+-----------+------------------+| host | user |+-----------+------------------+| % | USER |+-----------+------------------+요런식으로 ‘USER’@’%’ 모든 ip에서 접속 가능하도록 등록이 되어있었다.이 에러는 원인이 아니었다.Trouble Shooting 2. 특수문자 문제?MySQL 접속 비밀번호에는 특수문자가 포함되어있다.터미널에서 mysql에 접속하는 커맨드를 이용할 때 비밀번호에 특수문자가 포함되어있다면 \\를 앞에 넣어줘야한다.예를 들어, 비밀번호가 123!@#이라면mysql -h HOST -P PORT -u USER -p123\\!\\@\\# DB이런식으로 특수문자 앞에 \\를 넣어줘야 인식이 된다.예시$ echo (&amp;gt;$ echo \\((그래서 나는 스크립트에도 같은 식으로 패스워드를 넣었다.그런데 shell 에서는 특수문자를 받을 수 있어서 \\라는 문자를 그대로 받아버려서 비밀번호 에러가 발생한 것이었다.참고 stackoverflow에서 보면 알 수 있듯이, &quot;&quot; 이런식으로 쌍따옴표를 이용해서 감싸기도 하나보다.어쨌든 terminal에서는 \\를 붙여서 특수문자 표기를 하지만 shell에서는 그렇지 않다." }, { "title": "[SQLalchemy] NULL 조회 ", "url": "/posts/tb02-sqlalchemy/", "categories": "Trouble Shooting, SQLalchemy", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-02-17 00:00:00 +0900", "snippet": "SQLalchemy에서의 NULL 조회cluster_item = self.db.query(Cluster)\\ .filter((Cluster.userid.is_(None)) &amp;amp; (Cluster.domain.is_(None)))\\ .order_by(Cluster.id).first()위와 같이 NULL을 조회했었는데, 정상적으로 조회가 되지 않았다.살펴보니 방법이 달랐다.NULL 조회시userid is None으로 하면 안된다.userid == None# ORuserid.is_(None)이렇게 해야한다.두개 이상 키의 NULL을 찾고 싶을 때userid.is_(None) and domain.is_(None)and로 묶으면 안된다..filter((Cluster.userid.is_(None)) &amp;amp; (Cluster.domain.is_(None)))이런식으로 구분해줘야 한다.참고 : https://veluxer62.github.io/explanation/sqlalchemy-filter-is-null/" }, { "title": "[Kubernetes] Kubernetes 서비스 정리(ClusterIP, Nodeport, Loadbalancer) ", "url": "/posts/k8s_service/", "categories": "Kubernetes", "tags": "kubernetes, k8s, blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-02-14 00:00:00 +0900", "snippet": "Kubernetes ServiceClusterIP말 그대로 클러스터 내부에서 사용하는 IP클러스터 내부의 노드나 파드에서는 ClusterIP를 이용해서 서비스에 연결된 각 파드들에 접근한다.하지만 외부에서는 이 IP로 접근 불가능apiVersion: v1kind: Servicemetadata: name: testspec: ports: - port: 8080 protocol: TCP targetPort: 8080 selector: app: testport 로 접근 시 targetport 로 리다이렉팅NodePort클러스터 내부외 외부 모두 접근이 가능하다.Nodeport는 클러스터의 모든 노드에 특정 포트를 열어두고 어떤 노드에 접근하든지 모두 다 포트 포워딩을 해준다.apiVersion: v1kind: Servicemetadata: name: testspec: selector: app: test type: NodePort ports: - name: 30088-5000 nodePort: 30088 # 서비스가 클러스터 외부로 노출하는 포트 protocol: TCP port: 30001 # 서비스가 클러스터 내부로 오픈하는 포트 targetPort: 5000 # 타겟 파드에 요청을 보내는 포트즉, 외부에서 30088로 접근하면 30001로 포트포워딩 되고 다시 5000번으로 포트 포워딩 됨.LoadbalancerNodeport + ClusterIP 라고 보면 될 것 같다.LoadBalancer: 클라우드 공급자의 로드 밸런서를 사용하여 서비스를 외부에 노출시킨다. 외부 로드 밸런서가 라우팅되는 NodePort와 ClusterIP 서비스가 자동으로 생성된다.(요 부분은 혹시 틀린 내용이 있다면 지적부탁드립니다.)이전 ClusterIP, Nodeport 와는 다르게 External IP가 생성된다. 이를 통해서 외부에서 접근이 가능함.Nodeport의 확장판이라고 생각할 수 있는데, 외부에 노출이 가능하다는 점이 가장 큰 차이점이다. 또한 Nodeport 앞단에 Loadbalancer가 붙어서 헬스체크를 하면서 알고리즘을 통해 트래픽을 전달한다.kind: ServiceapiVersion: v1metadata: name: test annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: &quot;true&quot;spec: selector: app: test type: LoadBalancer ports: - name: http1 port: 30080 targetPort: 5000 - name: http2 port: 30081 targetPort: 5000 - name: http3 port: 30082 targetPort: 5000get svc 를 하게 되면 LoadBalancer의 External IP와 Ports가 나오게된다.NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S)service/test LoadBalancer &amp;lt;CLUSTER-ip&amp;gt; &amp;lt;external-ip&amp;gt; 30080:30352/TCP,30081:31824/TCP,30082:31996/TCP근데 yaml에서 30352 포트를 사용한적이 없는데 Ports에서 30080:30352 라고 나온다.이게 무엇인고 하니, 위의 그림처럼 LoadBalancer를 만들게 되면 자동으로 Nodeport가 생성이 된다.그래서 아래와 같이 describe service를 하게 되면 Nodeport 가 연결되어있는 것을 확인할 수 있다.Name: default-testNamespace: defaultLabels: &amp;lt;none&amp;gt;Annotations: service.beta.kubernetes.io/openstack-internal-load-balancer: trueSelector: app=testType: LoadBalancerPort: http1 30080/TCPTargetPort: 5000/TCPNodePort: http1 30352/TCPPort: http2 30081/TCPTargetPort: 5000/TCPNodePort: http2 31824/TCPPort: http3 30082/TCPTargetPort: 5000/TCPNodePort: http3 31996/TCPSession Affinity: NoneExternal Traffic Policy: ClusterEvents: &amp;lt;none&amp;gt;describe 해서 나온 값들을 보면Port: http1 30080/TCPTargetPort: 5000/TCPNodePort: http1 30352/TCP요렇게 되어있다.즉, 30080 -&amp;gt; 30352 -&amp;gt; 5000 이렇게 리다이렉팅이 된다는 의미이다!참고 : https://nearhome.tistory.com/95 , https://ooeunz.tistory.com/123, https://kim-dragon.tistory.com/52" }, { "title": "[Docker] Centos Image 에서 sudo command not found 에러", "url": "/posts/docker/", "categories": "Trouble Shooting, Docker", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, GitHub Pages", "date": "2022-02-07 00:00:00 +0900", "snippet": "Centos Image 에서 sudo command not foundcentos/python-36-centos7 이미지 사용 도중Dockerfile 내부에서 커맨드로 파일의 모드를 바꿔야할 일이 생겼다.RUN sudo chmod 755 start.sh근데 sudo 를 실행했을 떄 /bin/sh: sudo: command not found요런 에러가 발생한다.ubuntu 의 경우는 apt-get update 명령어를 통해 업데이트를 진행하면 되지만, centos의 경우는 yum update 또한 sudo 명령어가 필요하다.su -s 또한 password를 요구하기 때문에User 를 root 로 접속하여서 sudo 를 다운받았다.USER rootRUN yum install -y sudoRUN sudoUSER 명령어가 없다면, 기본적으로 default 유저로 접속이 된다.별 것 아니지만, 누군가에겐 도움이 되길 바란다!ps. 근데 root 접속이 가능하다면, 굳이 sudo 다운받지 않고 그냥 커맨드 실행하면 된다.." }, { "title": "[Linux] Sudoer 및 sudo 사용하기. sudo passwd 실행 안될 때", "url": "/posts/sudoer/", "categories": "Linux, Centos", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-02-03 00:00:00 +0900", "snippet": "Sudoer 및 sudo 사용하기root 권한root 는 일반적으로 unix에 존재하는 특권 사용자의 이름을 의미하는데 모든 권한을 가지고 있다고 보면 되겠다.매우 강력하여 편리하게 보이지만 위험할 수 있기 때문에 모든 계정에 root 권한을 주지는 않는다.일반 계정이 sudo 명령어로 root 권한을 가진 채 커맨드를 이용하기 위해서는 sudoer 파일에 필요한 부분을 적어서 사용하게 된다.Sudoer란sudoer란 일반 계정이 sudo 명령어를 사용해서 임시로 root 권한을 얻어 이용할 수 있는 것을 의미한다.또한 root 권한을 계정에 부여할 수도 있다.파일의 위치는 /etc/sudoers 이다.예를 들어deploy ALL=NOPASSWD: ALL, !/bin/su, !/sbin/reboot, !/usr/bin/reboot, !/sbin/shutdown, !/sbin/halt, !/usr/bin/halt, !/sbin/poweroff, !/usr/bin/poweroff, !/sbin/init, !/usr/sbin/adduser, !/usr/sbin/useradd, !/usr/sbin/userdel, !/sbin/iptables, !/usr/bin/passwd이런식으로 deploy 계정에 대해 사용할 수 있는 커맨드와 사용할 수 없는 커맨드들이 적혀있다.앞에 ! 느낌표가 붙은 커맨드는 sudo 명령어로 사용하게 되면 아래처럼 에러 문구가 발생한다.죄송하지만 deploy 사용자는 &#39;/usr/sbin/useradd TEST&#39;을(를) root(으)로 실행하도록 허가받지 않았습니다.sudo -s-s (–shell) 옵션은 쉘을 의미한다.즉, root 권한을 가진 shell을 새롭게 연다는 것이다.sudo passwd (*) 실행 안될 때/etc/sudoers 에서 deploy 계정의 경우 !/usr/bin/passwd 이렇게 passwd 명령어를 sudo 로 사용할 수 없게 막혀있는 것을 확인할 수 있다.[deploy@ ~]$ sudo passwd-&amp;gt; 죄송하지만 deploy 사용자는 &#39;/usr/bin/passwd&#39;을(를) root(으)로 실행하도록 허가받지 않았습니다.[deploy@ ~]$ sudo -s[root@ ~]# sudo passwd-&amp;gt; root 사용자의 비밀 번호 변경 중-&amp;gt; 새 암호:요런식으로 sudo (명령어) 로는 안되지만, sudo -s 옵션으로 들어가서는 되는 것을 확인할 수 있다." }, { "title": "[MySQL] == NULL 값 조회 안되는 이슈 ", "url": "/posts/mysql/", "categories": "Trouble Shooting, MySQL", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-01-21 00:00:00 +0900", "snippet": "[MySQL] == NULL 값 조회 안되는 이슈MySQL 쿼리문을 작성할 때 필드 값이 NULL인 Row를 조회하고 싶을 때가 있다.근데 아무 생각 없이SELECT * FROM your_table WHERE your_field = NULL이라고 썼더니 조회가 안된다.결론부터 이야기하자면SELECT * FROM your_table WHERE your_field IS NULLNULL은 요런식으로 IS 혹은 IS NOT 문을 사용해서 조회해야한다.이제부터 그 이유를 알아보자.MySQL에서 NULL의 의미NULL은 놓친 알 수 없는 값을 의미한다. 그리고 NULL은 다른 값들과는 다르게 대우된다.NULL을 테스트 해보기 위해서 IS NULL 과 IS NOT NULL을 아래와 같이 실행해보면mysql&amp;gt; SELECT 1 IS NULL, 1 IS NOT NULL;+-----------+---------------+| 1 IS NULL | 1 IS NOT NULL |+-----------+---------------+| 0 | 1 |+-----------+---------------+이렇게 나온다.아래의 테스트에서 볼 수 있듯이 수학적인 기호 (=, &amp;lt;, &amp;lt;&amp;gt;) 이런 것들은 쓸 수가 없다. &amp;lt;&amp;gt; 는 not equal을 의미한다.mysql&amp;gt; SELECT 1 = NULL, 1 &amp;lt;&amp;gt; NULL, 1 &amp;lt; NULL, 1 &amp;gt; NULL;+----------+-----------+----------+----------+| 1 = NULL | 1 &amp;lt;&amp;gt; NULL | 1 &amp;lt; NULL | 1 &amp;gt; NULL |+----------+-----------+----------+----------+| NULL | NULL | NULL | NULL |+----------+-----------+----------+----------+왜 NULL에는 수학적 기호를 사용하지 못할까?왜나하면 NULL의 수학적 비교 값은 여전히 NULL이기 때문이다. 이러한 비교를 통해서는 여전히 어떠한 의미있는 값을 얻을 수가 없다.MySQL에서는 0 혹은 NULL은 false를 의미하고 다른 값들은 true를 의미한다.boolean 연산에서 기본적인 true 값은 1이다.일반적인 에러는 NOT NULL인 칼럼에 0 혹은 빈 문자열 &#39;&#39;이 못들어간다고 생각하는데 그렇지 않다.NULL은 값이 없다 라는 의미이긴 하지만 사실 값은 값이다…mysql&amp;gt; SELECT 0 IS NULL, 0 IS NOT NULL, &#39;&#39; IS NULL, &#39;&#39; IS NOT NULL;+-----------+---------------+------------+----------------+| 0 IS NULL | 0 IS NOT NULL | &#39;&#39; IS NULL | &#39;&#39; IS NOT NULL |+-----------+---------------+------------+----------------+| 0 | 1 | 0 | 1 |+-----------+---------------+------------+----------------+-&amp;gt;조금 말이 어렵긴한데,,,(NULL에 대해서는 꽤 많은 문제점들이 시사되고 있다.) 어쨌든 NULL에다가 수학적 비교를 하게 되면 그 값 또한 NULL이 되기 때문에 WHERE 문에서 적용이 안된다. 따라서 IS NULL 혹은 IS NOT NULL 과 같은 구체적인 비교 구문으로 값을 찾아야한다.원문 참조 : https://dev.mysql.com/doc/refman/8.0/en/working-with-null.html" }, { "title": "[Crontab] [Tip] Crontab 이슈 해결 ", "url": "/posts/crontabl/", "categories": "Trouble Shooting, crontab", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-01-20 00:00:00 +0900", "snippet": "[Crontab] [Tip] Crontab 이슈 해결crontab 을 써보면서 겪었던 몇 가지 이슈를 해결하는 과정을 기록하려 한다.자주 사용하시는 분들은 당연하게 생각하는 것들을 몰랐던 나는 검색을 통해 해결했는데, trouble shooting 을 하는 다른 분들께도 도움이 되었으면 좋겠다.Permission denied*/30 * * * * ./my.sh이런식으로 crontab을 30분 간격으로 실행되도록 등록했다.그런데 아무런 변화가 없었다.그래서 로그를 살펴보았다.Crontab log/var/log/cron 에 기록이 되며 root 권한이기 때문에sudo tail -f /var/log/cron이런식으로 확인하면 되겠다.그런데 나같은 경우는 파일의 권한이 -rw-r–r–.(744) 로 되어있었는데, 이 부분을 755로 변경해주니 permission denied 이슈는 해결되었다.Crontab 실행 shell 로그 찍기permission denied 는 해결하였는데, 계속 실행이 안되었다.그래서 crontab으로 실행하는 쉘 스크립트의 로그를 찍어보았다.요 부분은(crontab -e)*/30 * * * * ./my.sh &amp;gt;&amp;gt; {LOG_DIR}/cron.log 2&amp;gt;&amp;amp;1이런식으로 로그를 직접 저장을 하면서 확인을 하면 되겠다.그래서 내가 확인한 이슈는 다음과 같다.Command not foundkubectl not found현재 수행하는 로직은 Crontab -&amp;gt; Shell -&amp;gt; Python 을 수행하고 있다. Python 에서는 subprocess call을 통해서 kubectl을 불러오는데, 이 부분에서 command not found 에러가 발생하였다.이유는 이러하다. 우리가 서버를 접속하거나 탭을 켜게 되면 자동으로 ~/.bash_profile 혹은 ~/.bashrc 가 실행이 된다. 이를 통해서 필요한 환경변수들을 불러오게 된다. 하지만 crontab을 통한 실행에서는 환경변수를 가지고 가지 않는다. 따라서 여러 가지 환경변수들을 crontab에 직접 저장을 해주어야한다. 대표적으로 /usr/local/bin 이 그러하다. crontab 의 기본 PATH는 /usr/bin이다. 즉, /usr/local/bin 등 다른 경로에 있는 command에 대해서는 crontab이 인식하지 못한다.$ which kubectl$ /usr/local/bin/kubectl이라고 뜨는 것을 확인할 수 있다.따라서 아래처럼 crontab -e 를 통해 편집 탭에서 환경변수를 직접 넣어주어야한다.PATH=/usr/bin:/bin:/usr/sbin:/sbin:/usr/local/binLD_LIBRARY_PATH=/usr/local/lib* * * * * my_command.sh some_argsPython 로그 안찍힘마지막으로는 Python 로그가 찍히지 않는 이슈이다.위에서 언급한 것 처럼 나는 Crontab -&amp;gt; Shell -&amp;gt; Python 로 작업을 수행했는데, Python 에서 loguru 모듈을 통해 찍고 있는 로그가 저장이 안되는 이슈가 발생하였다.아마 crontab에서 수행하다 보니 경로가 꼬인 것으로 확인되는데,,, 그래서 나는 그냥 crontab의 결과 값을 다시 한번 로그로 저장하도록 구현했다.* * * * * my_command.sh &amp;gt;&amp;gt; {LOG_DIR}/logs/cron_$(date +\\%Y\\%m\\%d\\%H\\%M\\%S).log 2&amp;gt;&amp;amp;1로그가 찍힐 때의 날짜 및 시간을 저장하기 위해서 Shell 스크립트 문법인 $(date +\\%Y\\%m\\%d\\%H\\%M\\%S) date를 사용하였다. 요 부분은 디렉토리가 꼬여서 저장이 안되고 있었던 것으로 확인되었다. 즉, 정상적으로 파이썬 로그는 찍힘." }, { "title": "[MongoDB] MongoDB 버전차이 연결 실패 에러 ", "url": "/posts/mongo/", "categories": "Trouble Shooting, MongoDB", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-01-14 00:00:00 +0900", "snippet": "MongoDB 버전차이 연결 실패 에러서버와 서버 사이 ACL은 telnet으로 확인했을 때 접속이 잘 되었다.하지만 몽고디비 접속이 안되었다.접속 커맨드$ mongo --host db_name/IP01:27017,IP02:27017,IP03:27017 --username name --password --authenticationDatabase DB_name DB_nameMongoDB shell version v3.4.13Enter password:에러로그connecting to: mongodb://IP01:27017,IP02:27017,IP03:27017/DB_name?replicaSet=DB_name2022-01-14T09:51:00.815+0000 I NETWORK [thread1] Starting new replica set monitor for DB_name/IP01:27017,IP02:27017,IP03:270172022-01-14T09:51:00.817+0000 I NETWORK [ReplicaSetMonitor-TaskExecutor-0] Successfully connected to IP01:27017 (1 connections now open to IP01:27017 with a 5 second timeout)2022-01-14T09:51:00.817+0000 I NETWORK [thread1] Successfully connected to IP02:27017 (1 connections now open to IP02:27017 with a 5 second timeout)2022-01-14T09:51:00.843+0000 I NETWORK [thread1] Detected bad connection created at 1642153860816699 microSec, clearing pool for IP02:27017 of 0 connections2022-01-14T09:51:00.843+0000 I NETWORK [thread1] Dropping all pooled connections to IP02:27017(with timeout of 5 seconds)2022-01-14T09:51:00.843+0000 I NETWORK [thread1] Ending connection to host IP02:27017(with timeout of 5 seconds) due to bad connection status; 0 connections to that host remain open2022-01-14T09:51:00.843+0000 I NETWORK [ReplicaSetMonitor-TaskExecutor-0] Detected bad connection created at 1642153860816672 microSec, clearing pool for IP03:27017 of 0 connections2022-01-14T09:51:00.843+0000 I NETWORK [ReplicaSetMonitor-TaskExecutor-0] Dropping all pooled connections to IP03:27017(with timeout of 5 seconds)2022-01-14T09:51:00.843+0000 I NETWORK [ReplicaSetMonitor-TaskExecutor-0] Ending connection to host IP03:27017(with timeout of 5 seconds) due to bad connection status; 0 connections to that host remain open처음에는 방화벽 문제인 줄 알았으나, 에러로그를 살펴보면 Connection은 처음에 성공적으로 확인되는 것을 볼 수 있다.혹시 몰라서 MongoDB 서버의 버전과 클라이언트의 MongoDB 버전을 확인해봤더니 차이가 좀 났다.Original Client Version$ mongo --versionMongoDB shell version v3.4.13git version: fbdef2ccc53e0fcc9afb570063633d992b2aae42OpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment:distmod: rhel70distarch: x86_64target_arch: x86_64Updated Client Version$ mongo --versionMongoDB shell version v5.0.5Build Info: {&quot;version&quot;: &quot;5.0.5&quot;,&quot;gitVersion&quot;: &quot;d65fd89df3fc039b5c55933c0f71d647a54510ae&quot;,&quot;openSSLVersion&quot;: &quot;OpenSSL 1.0.1e-fips 11 Feb 2013&quot;,&quot;modules&quot;: [],&quot;allocator&quot;: &quot;tcmalloc&quot;,&quot;environment&quot;: {&quot;distmod&quot;: &quot;rhel70&quot;,&quot;distarch&quot;: &quot;x86_64&quot;,&quot;target_arch&quot;: &quot;x86_64&quot;}}Destination MongoDB Server version$ mongo --versionMongoDB shell version v4.2.14git version: 0e6db36e92d82cc81cbd40ffd607eae88dc1f09dOpenSSL version: OpenSSL 1.0.1e-fips 11 Feb 2013allocator: tcmallocmodules: nonebuild environment:distmod: rhel70distarch: x86_64target_arch: x86_64MongoDB 서버의 버전은 4.2버전대였고, 접속 시도한 클라이언트의 버전은 3.4버전대였다.둘 사이의 버전 차이가 커서 접속에 실패했던 부분이었다.클라이언트를 5.0으로 업데이트하니 문제가 해결되었다.참고 : https://www.mongodb.com/community/forums/t/unable-to-connect-to-cluster-no-primary-detected/90700/17" }, { "title": "[독서][부의 추월차선] 부의 추월차선은 어떻게 갈 수 있을까 ", "url": "/posts/book01/", "categories": "Study, Book", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-01-08 00:00:00 +0900", "snippet": "[독서][부의 추월차선] 부의 추월차선은 어떻게 갈 수 있을까제목부터 상당히 자극적이다.부의 추월차선고속도로는 일반적으로 차선의 용도가 정해져있다.4차선 도로를 예로 든다면 4차선은 화물차량용, 2, 3차선은 서행차선용, 1차선은 추월차선으로 이용된다.천천히 달리고 싶은 차들은 2, 3차선에서 적정속도로 달린다.하지만 더 빠르게 가고 싶은 차들은 1차선에서 다른 차들을 추월하며 달린다.부의 추월차선. 과연 부에도 추월차선이 있어서 우리가 그 도로로 달린다면 서행하지 않고 빠르게 갈 수 있을까.그런 생각이 가장 먼저 든 책이었다.부자가 되겠다는 생각을 항상 하고 있는 나로서는 굉장히 매력적인 책 표지였다.이 책에서는 기본적으로 삶의 지도에는 인도, 서행차선, 추월차선이 존재한다고 이야기한다.부자가 되기 위해서는 추월차선에서 빠르게 달려야만 하고 인도와 서행차선으로 달렸을 경우 부자가 될 수 있는 확률은 희박하다고 말한다.가장 흥미로웠던 것은 추월차선을 통해서 빠르게 부자가 되는 것이 가능하다는 것을 증명하기 위해 부의 방정식을 제시했다.부의 방정식부에는 방정식이 있다. 하지만 자신이 위치한 차선에 따라서 방정식이 바뀐다.인도의 부의 방정식 부 = 소득 + 빚인도를 달리는 사람들은 현재를 사는 사람들이다. 미래에 대한 생각 없이 현재를 즐기며 살아간다.따라서 미래에 본인이 짊어질 부담인 빚을 계속해서 만들며 현재를 더 풍족하게 시간을 보낸다.서행차선의 부의 방정식 부 = (주요 수입원 : 직업) + (부의 증식방법 : 투자)일반적인 사람들이 살아가는 방식이다.현재를 희생하며 안정된 미래를 꿈꾼다.서행차선의 부의 방적식은 다시 말해원금가치 + 복리 이자로 해석할 수 있다.즉, 원금 가치는 자신의 능력 X 일할 수 있는 시간이 되고,복리 이자는 투자 자금으로 투자를 통해 얻을 수 있는 가치가 된다.하지만 이 책에서는 서행차선의 부의 방정식은 매우 오랜 시간이 걸리고 확률 또한 희박하다고 이야기한다.우리는 부자가 되기를 원한다. 하지만 60대가 넘어 하고 싶은 일들을 쉽게 하지 못하는 때에 부자가 되는 것보다 몇 십년 더 젋을 때, 혈기가 왕성할 떄 부자가 되는 것이 더 좋다는 것은 당연한다.서행차선으로 달리면 빠른 시간에 부자가 되는 것은 매우 힘들다.복리 이자가 적어도 5% 이상의 이익을 매번 기록한다는 것도 확률적인 부분에 우리의 미래를 건다고 볼 수 있다.원금가치를 자신의 능력 X 일할 수 있는 시간 라고 이야기 했다.여기서 우리가 변화시킬 수 있는 변수는 두 가지이다.자신의 능력을 변화시키는 것과 일할 수 있는 시간을 변화시키는 것.먼저 일할 수 있는 시간를 보자.이것은 아주 한정적이다. 일반적으로 우리는 주 40시간을 일한다. 우리의 능력이 정해져있다면 우리는 일할 수 있는 시간을 늘려야하는데, 24시간의 시간적 흐름에 존재하는 우리는 매우 한정적인 시간이라는 변수를 가지고 있다.그렇다면 자신의 능력을 극대화한다면? 맞다. 부자가 될 수 있다.예를 들어 매우 뛰어난 운동선수가 되거나 (르브론 제임스, 메시,,,,) 아주 뛰어난 끼로 연예인이 되거나 한가지 분야에서 탁월한 능력을 보이면 가능한 부분이다.하지만 이런 사람들은 우리가 이야기하는 내용의 요지에서 벗어난 사람들이다.여기서는 우리 같이 평범하고 일반적인 사람들을 기준으로 이야기하고 있다.즉, 우리는 서행차선에서 우리 삶의 통제력을 본인이 가지고 있는 것이 아니라 시간과 운이라는 외부에 맡기고 있는 셈이다.추월차선의 부의 방정식추월차선과 서행차선의 가장 큰 차이점은 통제권을 누가 가지고 있느냐이다.추월차선을 달리는 사람은 통제권을 본인이 가지고 있다.변수 또한 무한대로 늘어날 수 있다. 따라서 빠르게 부자되는 것이 가능하다.하지만 빠르게 부자가 된다는 것은 쉽게 부자가 된다는 것은 절대 아니다.결과가 있기 위해서는 과정이 분명히 존재한다. 그 과정은 매우 고되고 힘들 수 있다. 하지만 우리가 바라는 결과를 위해서는 그런 과정을 겪는 것을 즐길 수 있어야 한다.우리는 삶을 소비자로서 사는 것이 아닌 생산자로서 살아가야한다.살아가면서 사용하는 여러 소비재들, 음식 그리고 서비스들을 단순히 소모하고 이용하는 소비자의 마인드가 아니라 그것들을 어떻게 경쟁력있게 제공하고 고객들을 끌어당길 수 있는지에 대해 생각을 하며 살아가야한다.추월차선의 부의 방정식은 이러하다. 부 = 순이익 + 자산 가치순이익 = 판매 개수 X 단위당 이익자산 가치 = 순이익 X 산업 승수사실상 추월차선은 본인의 사업을 하지 않고서는 달릴 수 없다.순이익이란 자신이 판매하는 사업 아이템을 얼마나 이익을 보면서 많이 파는가에 대한 내용이다.자산 가치는 본인 사업의 가치에 대한 이야기를 하는 것이다.서행차선의 한계가 있는 변수와는 다르게 추월차선에서의 변수들은 한계가 없다. 몇개나 파는지는 사업의 종류에 따라 다르지만 무한정 늘릴 수 있다. 이익 또한 어떻게 구성하냐에 따라서 무한히 늘어날 수 있다.정리내가 아직 더 큰 세상을 경험하지 못해서 그런 것일 수도 있겠지만, R = VD는 성공 공식에 대해서 어디에서나 나오는 이야기인 듯 하다.지각을 바꾸면 미래의 행동이 바뀐다 거나 본인이 생각하는 마음가짐을 확신에 찬 어조로 바꾼다면 다가올 상황이 바뀐다거나…이 모든 것은 시크릿이야기로 흘러들어간다.될 수 있다고 믿고, 할 수 있다고 믿어라.이미 그렇게 되었다고 생각을 하라.시각화를 통해 더 실감나고 생생한 상상을 하라.이 모든 것들은 시크릿 내용이다.이 우리가 과학적으로 밝혀내지 못한 외부의 흐름들을 우리는 어떻게 받아들여야 할까.사실 나는 믿거나 말거나라고 생각한다.이 또한 종교와 닮아있다. 종교도 어떤이에게는 매우 유익할 것이다. 의지할 곳 없는 사람이 기댈 수 있는 안식처를 제공해주니깐.하나의 철학인 것이다.자신의 흔들리지 않는 철학적인 뼈대가 없다면 어디든 잡아야한다. 그게 종교가 될 수도 있고, 시크릿의 철학이 될 수 있다.하지만 그 철학을 믿음으로서 남에게 피해를 끼친다면 지금 이야기하려는 내용에서 벗어난 것일테니 번외로 하겠다.부자가 되는 것 또한 그렇다.나는 이미 부자다.삼성역 근처 람보르기니 매장에서 노란색 우라칸을 계약하고 강남대로를 쌩하고 달리는 내 모습이 보인다.이런 결과를 위해서 나는 생산자가 되기 위한 과정을 거칠 것이다.항상 주변을 예리하게 살펴보자.나는 이미 부자다!" }, { "title": "[번역] Google Pro Tip: 대략적인 계산방식을 통한 최적의 디자인을 하기", "url": "/posts/eng02-google/", "categories": "Translate", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2022-01-05 00:00:00 +0900", "snippet": " 다양한 기술 포스팅들이 올라오는 http://highscalability.com/ 사이트에는 양질의 개발관련 글이 올라온다. 그 중 시스템 설계를 위한 대략적인 계산 법에 대해 좋은 칼럼이 있어서 번역글을 작성해본다. 부족한 번역 실력 양해 부탁한다.원본 링크Google Pro Tip: 대략적인 계산방식을 통한 최적의 디자인을 하기[Back-Of-The-Envelope-Calculations : 일반적으로 봉투와 같은 사용 가능한 종이 조각에 적어 놓은 대략적인 계산을 의미한다.]주어진 문제에 대한 최고 의 설계가 어떤 것인지 어떻게 알 수 있을까?예를 들어서, 30장의 썸네일의 결과값을 내놓는 이미지 검색 엔진을 설계한다면 당신은 이미지들을 순차적으로 불러올 것인가?혹은 병렬적으로?캐싱은 할 건가?이런 것들은 어떻게 결정을 할 것인가?만약 너가 멀티버스의 힘(power of the multiverse)을 이용한다면, 생각하는 모든 옵션의 설계를 시도해보고 어떤 것이 가장 좋은지 확인할 수 있을 것이다.하지만 그건 터무니없는 이야기이다.다른 선택지는 대안이 되는 알고리즘의 순서를 고려하는 것이다.(order of various algorithm)계산적 사고 황금기의 예언자인 구글은 분명이 이렇게 하겠지만, 또 다른 방법으로는 어떻게 하고 있을까?Back-Of-The-Envelope(봉투 뒷면) 계산 방법을 이용해서 다양한 설계 계산하기구글의 사회 기반 시설 대학원 (School of Infrastructure Wizardry / 광고 서비스, 검색, 맵리듀스, 버퍼프로토콜 등 구글의 중요하고 다양한 대표 시스템들을 포함하고 있음) 대표인 제프 딘(Jeff Dean) 은 봉투 뒷편 계산법을 사용해서 다양한 설계를 계산하는 방법을 주장한다.그는 스탠포드 강의에서 전체 이야기를 풀어냈었다.봉투 뒷편 계산법은 당신이 사고 실험과 통용적인 성능의 숫자를 혼합해서 추정을 할 수 있게 해준다. 이를 통해 어떤 설계가 당신의 요구사항에 적절하게 부합하는지 알아차릴 수 있게 한다.딘 박사는 직접 설계를 통해서 설계 성능의 대체 시스템을 추정하는 것이 아니라, 봉투 뒷편 계산법을 이용해서 대략적인 설계를 할 수 있는 것이 모든 소프트웨어 엔지니어에게 중요한 기술이라고 생각한다.스탠포드 강의에서 딘 박사는 매우 좋은 예시를 하나 제시했는데, 그전에 먼저 알아야 할 것들이 존재한다.모두가 반드시 알아야하는 숫자들설계 대안을 계산하기 위해서 당신은 전형적인 운영 동작이 얼마나 오래 걸리는지에 대한 적절한 감각이 있어야만 한다.딘 박사는 아래의 리스트를 제시했다. L1 캐쉬 참고 : 0.5 ns 분기 예측 오류 : 5 ns L2 캐쉬 참조 : 7 ns 뮤텍스(Mutex) 락/언락 : 100 ns 주 메모리 참조 : 100 ns Zippy 로 1 KB 압축 : 10,000 ns 1Gbps 네트워크로 2KB 전송 : 20,000 ns 메모리에서 1 MB 순차적으로 Read : 250,000 ns 같은 데이터 센터 내에서의 메시지 왕복 지연 시간 : 500,000 ns 디스크 탐색 : 10,000,000 ns 네트워크에서 1 MB 순차적으로 Read : 10,000,000 ns 디스크에서 1 MB 순차적으로 Read : 30,000,000 ns 한 패킷의 CA(캘리포니아)로부터 네덜란드까지의 왕복 지연시간 : 150,000,000 ns몇 가지 주의할 점이 있다. 성능의 다양한 옵션에 따라서 규모의 차이가 있다는 것을 인지하라. 데이터 센터는 서로 멀리 떨어져 있기 때문에 서로 어떤 것들 보내는데 긴 시간이 소요된다. 메모리는 빠르고 디스크는 느리다. 값 싼 압축 알고리즘(2의 N제곱)을 사용한다면 네트워크 대역폭을 아낄 수 있다. 쓰기는 읽기보다 40배나 값 비싼 연산이다. 글로벌 공유 데이터는 값 비싸다. 이것은 분산 시스템의 근본적인 한계이다. 대량으로 작성된 공유 데이터에서의 락은 트랜잭션을 직렬화 시키면서 성능을 저하시킨다. 쓰기의 확장을 고려한 설계를 하라. 쓰기 경합을 낮추도록 최적화하라. 넓게 최적화 하라. 할 수 있는 한 쓰기를 병렬화 하라.에시: 30장의 썸네일을 가진 이미지 결과 생성 페이지 만들기이 예시는 스탠포드 강의에서 제시된 것이다. 두가지의 설계 대안은 사고 실험으로 사용된다.설계1 - 연속적 이미지를 연속적으로 읽어온다. 디스크 탐색을 한다. 256K의 이미지를 읽고 다음 이미지로 넘어간다. 성능 : 30번탐색 _ 10ms/탐색 + 30 _ 256K % 30MB/s = 560ms설계2 - 병렬적 문제를 병렬로 읽는다. 성능 : 10ms/탐색 + 256K read % 30MB/s = 18ms 디스크 읽기로부터 변화가 있을 것이다. 따라서 대략적인 소요 시간은 30-60ms 일 것이다.어떤 설계가 더 적합한가?당신의 요구사항에 따라서 다르겠지만 봉투 뒷편 계산법을 사용한다면 직접 설계하지 않고도 빠르게 성능을 비교할 수 있다.이제 당신은 스스로에게 다른 설계 질문과 설계의 다양성에 대한 차이를 물어볼 수 있는 프레임워크를 갖추었다. 썸네일 이미지를 하나씩 캐싱하는게 합리적인가? 하나의 엔트리에 전체 이미지를 캐싱해야만 하는가? 썸네일을 미리 계산하는게 합리적인가?이러한 추정치들을 현실적으로 하기 위해서 당신은 당신의 서비스의 성능을 알아야만 한다.만약 알지 못하는 변수가 있는 경우 해당 부분만 빠르게 프로토타입으로 제작하여 해결할 수 있다.예를 들어서 캐싱이 좋은 설계 대안인지 알아보기 위해서 당신은 캐쉬에 쓰기연산을 할 때 얼마나 시간이 소요되는지 알아야할 필요가 있다.얻은 교훈 봉투 뒷편 계산법 은 당신이 다양한 변형을 살펴볼 수 있게 한다. 당신의 시스템을 설계할 때, 이러한 종류의 게산은 당신 머리속으로 계속해서 반복해야 한다. 당신의 시스템 블럭들을 설게하기 위해서 봉투 뒷면의 숫자를 알고 있어야한다. 일반적인 성능의 수치를 단순히 안다는 것은 충분하지 않으며, 하위 시스템이 얼마나 성능을 내는지 알고 있어야만 한다. 어떻게 계산이 되었는지 알지 못한다면 봉투 뒷편 계산법을 반대로 수행할 수 없을 것이다.나는 개인적으로 이러한 접근을 상당히 좋아한다.일반적인 경우보다 봉투 뒷편 계산법은 시작부터 끝까지 시스템의 본질에 기반을 두는 것 같아 보인다.오늘날의 관행은 실제로 더 크고 전체적인 분석에서 연구 가능하고 틀어 막을 수 있는 다양한 알고리즘들의 속임수에 초점을 맞추고 있다.관련 글들 Numbers Everyone Should Know The Back of the Napkin by Dan Roam A Physicist Explains Why Parallel Universes May Exist by Brian Green " }, { "title": "[번역] Stack Overflow는 어떻게 운영되고 있을까?", "url": "/posts/eng01-stackoverflow/", "categories": "Study", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-28 00:00:00 +0900", "snippet": " 개발자들의 필수 사이트 Stack Overflow가 어느 정도의 부하를 감당하고 있는지, 어떤 스펙의 하드웨어로 운영하고 있는지에 대한 좋은 칼럼이 있어서 번역글을 작성해본다. 부족한 번역 실력 양해 부탁한다.원본 링크Stack Overflow는 어떻게 운영되고 있을까?Stack Overflow 가 규모 있게는 돌아가지만 완전한 규모에 위치하고 있지는 않은 것 같다고 생각한다.이게 무슨말이냐면 우리는 매우 효율적으로 웹을 운영하고 있지만, 나는 여전히 우리가 크다 라고는 생각하지 않는다.우리가 현재 어떠한 규모에 위치하고 있는지 알아보기 위해서 몇 가지의 숫자를 한번 제시해보겠다.이 숫자들은 몇 일 전 24시간 동안의 기록에 대한 것들이다. (정확하게는 2012년 11월 12일이다.)전형적으로 평일에 나온 숫자들이고 오직 활성화된 데이터 센터들만을 포함하고 있다. (우리가 소유하고 있는 것 말이다.)우리 CDN 서버로의 접근과 대역폭은 우리 네트워크를 접근하지 않기 때문에 포함되어 있지 않다. 148,084,883 - 로드벨런서로의 HTTP 응답 수 36,095,312 - 페이지 로드의 수 833,992,982,627 bytes (776 GB) - 보내진 HTTP 트래픽 양 286,574,644,032 bytes (267 GB) - 수신한 전체 양 1,125,992,557,312 bytes (1,048 GB) - 송신한 전체 양 334,572,103 SQL 쿼리 (HTTP 응답으로 부터만 본 결과) 412,865,051 - Redis hits 3,603,418 - 태그 엔진들의 요청 수 558,224,585 ms (155 hours) - SQL 쿼리들을 실행하는데 소요된 시간 99,346,916 ms (27 hours) - redis에 hit 하는데 소요된 시간 132,384,059 ms (36 hours) - 태그 엔진 요청에 소요된 시간 2,728,177,045 ms (757 hours) - ASP.Net 에서의 과정에 소요된 시간(우리가 이러한 숫자들을 어떻게 빠르게 얻을 수 있었는지, 그리고 그 숫자들을 가지는 것 만으로도 얼마나 노력할만한 가치가 있는지 포스팅을 해야만 했다.)이것들이 전체를 포함한 것이 아닌 오직 Stack Exchange 네트워크만을 의미한다는 것을 명심하길 바란다. 2가지 총합의 예외를 제외한 나머지 숫자들은 우리가 성능을 살펴보기 위해서 오직 HTTP 요청으로만 얻었다.숫자들이 하루 치고는 굉장히 많은 시간들인데 과연 어떻게 관리를 하고 있는 것일까?우리는 이것들을 마법이라고 부르고, 어떤 이들은 멀티 코어 프로세서를 가진 멀티 서버 라고 부른다.하지만 우린 계속해서 마법이라고 부를 것이다.아래는 데이터 센터의 Stack Exchange 네트워크에서 무엇이 동작하고 있는지에 대한 것이다. 4 MS SQL Servers 11 IIS Web Servers 2 Redis Servers 3 Tag Engine servers (anything searching by tag hits this, e.g. /questions/tagged/c++) 3 elasticsearch servers 2 Load balancers (HAProxy) 2 Networks (each a Nexus 5596 + Fabric Extenders) 2 Cisco 5525-X ASAs (think Firewall) 2 Cisco 3945 Routers이렇게 생겼다!우리는 사이트만 운영하는 것이 아니다. 가장 가까운 랙에 있는 나머지 서버들은 배포, 도메인 컨트롤러, 모니터링, 운영 데이터베이스 등과 같이 사이트를 직접 서비스하지 않는 보조 기능을 위한 VM과 다른 인프라 장비들이다.위의 리스트 중에서 2개의 SQL 서버는 최근까지 만해도 백업용도였다. - 지금은 읽기 전용 로드에 사용되어지고 있어서 길게 생각하지 않고 계속해서 규모를 늘리고 있다. (이건 주로 the Stack Exchange API 로 이루어져 있다.)Core Hardware만약 중복된 것들을 제외하고 Stack Exchange가 (현재와 같은 수준의 성능을 유지하면서) 동작하기 위해서는 아래의 것들이 필요하다. 2 SQL servers (SO가 한 대를 차지하고 있고 다른 모든 것들은 남은 한 대에 있다… 그래도 헤드룸에 남아 있는 기계 한대로 동작할 수 있긴 하다) 2 Web Servers (2개라고 믿고 있지만 아마 3개일 것이다.) 1 Redis Server 1 Tag Engine server 1 elasticsearch server 1 Load balancer 1 Network 1 ASA 1 Router(언젠가는 장비들을 종료시키고 Breaking Point 가 무엇인지 테스트 해야만 한다!)이제는 몇 개의 VM들과 백 그라운드에서 잡, 도메인 컨트롤러 등을 처리할 수 있는 몇 가지만 남아있다. 하지만 이것들은 너무나도 경량화되어 있어서 Stack Overflow 그 자체에 집중할 수가 없고 모든 페이지들을 최고 속도로 렌더링할 수가 없다.만약 제대로된 비교를 하기 위해서는 (apples to apples) 모든 VMware 서버들을 모든 경우의 수에 투입해봐라. 많은 수의 장비는 아니지만 이 장비들의 스펙을 클라우드에서 합리적인 가격으로 맞추는 것은 거의 불가능하다.아래는 빠르게 서버를 Scale up하는 방법들이다. SQL 서버들은 384 GB 의 메모리와 1.8TB of SSD 저장소를 가지고 있다. Redis 서버들은 96 GB 의 RAM 을 가지고 있다. elastic search 서버들은 196 GB 의 RAM을 가지고 있다. Tag engine 서버들은 우리가 살 수 있는 가장 빠른 raw processors 를 가지고 있다. 네크워크 코어들은 각 포트가 10 Gb 의 대역폭을 가지고 있다. 웹 서버들은 그렇게 특별하지 않은데, 32 GB 와 2x quad core, 그리고 300GB 의 SSD 저장소를 가지고 있다. 2x 10Gb를 가지고 있지 않는 서버들은 (e.g. SQL) 4x 1 Gb 의 네트워크 대역폭을 가지고 있다.20Gb 가 엄청난 과잉일까?물론이다! 활성화된 SQL 서버들은 20GB 파이프에서 평균적으로 약 100-200 Mb를 가지고 있다.하지만 백업, 리빌드 등과 같은 기능들은 메모리 및 SSD 저장소가 현재 얼마나 있는지에 따라서 완전히 포화될 수도 있기 때문에 목적에 부합한다.Storage현재 약 2TB의 SQL 데이터를 가지고 있기 떄문에(첫번째 클러스터의 18개 SSD에서 1.06 TB / 1.63 TB / 두번째 클러스터의 4개 SSD에서 1.45 TB), 우리는 클라우드가 필요하다.(흠, 이 단어가 또 나왔다.)모든 것은 SSD라는 것을 명심해라. 모든 데이터베이스에서 평균적인 쓰기 시간은 0 MS 이다. 이게 우리가 측정할 수 있는 최고의 단위는 아니기 때문에 저장소는 더 잘 대처하고 있을 것이다.데이터베이스가 메모리에 있고 그 앞에 2레벨의 캐쉬가 있다면, Stack Overflow의 실제 읽기-쓰기 비율은 40:60이다.그래, 제대로 읽은 것 맞다. 60%의 우리 데이터베이스 디스크 접근은 쓰기이다. (읽기/쓰기 부하를 알아야한다.)각 웹 서버 또한 2x 320GB SSDs in a RAID 1 의 저장소를 가지고 있다.Elastic Box들은 300 GB를 필요로 하고 SSD 보다 더 나은 성능을 보인다. (우린 write/reindex를 매우 자주 한다.)우리가 SAN과 핵심 네트워크에 2x 10Gb 속도로 연결이 되는 24x900GB 10K SAS(Equal Logic PS6110X)를 가지고 있는 것은 별 의미가 없다.이건 고가용성을 위한 공유 메모리로서 VM에 독점적으로 사용되지만 실제로는 우리 웹사이트를 호스팅하는데 지원되지 않는다.다시 말해서 SAN이 죽더라도 우리의 웹사이트는 일시적으로 알아차리지 못할 수도 있다. (VM 도메인 컨트롤러가 그 요소일 뿐이다.)종합내용 (Put it all together)그래서 전체적인 것들이 어떻다는 것일까?우리는 성능을 원한다. 아니, 우리는 성능이 필요하다.성능은 우리에게 매우 중요한 특징이다. (Performance is a feature)우리의 모든 사이트에서 로딩되는 메인 페이지는 질문 페이지인데 내부적으로는 Question/Show라고 잘 알려져있다. 11월 12일 페이지는 평균 28 밀리세컨즈로 렌더링이 되었다.50ms 의 속도를 유지하기 위해 노력하는 동안, 우리는 여러분의 페이지 로딩 경험을 가능한 빠르게 하기 위해서 정말로 고군분투했다.우리의 모든 개발자들은 성능에 관해서 보증 가능할 정도로 꼼꼼한 괴짜들이기 때문에 소요 시간을 낮게 유지하는데 큰 도움을 준다.아래의 내용은 24시간동안 평균적인 렌더링에 있어서 SO의 Top Hit Page들이다. Question/Show: 28 ms (29.7 million hits) User Profiles: 39 ms (1.7 million hits) Question List: 78 ms (1.1 million hits) Home page: 65 ms (1 million hits) (매우 느린편이다. - Kevin Montrose 는 곧 수정이 될 것이다. : 가장 주요한 원인)우리는 모든 요청들을 기록하면서 우리의 페이지에 어떤 것들이 오고가는지에 대해 아주 효율적인 가시성을 가지고 있다.당신은 아래 종류의 메트릭을 필요로한다. 만일 그렇지 않다면 결정을 할 때 어디에 근간을 두겠는가?이런 간편한 메트릭을 통해서 우리는 좀 더 쉽게 접근할 수 있고 쉽게 파악할 수 있다.페이지 접근 비율이 급격하게 떨어진 후, 만약 당신이 구체적인 페이지의 접근 비율 급락 원인에 대해서 궁금해 한다면 나는 이 숫자들을 첨부한 것에 대해 기쁘게 생각할 것 같다.나는 여기서 렌더링 시간에 초점을 맞추었는데, 이는 웹페이지를 생상하는데 얼마나 오래 걸리는지에 대한 시간이기 때문이다. 데이터의 전송 속도는 완전히 다른 주제이기 때문에 추후에 다시 한번 더 다루도록 하겠다. (솔직히 아주 관련이 있긴 하다.)Room to grow서버들이 매우 낮은 활용률로 동작하고 있다는 것은 분명히 주목할 필요가 있다.웹 서버들는 평균적으로 5-15% CPU, 15.5 GB 의 RAM 사용 그리고 20-40 Mb/s 의 네트워크 트레픽을 가지고 있다. SQL 서버들은 평균적으로 5-10% CPU, 365 GB 의 RAM 사용 그리고 100-200 Mb/s 의 네트워크 트레픽을 가지고 있다.이를 통해 다음과 같은 이점들을 얻을 수 있다.업그레이드 하기전 더 커질 수 있는 일반적인 공간, 무엇인가 잘못되었을 때(이상한 쿼리, 이상한 코드, 해커들의 공격 등 무엇이든 될 수 있다.) 머물 수 있는 헤드룸 그리고 필요하다면 전력을 재충전할 수 있는 능력.아래는 방금 Opserver 로 부터 얻은 우리 웹의 등급이다.활용율이 낮은 주요 원인은 효과적인 코드때문이다.이건 지금 이 포스팅의 주요한 주제는 아니지만 효율적인 코드가 당신의 하드웨어가 발전하는데 치명적일 수 있다. 수행할 필요가 없는 작업은 아무것도 하지 않는 것 보다 비용이 많이든다. 또한 당신의 코드 하위 집합이 더 효율적일 수 있는 경우에도 그러하다.비용은 다음과 같은 형태로 만들어진다.전력 소비, 하드웨어 비용(당신이 더 크고 많은 서버를 원한다면…), 더 복잡한 무언가를 이해하는 개발자들(솔직히 말해서 이건 양쪽으로 다 갈 수 있다. 효율적인게 반드시 간단한건 아니다.) 그리고 페이지 렌더링의 느린 속도(다음 페이지로 넘어가는 유저들이 더 적어지거나 아예 다시 방문하지 않을 수도 있다.).비효율적인 코드의 비용은 당신이 생각하는 것보다 훨씬 높을 수 있다.지금까지 우리는 Stack Overflow가 현재의 하드웨어로 어떻게 운영되는지 보았다. 다음 시간에는 우리가 왜 클라우드 상에서 운영하지 않는지를 알아볼 것이다." }, { "title": "[Audio] ffprobe 설명 및 사용법", "url": "/posts/audio-ffprobe/", "categories": "Audio", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-26 00:00:00 +0900", "snippet": "ffprobe 설명 및 사용법ffprobe 이란?많이 알려져 있는 ffmpeg은 미디어 포맷을 변환하는데 사용하는 도구라면, ffprobe는 쉽게 말해 간단한 멀티미디어 Stream 분석기이다.[공식 문서 : https://ffmpeg.org/ffprobe.html]사용 나는 서버 상에서 사용을 해야하는데, 설치하지 못하는 환경이라서 docker image를 이용해 run을 시켰다.Command output 을 json으로 출력을 하였고 몇 가지 옵션을 추가하였다.$ docker run --rm -v /home:/config DOCKER_IMAGE:latestffprobe -v quiet-print_format json-show_format-show_streams/config/test1.wavOutput 아래의 결과 값과 같이 해당 미디어의 다양한 Key값들을 분석해서 보여줄 수 있다.{ &quot;streams&quot;: [ { &quot;index&quot;: 0, &quot;codec_name&quot;: &quot;pcm_s16le&quot;, &quot;codec_long_name&quot;: &quot;PCM signed 16-bit little-endian&quot;, &quot;codec_type&quot;: &quot;audio&quot;, &quot;codec_time_base&quot;: &quot;1/16000&quot;, &quot;codec_tag_string&quot;: &quot;[1][0][0][0]&quot;, &quot;codec_tag&quot;: &quot;0x0001&quot;, &quot;sample_fmt&quot;: &quot;s16&quot;, &quot;sample_rate&quot;: &quot;16000&quot;, &quot;channels&quot;: 1, &quot;bits_per_sample&quot;: 16, &quot;r_frame_rate&quot;: &quot;0/0&quot;, &quot;avg_frame_rate&quot;: &quot;0/0&quot;, &quot;time_base&quot;: &quot;1/16000&quot;, &quot;duration_ts&quot;: 49920, &quot;duration&quot;: &quot;3.120000&quot;, &quot;bit_rate&quot;: &quot;256000&quot;, &quot;disposition&quot;: { &quot;default&quot;: 0, &quot;dub&quot;: 0, &quot;original&quot;: 0, &quot;comment&quot;: 0, &quot;lyrics&quot;: 0, &quot;karaoke&quot;: 0, &quot;forced&quot;: 0, &quot;hearing_impaired&quot;: 0, &quot;visual_impaired&quot;: 0, &quot;clean_effects&quot;: 0, &quot;attached_pic&quot;: 0, &quot;timed_thumbnails&quot;: 0 } } ], &quot;format&quot;: { &quot;filename&quot;: &quot;/config/test1.wav&quot;, &quot;nb_streams&quot;: 1, &quot;nb_programs&quot;: 0, &quot;format_name&quot;: &quot;wav&quot;, &quot;format_long_name&quot;: &quot;WAV / WAVE (Waveform Audio)&quot;, &quot;duration&quot;: &quot;3.120000&quot;, &quot;size&quot;: &quot;99884&quot;, &quot;bit_rate&quot;: &quot;256112&quot;, &quot;probe_score&quot;: 99 }}" }, { "title": "[K8S] Grafana vs Kibana", "url": "/posts/tb10-k8s-grafana-kibana/", "categories": "Kubernetes", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-19 00:00:00 +0900", "snippet": "Grafana vs Kibana쿠버네티스에서 서비스를 운영하기 위해서 모니터링 툴을 적용해야 했다.기존 시스템에서는 ELK 스택 중 하나인 Kibana를 써왔지만, 쿠버네티스 시스템에서는 Prometheus + Grafana 스택을 많이들 쓴다고 해서 기존 Kibana와 Grafana의 차이점에 대해서 알아보았다.Kibana 로깅 시스템에서의 시각화 도구 Kibana 는 Elasticsearch 및 더 광범위한 ELK Stack에서 통합되어 다양한 인덱스 데이터를 검색 및 확인하는데 사용됨. 막대 차트, 원형 차트, 표, 히스토그램, 지도 등을 생성할 수 있어 인덱스 데이터의 시각화가 간편함. 주로 Log Message 분석에 사용됨 Index를 통해서 Log를 상세하게 분석할 수 있다.Grafana 시계열 매트릭 데이터 수집에 강점을 가지고 있다. System info (CPU, Memory, Disk …) 등의 메트릭 지표를 시각화하는데 사용된다. 알람 기능이 무료이다. 대쉬보드가 위주이다.Kibana vs Grafana목적 Grafana 메시지 로깅에 대한 정보보다는 시스템의 메트릭을 확인하는데 더 큰 중점을 가지고 있음. Kibana 여러가지 Index 설정을 통해 메시지의 로그를 구체적이고 가시적으로 확인하기 위함. 어떤 것이 더 낫다고 이야기할 수 있는 부분이 아니다. 서로 다른 목적을 가지고 있는 시각화 툴이다. 어떤 팀에서는 Kibana 와 Grafana를 함께 운용하는 팀도 있다. 즉, 목적성에 맞게 사용하는 것이 필요함!" }, { "title": "[K8S] 쿠버네티스 재시작은 어떻게? Rollout restart vs apply", "url": "/posts/tb09-k8s/", "categories": "Kubernetes", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-15 00:00:00 +0900", "snippet": "Rollout restart vs applyDeployment 나 Kubernetes의 여러가지 Object들을 생성하기 위해서 Apply 명령어를 많이 쓴다.그렇다면 deployment 를 재시작하기 위해서는 어떤 명령어를 써야할까?Apply kubectl apply 는 모든 쿠버네티스의 Obejct들 (Pod, Deployment, ConfigMaps, Secrets, etc) 에 모두 적용이 가능하다. 명세(yaml) 파일의 spec 필드의 변경이 있어야 apply 실행시 적용이 가능함.Rollout restart Kubectl rollout 은 Deployments, Statefulsets 와 같은 연산이 가능한 부분에 대해서만 적용이 가능함. spec 필드의 변경과 같은 어떠한 변화의 필요 없이 pod 재시작이 가능하다. 간단하게 정리하자면 Rollout restart 를 하게 되면 Gracefully 하게 pod가 Terminating 되고 새로운 pod가 생성된다. 하지만 Apply는 rollout restart 처럼 파드가 재시작되지는 않는다. config (yaml) 값이 변경되어야만 재시작이 됨. 이외에도 replace와 같은 명령어를 사용하게 되면 Gracefull 하진 않지만, pod가 delete 되고 create된다.참고 : https://stackoverflow.com/a/66420597/14995221" }, { "title": "[K8S] kubernetes imagepullpolicy 는 무엇이고 언제 적용될까?", "url": "/posts/tb08-k8s/", "categories": "Kubernetes", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-11 00:00:00 +0900", "snippet": "kubernetes imagepullpolicy 는 무엇이고 언제 적용될까?imagepullpolicy해석 그대로 컨테이너를 생성할때 사용하는 Image의 Pull 정책에 대한 설정값이다.IfNotPresent이미지가 로컬에 없는 경우에만 내려받는다.Alwayskubelet이 컨테이너를 기동할 때마다, kubelet이 컨테이너 이미지 레지스트리에 이름과 이미지의 다이제스트가 있는지 질의한다. 일치하는 다이제스트를 가진 컨테이너 이미지가 로컬에 있는 경우, kubelet은 캐시된 이미지를 사용한다. 이외의 경우, kubelet은 검색된 다이제스트를 가진 이미지를 내려받아서 컨테이너를 기동할 때 사용한다.Neverkubelet은 이미지를 가져오려고 시도하지 않는다. 이미지가 어쨌든 이미 로컬에 존재하는 경우, kubelet은 컨테이너 기동을 시도한다. 이외의 경우 기동은 실패한다. 보다 자세한 내용은 미리 내려받은 이미지를 참조한다.IfNotPresent 이미지가 Local에 없는 경우에만 내려받음Always kubelet이 컨테이너를 가동할 때마다 컨테이너 이미지의 이름과 이미지의 Digest가 있는지 확인한다. 로컬에 있는 경우 캐시된 이미지를 사용, 이외의 경우는 다시 이미지를 내려받음Never 이미지를 가져오려고 시도하지 않음. 로컬에 이미지가 존재하지 않는다면 컨테이너 실행은 실패한다.그렇다면 어떤 특정 시기에 이 값이 영향을 미칠까?나 같은 경우는 일반적으로 Always값을 사용한다. 위의 설명에서 확인할 수 있듯이 컨터이너 실행에서 정책이 영향을 미친다. 즉, Deployment 가 항상 새로운 이미지를 바라보는 것이 아니다. 컨테이너가 Run할때 어떤식으로 동작할 것인지를 본다. 그렇다면 언제 컨테이너를 실행할까?Pod의 Life cyclePod에는 1개 이상의 Container가 실행될 수 있는데,언제 컨테이너가 실행되는지 알기 위해서는 Pod의 Life Cycle을 확인할 필요가 있다. Pending에서 시작해서 컨테이너 중 하나라도 OK로 시작되면 Running 으로 바뀐다. 컨테이너가 실패로 종료되었는지 여부에 따라 Succeeded or Failde 로 바뀜.CrashLoopBackOff 에서의 ImagePullPolicyPod가 crashloopbackoff 상태일때는 ImagePullPolicy가 어떻게 적용될까?CrashLoopBackOff CrashloopBackOff 는 Pod가 Starting -&amp;gt; Crashing -&amp;gt; Starting Again 을 계속 반복한다는 것을 의미한다. 즉, 컨테이너 실행 중 Crashing 이 발생하면 Pod가 반복적으로 Restart를 수행한다. Pod가 CrashloopBackOff 상태일때 Restart 값을 확인해보면 계속 증가하는 것을 확인할 수 있다. 즉, container가 새로 생성될 때마다 imagepull을 하게된다. crashloopbackoff 이면 pod가 pending -&amp;gt; restart -&amp;gt; running 으로 가는거니깐 container 새로 생성. 즉 image 받아옴.참고 : https://stackoverflow.com/a/45906651/14995221" }, { "title": "[MLops] [MLops Deployment &amp; Monitoring] Model Monitoring", "url": "/posts/ml/", "categories": "MLops", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-12-03 00:00:00 +0900", "snippet": "[ MLops Deployment &amp;amp; Monitoring ] Model MonitoringTrain, Test, Deploy.그래서, 다 끝난거야?배포 작업 이후에 어떤 것이 문제가 될 수 있을까?&amp;lt;/br&amp;gt;그것을 알기 위해서는 Monitoring 작업이 필수적이다.&amp;lt;/br&amp;gt;Model Monitoring 는 크게 3가지로 나뉜다. Data Drift Model Drift (Concept Drift) Domain Shift그럼 대략적으로 하나씩 훑어보자.Data Drift 의 종류 즉각적인 Drift 점진적인 Drift 주기적인 Drift 일시적인 Drift그렇다면 Data Drift 는 어떻게 알아차릴까?먼저 A 부분을 Healthy 한 Data라고 가정한다.그리고 비교하고 싶은 B라는 window를 지정한다.그 후 둘 간의 Distance를 비교한다.비교 방법 Rule-based distance metrics (aka, data quality) 통계적 방법 KL divergence ( 가장 많이 사용함 ) Kolmogorov-Smirnov statistic D_1 distance High Dimensional data일 경우? 단순히 Distance를 비교하게 되면 데이터 손실이 크다-&amp;gt; 차원 축소의 Logic을 잘 거친 후에 Distance를 비교해야한다. Evaluation Store (평가 저장소)ML에서는 버그가 매우 조용하고 잡기 힘들다. 또한 Monitoring 을 통해서 얻을 수 있는 데이터가 다시 Model의 학습데이터로 사용될 수 있다.따라서 Monitoring은 기존 Legacy SW보다 ML에서는 매우 중요하다.이 그림은 ML에서 Monitoring 을 통해 데이터를 어떻게 활용할건지에 대한 Flywheel 그림이다.단순히 문제가 발생한 데이터를 버리는 것이 아니라 재가공을 진행하여 다시 학습을 시켜 Model을 한번 더 강화시키는 것이다.참고 : https://fullstackdeeplearning.com/spring2021/lecture-11/" }, { "title": "[Python] Python에서 * 와 ** 의 차이", "url": "/posts/tb01-args/", "categories": "Python", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-24 00:00:00 +0900", "snippet": "Python에서 * 와 ** 의 차이*args 몇개의 파라미터를 받을지 모른다. 이럴 경우 args는 Tuple 형태로 전달된다.def test(*args): for a in args: print(a)test(1,2,3,4,5,6)&amp;gt;&amp;gt;123456**kwargs 파라미터 명을 같이 보낼 수 있다. Dictionary 형태로 전달된다.def test2(**kwargs): print(kwargs.keys()) print(kwargs.values())test2(a=1,b=2,c=3)&amp;gt;&amp;gt;&amp;gt;dict_keys([&#39;a&#39;, &#39;b&#39;, &#39;c&#39;])dict_values([1, 2, 3])Trouble ShottingClass로 정의해둔 형태를 호출해서 Dictionary 형태로 넘기고 싶었다.class FooCreate(FooBase): a: str b: str c: strclass FooDeployService: def create_item(self, item, db): item = FooCreate(**item) result = FooService(db).create_item(item) return handle_result(result)class FooService(AppService): def create_item(self, item: FooCreate) -&amp;gt; ServiceResult: Foo_item = FooCRUD(self.db).create_item(item) if not Foo_item: return ServiceResult(AppException.FooCreateItem()) return ServiceResult(Foo_item)즉, 아래와 같은 test형태로 create_item을 호출하고 싶었는데,test -&amp;gt; FooDeployService -&amp;gt; FooService**item이 아니라 item을 넣으니 FooCreate 클래스의 attribute 형태에 맞게 Mapping 되지 않고 에러가 발생했다. 아래와 같이 ** 형태로 넣으니 Key값을 가진 파라미터 형태로 하나씩 들어가기 때문에 에러가 나지 않고 해결되었다.def test(): item = {&quot;a&quot;: a, &quot;b&quot;: b, &quot;c&quot;: c} FooDeployService().create_item(item, db)참고 : https://sshkim.tistory.com/182" }, { "title": "[K8S] 쿠버네티스(K8S)에서의 Log Aggregator, Logstash vs Fluentd", "url": "/posts/tb05-k8s_log_aggregator/", "categories": "Kubernetes", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-23 00:00:00 +0900", "snippet": "쿠버네티스(K8S)에서의 Log Aggregator, Logstash vs FluentdLog Aggregator (로그 수집기) 란? 거의 대부분의 서비스에서는 디버깅을 위한 로그 수집은 필수적이다.ELK 스택으로 로그 수집 및 시각화를 많이 하곤 한다. ELK Elastic Search Apache Lucene 기반의 Java 오픈소스 분산 검색 엔진 방대한 양의 데이터를 신속하게 (Near Real Time)으로 저장, 검색, 분석할 수 있다. Logstash Elastic 에서 만든 오픈 소스 서버측 데이터 처리 파이프라인 Kibana 로그 수집기로부터 데이터를 받아 시각화할 수 있는 오픈소스 Kubernetes에서의 Logging 가장 쉽고 가장 널리 사용되는 로깅 방법은 표준 출력과 표준 에러 스트림에 작성하지만 표준출력으로는 완전한 로깅이 불가능하다-&amp;gt; 컨테이너가 크래시되거나, 노드가 종료된 경우에도 애플리케이션의 로그에 접근해야한다. 따라서 클러스터에서 로그는 노드, 파드 또는 컨테이너와는 독립적으로 별도의 스토리지와 라이프사이클을 가져야 한다. 이를 클러스터-레벨-로깅이라고 한다.Logstash vs Fluentd Kubernetes에서의 로그 수집은을 할 떄는 어떤 툴을 사용할까?상황에 따라 다르지만 Fluentd가 더 매력적인 부분들이 많다. Fluentd는 Treasure Data에 의해 구축되었고 CNCF의 일부이다. CNCF : Cloud Native Computing Foundation은 컨테이너 기술을 발전시키고 기술 산업이 발전할 수 있도록 지원하기 위해 2015년에 설립된 Linux Foundation 프로젝트 Kubernetes나 Cloud 환경에서 더 Support가 잘 되고 있음 Docker에서 Fluentd를 위한 build-in logging driver를 가지고 있다. 이를 통해 Fluentd를 사용하면 다른 로그 파일 도움없이 직접적으로 STDOUT 을 내보낼 수 있다. Logstash는 Filebeat와 같은 플러그인을 이용해야만 가능 전체적인 차이점을 보고 싶다면 여기를 참고Logstash, Fluentd 각각 장단점이 있지만 Kubernetes와 같은 Cloud환경에서는 Fluentd가 더 적합한 것 같다. " }, { "title": "[Blog] Bucket List", "url": "/posts/bucketList/", "categories": "ETC", "tags": "blog, jekyll, jekyll theme, NexT theme, Computer Science, 컴퓨터공학, 개발, 소프트웨어, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-22 00:00:00 +0900", "snippet": "Bucket List2021.11.22 작성 대도시와 근접한 마당있는 넓은 집에서 아내와 큰 리트리버와 함께 바베큐해먹기 작성중…" }, { "title": "[Audio] ffmpeg 사용해서 16k wav 파일로 변환하기", "url": "/posts/tb04-ffmpeg/", "categories": "Audio", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-17 00:00:00 +0900", "snippet": "ffmpeg 사용해서 16k wav 파일로 변환하기Audio AttributesAudio 파일을 사용하기 위해서는 여러가지 값들을 알고 있어야한다.Sample Rate 샘플의 빈도 수 즉, 1초당 추출되는 샘플의 개수. SR이 높으면 높은 밀도의 음성, 낮으면 낮은 밀도의 음성. → 이것을 강제로 낮추면 느린 배속으로 재생이 되고, 높히면 빠른 배속으로 재생이 됨.Bit Rate 1초당 전송되는 데이터 양 즉, 1초당 전송되는 비트 수를 의미함.ffmpeg커맨드에서 Audio 파일을 원하는 포맷에 맞게 변경하기 위해서 ffmpeg과 soxi를 많이 쓰는데 오늘은 ffmpeg으로 16k wav파일을 만드는 커맨드를 보려한다.사용 나는 서버 상에서 ffmpeg을 사용해야하는데, 설치하지 못하는 환경이라서 docker image를 이용해 run을 시켰다.$ docker run --rm -v /home:/config DOCKER_IMAGE:latest ffmpeg -i /config/test1.wav 이런식으로 하면 ffmpeg이 사용 가능하다.ffmpeg version n4.3.1 Copyright (c) 2000-2020 the FFmpeg developers built with gcc 4.8.5 (GCC) 20150623 (Red Hat 4.8.5-44) libavutil 56. 51.100 / 56. 51.100 libavcodec 58. 91.100 / 58. 91.100 libavformat 58. 45.100 / 58. 45.100 libavdevice 58. 10.100 / 58. 10.100 libavfilter 7. 85.100 / 7. 85.100 libswscale 5. 7.100 / 5. 7.100 libswresample 3. 7.100 / 3. 7.100Guessed Channel Layout for Input Stream #0.0 : monoInput #0, wav, from &#39;/config/test1.wav&#39;: Duration: 00:00:03.12, bitrate: 256 kb/s Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 16000 Hz, mono, s16, 256 kb/sAt least one output file must be specified원하는 옵션으로 만들기나는 들어오는 모든 input Audio를wav, 16k, 16bit, mono 의 Format으로 만들고 싶었다.$ docker run --rm -v /home:/config DOCKER_IMAGE:latest ffmpeg -y -i /config/test1.aac -f s16le -ac 1 -ar 16000 /config/test1_out.wavffmpeg에서는 input 파일명과 output 파일명이 같을 수 없다. 원하는 옵션으로 output 파일이 만들어진다." }, { "title": "[Trouble Shooting] [DB] ERROR 2006 (HY000) MySQL server has gone away 에러 해결", "url": "/posts/tb06-mysql-server-error/", "categories": "Trouble Shooting, DB", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-15 00:00:00 +0900", "snippet": "ERROR 2006 (HY000) MySQL server has gone away 에러 해결Flask에서 MySQL을 이용하기 위해 Sqlalchemy 를 사용하던 도중 API 호출 시ERROR 2006 (HY000) MySQL server has gone away에러가 자주 발생하였다. 원인① MySQL 과 연결에 오류가 있는 경우② 패킷 전송에 문제가 있는 경우③ 이전 연결 세션에 영향을 받은 경우구글링해서 찾아본 결과, DB서버쪽의 설정값과 Flask client에서의 문제가 있을 수 있다.MySQL 서버 설정하기max_allowed_packet 값을 변경해주는 것인데,max_allowed_packet 는 서버로 질의하거나 받게되는 패킷의 최대 길이를 나타내는 변수 값이다.즉, client와 통신할 때 핸들링 할 수 있는 데이터 양을 의미한다.$ SET GLOBAL max_allowed_packet=64*1024*1024;이 컨맨드로 64MB로 max값을 수정했다.영구적으로 수정하려면 myslq의 conf에서 설정을 변경해주어야한다. (my.ini 파일)Client Sqlalchemy에서 설정 값 부여하기engine = db.create_engine(&quot;mysql+pymysql://root:PASSWORD@IP:3306/DBNAME&quot;, pool_pre_ping=True)engine을 선언할 때 pool_pre_ping 옵션 값에 True를 부여한다.pool_pre_ping 옵션은 Disconnect handling을 해결하는 옵션인데, DB서버 접속 전 “select 1” 과 같은 쿼리문을 ping으로 날려서 connection을 확인하고 연결을 진행한다.참고 : https://tjddnjs.tistory.com/69, https://blog.dork94.com/195" }, { "title": "[Trouble Shooting] Jekyll chirpy 템플릿으로 Github 블로그 시작하기. (Bundler Install Error)", "url": "/posts/tb07-jekyll-bundler-error/", "categories": "Trouble Shooting, Jekyll", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-13 00:00:00 +0900", "snippet": "Jekyll chirpy 템플릿으로 Github 블로그 시작하기. (Bundler Install Error)Github 블로그 with Chirpy Jekyll Themegithub 블로그를 시작할 때 Jekyll을 많이 사용한다.그럼 Jekyll이 무엇일까?Jekyll? 텍스트 변환 엔진으로, Markup 언어로 글을 작성하면 이를 통해 웹사이트를 만들어줌. 서버 소프트웨어가 필요 없어 매우 빠르고 가볍다.-&amp;gt; Jekyll을 사용한 템플릿을 땡겨와서 블로그를 시작하면 기본 틀을 잡기 매우 간편하다.Chirpy Jekyll을 이용한 여러 템플릿을 제공해준다. Git : https://github.com/cotes2020/jekyll-theme-chirpy Tutorial : https://chirpy.cotes.info/categories/tutorial/-&amp;gt; Readme를 꼼꼼히 읽어보면 시작하는 방법이 상세하게 나와있다.(사실 비전공자의 입장에서 본다면 시작하기가 꽤나 까다로울 수도…)처음 시작[ Mac 환경에서 진행함 ]$ brew install ruby$ bundle$ bundle exec jekyll s # Running Local Server먼저 bundle을 설치해야한다.Bundler 란?-&amp;gt; Bundler는 정확히 필요한 gem과 그 gem의 버전을 설치하고, 추적하는 것으로 일관성 있는 Ruby 프로젝트를 제공하는 도구!근데 bundle 명령어를 칠 떄마다An error occurred while installing racc (1.6.0), and Bundler cannot continue.요런 비슷한 에러가 자꾸 발생했다.해결책xcode-select --installsudo gem install -n /usr/local/bin cocoapods요걸로 설치가 되면 끝! (참고 : https://hello-bryan.tistory.com/208)하지만 나는…xcode-select: error: command line tools are already installed, use &quot;Software Update&quot; to install updates이런 에러가 다시 떴다.$ sudo rm -rf /Library/Developer/CommandLineTools$ sudo xcode-select --install다시 깔아줬음 그랬더니 gem 으로 설치됨. bundler 도 설치된다.sudo gem install -n /usr/local/bin cocoapods(참고 : https://blog.ddoong2.com/2019/10/09/Install-Command-Line-Tool/)-&amp;gt; 이렇게 했더니 bundler 설치 완료!!그 외의 것들은 Chirpy Jekyll Theme 튜토리얼을 잘 따라하면 블로그 포스팅이 완료된다.Github Action을 이용해서 page를 만들어주는게 독특했다." }, { "title": "First Post", "url": "/posts/First-post/", "categories": "ETC", "tags": "blog, jekyll, blog, jekyll theme, NexT theme, 지킬 테마, 지킬 블로그 포스팅, GitHub Pages", "date": "2021-11-12 00:00:00 +0900", "snippet": "WelcomeHello world, this is my first Jekyll blog post.I hope you like it!" } ]
